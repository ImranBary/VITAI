{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook:\n",
    "1. Uses the CCI integrated dataset.\n",
    "2. Filters to diabetes patients.\n",
    "3. Runs VAE and TabNet with parameter input_file.\n",
    "4. Performs clustering analysis and visualization (t-SNE, UMAP) as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap.umap_ as umap\n",
    "from scipy.stats import f_oneway, kruskal, shapiro, levene\n",
    "\n",
    "project_dir = os.path.abspath(\"..\")\n",
    "data_dir = os.path.join(project_dir, \"Data\")\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "from vae_model import main as vae_main\n",
    "from tabnet_model import main as tabnet_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CCI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cci_file='patient_data_with_health_index_cci.pkl'\n",
    "cci_path=os.path.join(data_dir,cci_file)\n",
    "if not os.path.exists(cci_path):\n",
    "    raise FileNotFoundError(\"CCI file not found. Run 01_full_dataset_with_cci first.\")\n",
    "\n",
    "patient_data=pd.read_pickle(cci_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to diabetes patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions=pd.read_csv(os.path.join(r'C:\\Users\\imran\\Documents\\VITAI\\Data','conditions.csv'),usecols=['PATIENT','DESCRIPTION'])\n",
    "diabetes_patients=conditions[conditions['DESCRIPTION'].str.lower().str.contains('diabetes')]['PATIENT'].unique()\n",
    "\n",
    "subset_data=patient_data[patient_data['Id'].isin(diabetes_patients)].copy()\n",
    "subset_file='patient_data_with_health_index_cci_diabetes.pkl'\n",
    "subset_data.to_pickle(os.path.join(data_dir,subset_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run models on diabetes subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_main(input_file=subset_file)\n",
    "tabnet_main(input_file=subset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latent features and predictions\n",
    "latent_features = pd.read_csv('latent_features_vae.csv')\n",
    "tabnet_predictions = pd.read_csv('tabnet_predictions.csv')\n",
    "data_merged = latent_features.merge(tabnet_predictions,on='Id',how='inner')\n",
    "\n",
    "X = data_merged.drop(columns=['Id','Predicted_Health_Index'])\n",
    "scaler = StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X)\n",
    "\n",
    "cluster_range=range(2,10)\n",
    "sil_kmeans=[]\n",
    "for n in cluster_range:\n",
    "    km=KMeans(n_clusters=n,random_state=42)\n",
    "    labels=km.fit_predict(X_scaled)\n",
    "    sil_kmeans.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "optimal_k = cluster_range[np.argmax(sil_kmeans)]\n",
    "kmeans=KMeans(n_clusters=optimal_k,random_state=42).fit(X_scaled)\n",
    "kmeans_labels=kmeans.labels_\n",
    "\n",
    "sil_agg=[]\n",
    "for n in cluster_range:\n",
    "    agg=AgglomerativeClustering(n_clusters=n)\n",
    "    labels=agg.fit_predict(X_scaled)\n",
    "    sil_agg.append(silhouette_score(X_scaled,labels))\n",
    "\n",
    "optimal_agg=cluster_range[np.argmax(sil_agg)]\n",
    "agg=AgglomerativeClustering(n_clusters=optimal_agg)\n",
    "agg_labels=agg.fit_predict(X_scaled)\n",
    "\n",
    "neighbors=5\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs=NearestNeighbors(n_neighbors=neighbors).fit(X_scaled)\n",
    "distances,indices=nbrs.kneighbors(X_scaled)\n",
    "distances=np.sort(distances[:,neighbors-1],axis=0)\n",
    "epsilon=distances[int(0.9*len(distances))]\n",
    "db=DBSCAN(eps=epsilon,min_samples=5).fit(X_scaled)\n",
    "dbscan_labels=db.labels_\n",
    "\n",
    "def cluster_scores(X,labels):\n",
    "    if len(set(labels))>1:\n",
    "        sil=silhouette_score(X,labels)\n",
    "        ch=calinski_harabasz_score(X,labels)\n",
    "        db=davies_bouldin_score(X,labels)\n",
    "    else:\n",
    "        sil=ch=db=np.nan\n",
    "    return sil,ch,db\n",
    "\n",
    "sil_km,ch_km,db_km=cluster_scores(X_scaled,kmeans_labels)\n",
    "sil_a,ch_a,db_a=cluster_scores(X_scaled,agg_labels)\n",
    "if len(set(dbscan_labels))>1:\n",
    "    sil_db,ch_db,db_db=cluster_scores(X_scaled,dbscan_labels)\n",
    "else:\n",
    "    sil_db=ch_db=db_db=np.nan\n",
    "\n",
    "validation_df=pd.DataFrame({\n",
    "    'Method':['KMeans','Agglomerative','DBSCAN'],\n",
    "    'Silhouette':[sil_km,sil_a,sil_db],\n",
    "    'CH':[ch_km,ch_a,ch_db],\n",
    "    'DB':[db_km,db_a,db_db]\n",
    "})\n",
    "validation_df['Sil_rank']=validation_df['Silhouette'].rank(ascending=False)\n",
    "validation_df['CH_rank']=validation_df['CH'].rank(ascending=False)\n",
    "validation_df['DB_rank']=validation_df['DB'].rank(ascending=True)\n",
    "validation_df['Avg_rank']=validation_df[['Sil_rank','CH_rank','DB_rank']].mean(axis=1)\n",
    "\n",
    "# Function to count clusters (excluding noise for DBSCAN)\n",
    "def get_n_clusters(labels):\n",
    "    unique_lbls = set(labels)\n",
    "    # If DBSCAN includes noise (-1), exclude it\n",
    "    if -1 in unique_lbls:\n",
    "        unique_lbls.remove(-1)\n",
    "    return len(unique_lbls)\n",
    "\n",
    "kmeans_n = get_n_clusters(kmeans_labels)\n",
    "agg_n = get_n_clusters(agg_labels)\n",
    "dbscan_n = get_n_clusters(dbscan_labels)\n",
    "\n",
    "validation_df['n_clusters'] = [kmeans_n, agg_n, dbscan_n]\n",
    "\n",
    "# Rank by n_clusters descending (more clusters = better)\n",
    "validation_df['Cluster_rank'] = validation_df['n_clusters'].rank(ascending=False)\n",
    "\n",
    "# Combine cluster preference\n",
    "# Adjust weight to control how much cluster count influences the final selection.\n",
    "# Positive weight means more clusters reduces the final rank (i.e. better).\n",
    "weight = 0.5\n",
    "validation_df['New_Avg_rank'] = validation_df['Avg_rank'] - weight*validation_df['Cluster_rank'].rank(ascending=True)\n",
    "\n",
    "best_method=validation_df.loc[validation_df['New_Avg_rank'].idxmin(),'Method']\n",
    "\n",
    "if best_method=='KMeans':\n",
    "    final_labels=kmeans_labels\n",
    "elif best_method=='Agglomerative':\n",
    "    final_labels=agg_labels\n",
    "else:\n",
    "    final_labels=dbscan_labels\n",
    "\n",
    "data_merged['Cluster']=final_labels\n",
    "cluster_map=data_merged.groupby('Cluster')['Predicted_Health_Index'].mean().sort_values().reset_index()\n",
    "cluster_map['Severity_Index']=range(1,len(cluster_map)+1)\n",
    "mapping=cluster_map.set_index('Cluster')['Severity_Index'].to_dict()\n",
    "data_merged['Severity_Index']=data_merged['Cluster'].map(mapping)\n",
    "\n",
    "tsne=TSNE(n_components=2,random_state=42)\n",
    "tsne_results=tsne.fit_transform(X_scaled)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x=tsne_results[:,0],y=tsne_results[:,1],hue=data_merged['Severity_Index'],palette='viridis')\n",
    "plt.title(f't-SNE visualization ({best_method})')\n",
    "plt.show()\n",
    "\n",
    "reducer=umap.UMAP(n_components=2,random_state=42)\n",
    "umap_results=reducer.fit_transform(X_scaled)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x=umap_results[:,0],y=umap_results[:,1],hue=data_merged['Severity_Index'],palette='viridis')\n",
    "plt.title(f'UMAP visualization ({best_method})')\n",
    "plt.show()\n",
    "\n",
    "clusters=data_merged['Cluster'].unique()\n",
    "normality_pvals=[]\n",
    "for c in clusters:\n",
    "    grp=data_merged[data_merged['Cluster']==c]['Predicted_Health_Index']\n",
    "    stat,p=shapiro(grp)\n",
    "    normality_pvals.append(p)\n",
    "\n",
    "if any(p<0.05 for p in normality_pvals):\n",
    "    groups=[data_merged[data_merged['Cluster']==c]['Predicted_Health_Index'] for c in clusters]\n",
    "    kw_stat,kw_p=kruskal(*groups)\n",
    "    print(f\"Kruskal-Wallis: H={kw_stat}, p={kw_p}\")\n",
    "else:\n",
    "    groups=[data_merged[data_merged['Cluster']==c]['Predicted_Health_Index'] for c in clusters]\n",
    "    lv_stat,lv_p=levene(*groups)\n",
    "    if lv_p<0.05:\n",
    "        kw_stat,kw_p=kruskal(*groups)\n",
    "        print(f\"Kruskal-Wallis: H={kw_stat}, p={kw_p}\")\n",
    "    else:\n",
    "        f_stat,f_p=f_oneway(*groups)\n",
    "        print(f\"ANOVA: F={f_stat}, p={f_p}\")\n",
    "\n",
    "print(\"Analysis complete. Full dataset with CCI integrated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# ---- 3D Embedding & Clustering with Expanded Metrics ----\n",
    "###########################################################\n",
    "from mpl_toolkits.mplot3d import Axes3D  # for 3D plotting\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# 1) Create a 3D embedding (pick t-SNE or UMAP)\n",
    "#    We'll demonstrate BOTH, then choose one:\n",
    "\n",
    "# Option A: 3D t-SNE\n",
    "tsne_3d = TSNE(n_components=3, random_state=42)\n",
    "tsne_3d_results = tsne_3d.fit_transform(X_scaled)\n",
    "\n",
    "# Option B: 3D UMAP\n",
    "reducer_3d = umap.UMAP(n_components=3, random_state=42)\n",
    "umap_3d_results = reducer_3d.fit_transform(X_scaled)\n",
    "\n",
    "# Choose which 3D embedding you want to use:\n",
    "# embedding_3d = tsne_3d_results\n",
    "embedding_3d = umap_3d_results  # <--- If you prefer UMAP in 3D\n",
    "\n",
    "# 2) Run K-Means for cluster_range = [6..10] in the 3D space.\n",
    "cluster_range_3d = range(6, 10)  # or range(6, 11) if you want 6..10 inclusive\n",
    "kmeans_3d_metrics = []\n",
    "\n",
    "for k_3d in cluster_range_3d:\n",
    "    kmeans_3d = KMeans(n_clusters=k_3d, random_state=42)\n",
    "    labels_3d = kmeans_3d.fit_predict(embedding_3d)\n",
    "    \n",
    "    # Compute 3 validation metrics\n",
    "    sil_3d = silhouette_score(embedding_3d, labels_3d)\n",
    "    ch_3d = calinski_harabasz_score(embedding_3d, labels_3d)\n",
    "    db_3d = davies_bouldin_score(embedding_3d, labels_3d)\n",
    "    \n",
    "    kmeans_3d_metrics.append({\n",
    "        'n_clusters': k_3d,\n",
    "        'silhouette': sil_3d,\n",
    "        'calinski_harabasz': ch_3d,\n",
    "        'davies_bouldin': db_3d\n",
    "    })\n",
    "\n",
    "# 3) Convert metrics to a DataFrame for ranking\n",
    "df_3d_eval = pd.DataFrame(kmeans_3d_metrics)\n",
    "\n",
    "# Rank: higher silhouette/CH is better => descending rank; lower DB is better => ascending rank\n",
    "df_3d_eval['sil_rank'] = df_3d_eval['silhouette'].rank(ascending=False)\n",
    "df_3d_eval['ch_rank'] = df_3d_eval['calinski_harabasz'].rank(ascending=False)\n",
    "df_3d_eval['db_rank'] = df_3d_eval['davies_bouldin'].rank(ascending=True)\n",
    "\n",
    "# Average rank across all three\n",
    "df_3d_eval['avg_rank'] = df_3d_eval[['sil_rank', 'ch_rank', 'db_rank']].mean(axis=1)\n",
    "\n",
    "# Pick best cluster count based on minimal avg_rank\n",
    "best_idx_3d = df_3d_eval['avg_rank'].idxmin()\n",
    "best_k_3d = df_3d_eval.loc[best_idx_3d, 'n_clusters']\n",
    "\n",
    "print(\"==== 3D K-Means Clustering Metrics (6..9 or 6..10) ====\")\n",
    "print(df_3d_eval[['n_clusters','silhouette','calinski_harabasz','davies_bouldin','avg_rank']])\n",
    "print(f\"\\nChosen #clusters for 3D embedding: {best_k_3d}\\n\")\n",
    "\n",
    "# 4) Fit final K-Means with best_k_3d in 3D embedding\n",
    "kmeans_final_3d = KMeans(n_clusters=best_k_3d, random_state=42)\n",
    "final_labels_3d = kmeans_final_3d.fit_predict(embedding_3d)\n",
    "\n",
    "# 5) Attach new 3D clusters to data_merged\n",
    "data_merged['Cluster_3D'] = final_labels_3d\n",
    "\n",
    "# Create a \"Severity_Index_3D\" by ordering clusters on mean predicted health index\n",
    "cluster_map_3d = (\n",
    "    data_merged\n",
    "    .groupby('Cluster_3D')['Predicted_Health_Index']\n",
    "    .mean()\n",
    "    .sort_values()\n",
    "    .reset_index()\n",
    ")\n",
    "cluster_map_3d['Severity_Index_3D'] = range(1, len(cluster_map_3d)+1)\n",
    "mapping_3d = cluster_map_3d.set_index('Cluster_3D')['Severity_Index_3D'].to_dict()\n",
    "data_merged['Severity_Index_3D'] = data_merged['Cluster_3D'].map(mapping_3d)\n",
    "\n",
    "# 6) 3D scatter plot (color-coded by Severity_Index_3D)\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter_3d = ax.scatter(\n",
    "    embedding_3d[:, 0],\n",
    "    embedding_3d[:, 1],\n",
    "    embedding_3d[:, 2],\n",
    "    c=data_merged['Severity_Index_3D'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "ax.set_title(f\"3D Clustering (K={best_k_3d}) - {'UMAP' if embedding_3d is umap_3d_results else 't-SNE'}\")\n",
    "ax.set_xlabel(\"Dimension 1\")\n",
    "ax.set_ylabel(\"Dimension 2\")\n",
    "ax.set_zlabel(\"Dimension 3\")\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = plt.colorbar(scatter_3d, ax=ax, fraction=0.03, pad=0.09)\n",
    "cbar.set_label(\"Severity_Index_3D\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 7) Optional: If you want a quick stats check across clusters in 3D:\n",
    "#    We can re-use the cluster_scores function, but now pass embedding_3d + final_labels_3d\n",
    "sil_3d_final, ch_3d_final, db_3d_final = cluster_scores(embedding_3d, final_labels_3d)\n",
    "print(f\"Final 3D KMeans (K={best_k_3d}): silhouette={sil_3d_final:.3f}, \"\n",
    "      f\"CH={ch_3d_final:.1f}, DB={db_3d_final:.3f}\")\n",
    "\n",
    "print(\"---- 3D clustering with expanded metrics complete ----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
