{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health Severity Index Implementation Using TabNet in Unsupervised Mode\n",
    " \n",
    "In this script, we'll implement the health severity index using TabNet, following my project plan. We'll perform the necessary preprocessing, train the TabNet model, and integrate explainable AI techniques using SHAP to interpret the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script, we'll implement a method to generate a health severity index using TabNet in unsupervised mode. We'll leverage TabNet's ability to learn meaningful representations from tabular data without labels. By clustering these representations, we can derive a severity index that reflects different levels of health status among patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\imran\\miniconda3\\envs\\tf_gpu_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import gc\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TabNet library\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Explainable AI libraries\n",
    "import shap\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Load Patient Data With Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded patient data with sequences.\n"
     ]
    }
   ],
   "source": [
    "# Load patient data with sequences\n",
    "patient_data = pd.read_pickle('Data/patient_data_sequences.pkl')\n",
    "print(\"Loaded patient data with sequences.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Code Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load code mappings\n",
    "code_mappings = pd.read_csv('Data/code_mappings.csv')\n",
    "code_to_id = dict(zip(code_mappings['UNIQUE_CODE'], code_mappings['CODE_ID']))\n",
    "id_to_code = dict(zip(code_mappings['CODE_ID'], code_mappings['UNIQUE_CODE']))\n",
    "num_codes = len(code_to_id)\n",
    "print(f\"Number of unique codes: {num_codes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate Embeddings for Each Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding dimension\n",
    "embedding_dim = 128  # Adjust as needed\n",
    "\n",
    "# Initialize embeddings randomly\n",
    "np.random.seed(42)\n",
    "code_embeddings = np.random.normal(size=(num_codes, embedding_dim))\n",
    "\n",
    "# Function to get embedding for a code ID\n",
    "def get_code_embedding(code_id):\n",
    "    return code_embeddings[code_id]\n",
    "\n",
    "# Function to aggregate embeddings for a patient\n",
    "def aggregate_patient_embeddings(visits):\n",
    "    all_code_ids = [code_id for visit in visits for code_id in visit]\n",
    "    if not all_code_ids:\n",
    "        return np.zeros(embedding_dim)\n",
    "    embeddings = np.array([get_code_embedding(code_id) for code_id in all_code_ids])\n",
    "    mean_embedding = embeddings.mean(axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "# Aggregate embeddings for each patient\n",
    "patient_embeddings = np.array([\n",
    "    aggregate_patient_embeddings(row['SEQUENCE']) for _, row in patient_data.iterrows()\n",
    "])\n",
    "\n",
    "print(\"Aggregated embeddings for all patients.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Demographic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select demographic features\n",
    "demographic_features = ['Id', 'AGE', 'DECEASED', 'GENDER', 'RACE', 'ETHNICITY',\n",
    "                        'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME']\n",
    "\n",
    "demographics = patient_data[demographic_features]\n",
    "print(\"Prepared demographic features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Features for TabNet\n",
    " \n",
    " Since TabNet can handle categorical features natively, we'll label and encode the categorical variables instead of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy demographics to avoid modifying the original DataFrame\n",
    "demographics_tabnet = demographics.copy()\n",
    "\n",
    "# Label encode categorical variables\n",
    "categorical_columns = ['GENDER', 'RACE', 'ETHNICITY']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    demographics_tabnet[col] = le.fit_transform(demographics_tabnet[col].astype(str))\n",
    "\n",
    "# Fill missing values if any\n",
    "demographics_tabnet.fillna(0, inplace=True)\n",
    "\n",
    "# Combine embeddings and demographics\n",
    "features = pd.DataFrame(patient_embeddings)\n",
    "features.columns = [f'embedding_{i}' for i in range(embedding_dim)]\n",
    "features.reset_index(drop=True, inplace=True)\n",
    "demographics_tabnet.reset_index(drop=True, inplace=True)\n",
    "data = pd.concat([demographics_tabnet, features], axis=1)\n",
    "\n",
    "print(\"Combined embeddings and demographics for TabNet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Representation Learning with TabNet\n",
    "#### 1. Prepare Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (exclude 'Id')\n",
    "feature_columns = data.columns.drop(['Id'])\n",
    "\n",
    "# Identify categorical column indices\n",
    "categorical_columns_indices = [data.columns.get_loc(col) for col in categorical_columns]\n",
    "\n",
    "# Prepare data for TabNet\n",
    "X = data[feature_columns].values\n",
    "print(\"Prepared data for TabNet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Standardize Numerical Features\n",
    "TabNet can handle raw numerical features, but standardizing can sometimes improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numerical features (excluding categorical columns)\n",
    "numerical_columns = list(set(feature_columns) - set(categorical_columns))\n",
    "numerical_columns_indices = [data.columns.get_loc(col) for col in numerical_columns]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[:, numerical_columns_indices] = scaler.fit_transform(X[:, numerical_columns_indices])\n",
    "print(\"Standardized numerical features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Initialize and Train TabNet Pretrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TabNet Pretrainer\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    n_d=64,\n",
    "    n_a=64,\n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    n_independent=2,\n",
    "    n_shared=2,\n",
    "    lambda_sparse=1e-4,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax',  # \"sparsemax\"\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "max_epochs = 1000\n",
    "patience = 50  # Early stopping patience\n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train=X,\n",
    "    eval_set=[X],\n",
    "    pretraining_ratio=0.8,\n",
    "    max_epochs=max_epochs,\n",
    "    patience=patience,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(\"Unsupervised pretraining completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering and Severity Index Generation\n",
    "#### 1. Extract Learned Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings from the trained model\n",
    "embedded_representation = unsupervised_model.transform(X)\n",
    "print(f\"Extracted embedded representations with shape: {embedded_representation.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Perform Clustering on Learned Representations\n",
    "We'll use KMeans clustering to cluster the representations. The number of clusters can be adjusted based on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 10\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embedded_representation)\n",
    "\n",
    "# Add cluster labels to the data\n",
    "data['cluster'] = cluster_labels\n",
    "print(\"Performed clustering on embedded representations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Map Clusters to Severity Index\n",
    "Assuming that higher cluster labels correspond to higher severity (you may need to adjust this based on analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map clusters to severity index\n",
    "cluster_severity = {cluster: index for index, cluster in enumerate(sorted(data['cluster'].unique()))}\n",
    "data['severity_index'] = data['cluster'].map(cluster_severity)\n",
    "\n",
    "# Scale severity index to 0-10 range\n",
    "scaler_severity = StandardScaler()\n",
    "data['severity_index_scaled'] = scaler_severity.fit_transform(data[['severity_index']])  # Standardize\n",
    "data['severity_index_scaled'] = (data['severity_index_scaled'] - data['severity_index_scaled'].min()) / (data['severity_index_scaled'].max() - data['severity_index_scaled'].min()) * 10  # Scale to 0-10\n",
    "\n",
    "print(\"Mapped clusters to severity index.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use t-SNE for visualization\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "tsne_results = tsne.fit_transform(embedded_representation)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "visualization_df = pd.DataFrame()\n",
    "visualization_df['tsne_1'] = tsne_results[:, 0]\n",
    "visualization_df['tsne_2'] = tsne_results[:, 1]\n",
    "visualization_df['cluster'] = data['cluster']\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    x='tsne_1', y='tsne_2',\n",
    "    hue='cluster',\n",
    "    palette=sns.color_palette('hsv', n_clusters),\n",
    "    data=visualization_df,\n",
    "    legend='full',\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('t-SNE Visualization of Clusters')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainable AI Integration with SHAP\n",
    "Although SHAP is primarily used for supervised models, we can use it to interpret the TabNet model's feature importance even in unsupervised settings.\n",
    "\n",
    "#### 1. Initialize SHAP Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unsupervised TabNet, we can use the reconstruction error as the target\n",
    "# We'll train a TabNetRegressor to predict reconstruction error for SHAP\n",
    "\n",
    "# Compute reconstruction error\n",
    "reconstructed_X = unsupervised_model.predict(X)\n",
    "reconstruction_errors = np.mean((X - reconstructed_X) ** 2, axis=1)\n",
    "\n",
    "# Train a TabNetRegressor on reconstruction errors\n",
    "tabnet_regressor = TabNetRegressor()\n",
    "tabnet_regressor.fit(\n",
    "    X_train=X,\n",
    "    y_train=reconstruction_errors,\n",
    "    max_epochs=100,\n",
    "    patience=20,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    ")\n",
    "\n",
    "print(\"Trained TabNetRegressor on reconstruction errors for SHAP analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer\n",
    "explainer = shap.Explainer(tabnet_regressor.predict, X)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# Save SHAP values\n",
    "np.save('shap_values_unsupervised.npy', shap_values.values)\n",
    "print(\"Computed and saved SHAP values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Visualize Feature Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot summary of feature importance\n",
    "shap.summary_plot(shap_values, feature_names=feature_columns, show=False)\n",
    "plt.savefig('shap_summary_plot_unsupervised.png')\n",
    "print(\"Generated SHAP summary plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Results\n",
    "#### 1. Save the Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data with severity index\n",
    "data.to_csv('patient_severity_index_tabnet.csv', index=False)\n",
    "print(\"Saved patient data with severity index to 'patient_severity_index.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Save the Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the unsupervised TabNet model\n",
    "unsupervised_model.save_model('tabnet_unsupervised_model.zip')\n",
    "print(\"Saved the unsupervised TabNet model.\")\n",
    "\n",
    "# Save the TabNetRegressor model\n",
    "tabnet_regressor.save_model('tabnet_regressor_model.zip')\n",
    "print(\"Saved the TabNetRegressor model.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
