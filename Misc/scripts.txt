# charlson_comorbidity.py
# Author: Imran Feisal  
# Date: 22/15/2024
# Description:
# This script computes the Charlson Comorbidity Index (CCI) from SNOMED codes.
# It expects a CSV mapping file with columns:
# code, coding_system, description, entity, list_name, upload_date, medcode, snomedctdescriptionid, CharlsonCategory
# It also expects a conditions dataframe with SNOMED-CT codes.
#
# The output is a DataFrame with patient IDs and their computed CCI scores.

import pandas as pd
import os

def load_cci_mapping(data_dir):
    """
    Load the Charlson mapping file.
    Expects a CSV with columns: ['code', 'CharlsonCategory', ...].
    If no CSV or the file is missing, this function might raise FileNotFoundError
    or produce an empty DataFrame (depending on your needs).
    """
    cci_filepath = os.path.join(data_dir, 'res195-comorbidity-cci-snomed.csv')
    cci_df = pd.read_csv(cci_filepath)
    # Basic cleanup or renaming if needed
    # e.g. rename columns to something standard
    # cci_df.rename(columns={'code': 'SNOMED', 'CharlsonCategory': 'CharlsonCategory'}, inplace=True)
    return cci_df

def assign_cci_weights(CharlsonCategory):
    """
    Assign Charlson weights based on category.
    As per the original Charlson Comorbidity Index:
      - Myocardial infarction, Congestive heart failure, Peripheral vascular disease,
        Cerebrovascular disease, Dementia, Chronic pulmonary disease, Connective tissue disease,
        Ulcer disease, Mild liver disease, Diabetes without end-organ damage => weight 1
      - Hemiplegia, Moderate/severe kidney disease, Diabetes with end-organ damage,
        Any tumour (solid tumor), leukemia, lymphoma => weight 2
      - Moderate or severe liver disease => weight 3
      - Metastatic solid tumour, AIDS => weight 6
    """
    category_to_weight = {
        'Myocardial infarction': 1,
        'Congestive heart failure': 1,
        'Peripheral vascular disease': 1,
        'Cerebrovascular disease': 1,
        'Dementia': 1,
        'Chronic pulmonary disease': 1,
        'Connective tissue disease': 1,
        'Ulcer disease': 1,
        'Mild liver disease': 1,
        'Diabetes without end-organ damage': 1,
        'Hemiplegia': 2,
        'Moderate or severe kidney disease': 2,
        'Diabetes with end-organ damage': 2,
        'Any tumour, leukaemia, lymphoma': 2,
        'Moderate or severe liver disease': 3,
        'Metastatic solid tumour': 6,
        'AIDS/HIV': 6
    }
    return category_to_weight.get(CharlsonCategory, 0)

def compute_cci(conditions, cci_mapping):
    """
    Compute the Charlson Comorbidity Index for each patient from a DataFrame of SNOMED-CT codes.
    
    Args:
        conditions (pd.DataFrame): Must include ['PATIENT', 'CODE'] columns where
                                   CODE is a SNOMED code (int or str).
        cci_mapping (pd.DataFrame): A CSV-based lookup with at least:
                                   ['code', 'CharlsonCategory']
                                   (Loaded from load_cci_mapping).
                                   Some codes may be missing from the CSV.
    
    Returns:
        pd.DataFrame: A DataFrame with columns ['PATIENT', 'CharlsonIndex'].
                      If a patient has no mapped comorbidities, CharlsonIndex = 0.
    """

    # 1. Define a fallback dictionary for SNOMED -> CharlsonCategory
    SNOMED_TO_CHARLSON = {
        #
        # MYOCARDIAL INFARCTION (weight 1)
        #
        22298006: "Myocardial infarction",      # "Myocardial infarction (disorder)"
        401303003: "Myocardial infarction",     # "Acute ST segment elevation myocardial infarction"
        401314000: "Myocardial infarction",     # "Acute non-ST segment elevation myocardial infarction"
        129574000: "Myocardial infarction",     # "Postoperative myocardial infarction (disorder)"
        #
        # CONGESTIVE HEART FAILURE (weight 1)
        #
        88805009: "Congestive heart failure",   # "Chronic congestive heart failure (disorder)"
        84114007: "Congestive heart failure",   # "Heart failure (disorder)"
        #
        # PERIPHERAL VASCULAR DISEASE (weight 1)
        #
        # -- None in your list match typical “peripheral vascular disease” codes --
        #
        # CEREBROVASCULAR DISEASE (weight 1)
        #
        230690007: "Cerebrovascular disease",   # "Cerebrovascular accident (disorder)"
        #
        # DEMENTIA (weight 1)
        #
        26929004: "Dementia",                   # "Alzheimer's disease (disorder)"
        230265002: "Dementia",                  # "Familial Alzheimer's disease of early onset (disorder)"
        #
        # CHRONIC PULMONARY DISEASE (weight 1)
        #
        185086009: "Chronic pulmonary disease", # "Chronic obstructive bronchitis (disorder)"
        87433001:  "Chronic pulmonary disease", # "Pulmonary emphysema (disorder)"
        195967001: "Chronic pulmonary disease", # "Asthma (disorder)" – (Some include chronic asthma under COPD)
        233678006: "Chronic pulmonary disease", # "Childhood asthma (disorder)"
        #
        # CONNECTIVE TISSUE DISEASE (weight 1)
        #
        69896004: "Connective tissue disease",  # "Rheumatoid arthritis (disorder)"
        200936003: "Connective tissue disease", # "Lupus erythematosus (disorder)"
        #
        # ULCER DISEASE (weight 1)
        #
        # -- None in your list specifically match “peptic ulcer disease” --
        #
        # MILD LIVER DISEASE (weight 1)
        #
        128302006: "Mild liver disease",        # "Chronic hepatitis C (disorder)" 
        61977001:  "Mild liver disease",        # "Chronic type B viral hepatitis (disorder)"
        #
        # DIABETES WITHOUT END-ORGAN DAMAGE (weight 1)
        #
        44054006: "Diabetes without end-organ damage",  # "Diabetes mellitus type 2 (disorder)"
        #
        # DIABETES WITH END-ORGAN DAMAGE (weight 2)
        #
        368581000119106: "Diabetes with end-organ damage",  # "Neuropathy due to type 2 diabetes mellitus"
        422034002:        "Diabetes with end-organ damage",  # "Retinopathy due to type 2 diabetes mellitus"
        127013003:        "Diabetes with end-organ damage",  # "Disorder of kidney due to diabetes mellitus"
        90781000119102:   "Diabetes with end-organ damage",  # "Microalbuminuria due to type 2 diabetes mellitus"
        157141000119108:  "Diabetes with end-organ damage",  # "Proteinuria due to type 2 diabetes mellitus"
        60951000119105:   "Diabetes with end-organ damage",  # "Blindness due to type 2 diabetes mellitus"
        97331000119101:   "Diabetes with end-organ damage",  # "Macular edema & retinopathy due to T2DM"
        1501000119109:    "Diabetes with end-organ damage",  # "Proliferative retinopathy due to T2DM"
        1551000119108:    "Diabetes with end-organ damage",  # "Nonproliferative retinopathy due to T2DM"
        #
        # HEMIPLEGIA or PARAPLEGIA (weight 2)
        #
        # -- None in your list appear to indicate hemiplegia or paraplegia, 
        #    e.g. “cerebral palsy” is not typically counted as hemiplegia. 
        #
        # MODERATE OR SEVERE KIDNEY DISEASE (weight 2)
        #
        # Some references only count CKD stage 3 or worse. 
        # The user had stage 1 & 2 included, so we’ll keep that approach consistent:
        431855005: "Moderate or severe kidney disease",  # "CKD stage 1 (disorder)"
        431856006: "Moderate or severe kidney disease",  # "CKD stage 2 (disorder)"
        433144002: "Moderate or severe kidney disease",  # "CKD stage 3 (disorder)"
        431857002: "Moderate or severe kidney disease",  # "CKD stage 4 (disorder)"
        46177005:  "Moderate or severe kidney disease",  # "End-stage renal disease (disorder)"
        129721000119106: "Moderate or severe kidney disease",  # "Acute renal failure on dialysis (disorder)"
        #
        # ANY TUMOUR (solid tumor), LEUKEMIA, LYMPHOMA (weight 2)
        #
        254637007: "Any tumour, leukaemia, lymphoma",  # "Non-small cell lung cancer (disorder)"
        254632001: "Any tumour, leukaemia, lymphoma",  # "Small cell carcinoma of lung (disorder)"
        93761005:  "Any tumour, leukaemia, lymphoma",  # "Primary malignant neoplasm of colon"
        363406005: "Any tumour, leukaemia, lymphoma",  # "Malignant neoplasm of colon"
        109838007: "Any tumour, leukaemia, lymphoma",  # "Overlapping malignant neoplasm of colon"
        126906006: "Any tumour, leukaemia, lymphoma",  # "Neoplasm of prostate (disorder)"
        92691004:  "Any tumour, leukaemia, lymphoma",  # "Carcinoma in situ of prostate"
        254837009: "Any tumour, leukaemia, lymphoma",  # "Malignant neoplasm of breast"
        109989006: "Any tumour, leukaemia, lymphoma",  # "Multiple myeloma (disorder)"
        93143009:  "Any tumour, leukaemia, lymphoma",  # "Leukemia disease (disorder)"
        91861009:  "Any tumour, leukaemia, lymphoma",  # "Acute myeloid leukemia (disorder)"
        #
        # MODERATE OR SEVERE LIVER DISEASE (weight 3)
        #
        # -- None in your list mention cirrhosis or advanced hepatic failure 
        #    that we'd classify as 'moderate/severe liver disease'.
        #
        # METASTATIC SOLID TUMOUR (weight 6)
        #
        94503003: "Metastatic solid tumour",    # "Metastatic malignant neoplasm to prostate"
        94260004: "Metastatic solid tumour",    # "Metastatic malignant neoplasm to colon"
        #
        # AIDS/HIV (weight 6)
        #
        62479008: "AIDS/HIV",   # "Acquired immune deficiency syndrome (disorder)"
        86406008: "AIDS/HIV",   # "Human immunodeficiency virus infection (disorder)"
    }


    # 2. Merge conditions with cci_mapping on SNOMED code (left_on='CODE', right_on='code')
    #    This way if a code is present in the CSV, it overrides or supplies CharlsonCategory
    merged = conditions.merge(
        cci_mapping[['code', 'CharlsonCategory']],
        how='left',
        left_on='CODE',
        right_on='code'
    )

    # 3. Fallback: For rows where the CSV didn't provide a CharlsonCategory, try the SNOMED_TO_CHARLSON dict
    #    If neither the CSV nor the dict have it, it remains None/NaN
    def fallback_category(row):
        if pd.notna(row['CharlsonCategory']):
            return row['CharlsonCategory']
        else:
            # Attempt dictionary lookup
            return SNOMED_TO_CHARLSON.get(row['CODE'], None)

    merged['CharlsonCategory'] = merged.apply(fallback_category, axis=1)

    # 4. Compute the Charlson weight for each row
    merged['CCI_Weight'] = merged['CharlsonCategory'].apply(assign_cci_weights)

    # 5. For each patient, sum the unique categories.
    #    i.e. if a patient has multiple codes in the same category, only count once.
    #    We do this by grouping on (PATIENT, CharlsonCategory) and taking the max weight
    #    Then summing across categories for each patient
    patient_cci = (
        merged
        .groupby(['PATIENT', 'CharlsonCategory'])['CCI_Weight']
        .max()
        .reset_index()
    )

    patient_cci_sum = (
        patient_cci
        .groupby('PATIENT')['CCI_Weight']
        .sum()
        .reset_index()
    )

    # Rename column to match the expected return type
    patient_cci_sum.rename(columns={'CCI_Weight': 'CharlsonIndex'}, inplace=True)

    return patient_cci_sum



# data_exploration.py

import pandas as pd
import os

def load_data(data_dir):
    """Load data for exploration."""
    conditions = pd.read_csv(os.path.join(data_dir, 'conditions.csv'))
    encounters = pd.read_csv(os.path.join(data_dir, 'encounters.csv'))
    medications = pd.read_csv(os.path.join(data_dir, 'medications.csv'))
    observations = pd.read_csv(os.path.join(data_dir, 'observations.csv'))
    patients = pd.read_csv(os.path.join(data_dir, 'patients.csv'))
    procedures = pd.read_csv(os.path.join(data_dir, 'procedures.csv'))
    return conditions, encounters, medications, observations, patients, procedures

def explore_conditions(conditions):
    """Explore conditions data."""
    print("Conditions Data Sample:")
    print(conditions.head())
    print("\nUnique Condition Descriptions:")
    print(conditions['DESCRIPTION'].unique())

def explore_encounters(encounters):
    """Explore encounters data."""
    print("Encounters Data Sample:")
    print(encounters.head())
    print("\nEncounter Classes:")
    print(encounters['ENCOUNTERCLASS'].unique())

def explore_medications(medications):
    """Explore medications data."""
    print("Medications Data Sample:")
    print(medications.head())
    print("\nUnique Medication Descriptions:")
    print(medications['DESCRIPTION'].unique())

def explore_observations(observations):
    """Explore observations data."""
    print("Observations Data Sample:")
    print(observations.head())
    print("\nUnique Observation Descriptions:")
    print(observations['DESCRIPTION'].unique())
    
def explore_patients(patients):
    """Explore patients data."""
    print("Patients Data Sample:")
    #print(patients.head())
    #patients.head().to_csv("patients_sample.csv")
    
def explore_procedures(procedures):
    """Explore procedures data."""
    print("Procedures Data Sample:")
    print(procedures.head())

def main():
    data_dir =  'Data'  
    #conditions, encounters, medications, observations, patients, procedures = load_data(data_dir)
    patients = load_data(data_dir)
    patients.head().to_csv("patients_sample.csv")


    #explore_conditions(conditions)
    #explore_encounters(encounters)
    #explore_medications(medications)
    #explore_observations(observations)
    explore_patients(patients)
    #explore_procedures(procedures)

if __name__ == '__main__':
    main()

# data_preprocessing.py
# Author: Imran Feisal 
# Date: 31/10/2024
# Description:
# This script loads Synthea data from CSV files, 
# enhances feature extraction from patient demographics,
# handles missing data more robustly,
# aggregates codes from conditions, medications, procedures, and observations, 
# builds sequences of visits for each patient, and saves the processed data for modeling.

import pandas as pd
import numpy as np
from datetime import datetime
import os
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------
# 1. Load Data
# ------------------------------

def load_data(data_dir):
    """
    Load Synthea data from CSV files and preprocess patient demographics.

    Args:
        data_dir (str): Directory where Synthea CSV files are stored.

    Returns:
        patients (pd.DataFrame): Processed patient demographics data.
        encounters (pd.DataFrame): Processed encounters data.
    """
    # Load Patients Data
    patients = pd.read_csv(os.path.join(data_dir, 'patients.csv'), usecols=[
        'Id', 'BIRTHDATE', 'DEATHDATE', 'GENDER', 'RACE', 'ETHNICITY',
        'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME', 'MARITAL'
    ])

    # Convert 'BIRTHDATE' and 'DEATHDATE' to datetime format
    patients['BIRTHDATE'] = pd.to_datetime(patients['BIRTHDATE'], errors='coerce')
    patients['DEATHDATE'] = pd.to_datetime(patients['DEATHDATE'], errors='coerce')

    # Check for future birthdates and deaths before births
    patients = patients[patients['BIRTHDATE'] <= patients['BIRTHDATE'].max()]
    patients = patients[(patients['DEATHDATE'].isnull()) | (patients['DEATHDATE'] >= patients['BIRTHDATE'])]

    # Calculate Age using the latest date in encounters as reference
    encounters_dates = pd.read_csv(os.path.join(data_dir, 'encounters.csv'), usecols=['START', 'STOP'])
    encounters_dates['START'] = pd.to_datetime(encounters_dates['START']).dt.tz_localize(None)
    latest_date = encounters_dates['START'].max()
    patients['AGE'] = (latest_date - patients['BIRTHDATE']).dt.days / 365.25
    patients['AGE'] = patients['AGE'].fillna(0)

    # Calculate if patient is deceased
    patients['DECEASED'] = patients['DEATHDATE'].notnull().astype(int)

    # Calculate age at death
    patients['AGE_AT_DEATH'] = ((patients['DEATHDATE'] - patients['BIRTHDATE']).dt.days / 365.25).fillna(patients['AGE'])

    # Drop unnecessary columns
    patients.drop(columns=['BIRTHDATE', 'DEATHDATE'], inplace=True)

    # Handle missing data using median imputation for numerical features
    numerical_features = ['HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME']
    for col in numerical_features:
        patients[col].fillna(patients[col].median(), inplace=True)

    # Handle missing data for categorical features by creating an 'Unknown' category
    categorical_features = ['GENDER', 'RACE', 'ETHNICITY', 'MARITAL']
    for col in categorical_features:
        patients[col].fillna('Unknown', inplace=True)
        patients[col] = patients[col].replace('', 'Unknown')

    # Load Encounters Data (Visits)
    encounters = pd.read_csv(os.path.join(data_dir, 'encounters.csv'), usecols=[
        'Id', 'PATIENT', 'ENCOUNTERCLASS', 'START', 'STOP', 'REASONCODE', 'REASONDESCRIPTION'
    ])

    # Convert START and STOP to datetime without timezone
    encounters['START'] = pd.to_datetime(encounters['START']).dt.tz_localize(None)
    encounters['STOP'] = pd.to_datetime(encounters['STOP']).dt.tz_localize(None) 

    # Sort encounters by patient and start date
    encounters.sort_values(by=['PATIENT', 'START'], inplace=True)

    logger.info("Data loaded and preprocessed successfully.")

    return patients, encounters

# ------------------------------
# 2. Prepare Visit-Level Data
# ------------------------------

def aggregate_codes(data_dir):
    """
    Aggregate codes from conditions, medications, procedures, and observations.

    Args:
        data_dir (str): Directory where Synthea CSV files are stored.

    Returns:
        codes (pd.DataFrame): Aggregated codes with unified code system.
        code_to_id (dict): Mapping from UNIQUE_CODE to integer IDs.
        id_to_code (dict): Reverse mapping from IDs to UNIQUE_CODE.
    """
    # Load data
    conditions = pd.read_csv(os.path.join(data_dir, 'conditions.csv'), usecols=[
        'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION'
    ])
    medications = pd.read_csv(os.path.join(data_dir, 'medications.csv'), usecols=[
        'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION'
    ])
    procedures = pd.read_csv(os.path.join(data_dir, 'procedures.csv'), usecols=[
        'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION'
    ])
    observations = pd.read_csv(os.path.join(data_dir, 'observations.csv'), usecols=[
        'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION', 'VALUE', 'UNITS'
    ])

    # Combine all codes into a single DataFrame
    conditions['TYPE'] = 'condition'
    medications['TYPE'] = 'medication'
    procedures['TYPE'] = 'procedure'
    observations['TYPE'] = 'observation'

    codes = pd.concat([conditions, medications, procedures, observations], ignore_index=True)

    # Handle missing codes
    codes['CODE'] = codes['CODE'].fillna('UNKNOWN')

    # Create a unified code system
    codes['UNIQUE_CODE'] = codes['TYPE'] + '_' + codes['CODE'].astype(str)

    # Generate a mapping from UNIQUE_CODE to integer IDs
    unique_codes = codes['UNIQUE_CODE'].unique()
    code_to_id = {code: idx for idx, code in enumerate(unique_codes)}
    id_to_code = {idx: code for code, idx in code_to_id.items()}

    # Map codes to IDs
    codes['CODE_ID'] = codes['UNIQUE_CODE'].map(code_to_id)

    logger.info("Codes aggregated successfully.")

    return codes, code_to_id, id_to_code

# ------------------------------
# 3. Build Patient Sequences
# ------------------------------

def build_patient_sequences(encounters, codes):
    """
    Build sequences of visits for each patient.

    Args:
        encounters (pd.DataFrame): Encounters data.
        codes (pd.DataFrame): Aggregated codes.

    Returns:
        patient_sequences (dict): Mapping of patient IDs to sequences of visits.
    """
    # Create a mapping from ENCOUNTER to CODE_IDs
    encounter_code_map = codes.groupby('ENCOUNTER')['CODE_ID'].apply(list)

    # Merge encounters with codes
    encounters_with_codes = encounters[['Id', 'PATIENT']].merge(encounter_code_map, left_on='Id', right_on='ENCOUNTER', how='left')

    # Group by patient and collect sequences
    patient_sequences = encounters_with_codes.groupby('PATIENT')['CODE_ID'].apply(list).to_dict()

    logger.info("Patient sequences built successfully.")

    return patient_sequences

# ------------------------------
# 4. Save Processed Data
# ------------------------------

def save_processed_data(patients, patient_sequences, code_to_id, output_dir):
    """
    Save the processed data for modeling.

    Args:
        patients (pd.DataFrame): Patient demographics data.
        patient_sequences (dict): Patient sequences data.
        code_to_id (dict): Code to ID mapping.
        output_dir (str): Directory to save processed data.
    """
    # Convert patient_sequences to a DataFrame
    patient_sequence_df = pd.DataFrame([
        {'PATIENT': patient_id, 'SEQUENCE': visits}
        for patient_id, visits in patient_sequences.items()
    ])

    # Merge with patient demographics
    patient_data = patients.merge(patient_sequence_df, how='inner', left_on='Id', right_on='PATIENT')

    # Drop redundant 'PATIENT' column
    patient_data.drop(columns=['PATIENT'], inplace=True)

    # Save code mappings
    code_mappings = pd.DataFrame(list(code_to_id.items()), columns=['UNIQUE_CODE', 'CODE_ID'])
    code_mappings.to_csv(os.path.join(output_dir, 'code_mappings.csv'), index=False)

    # Save patient data with sequences
    patient_data.to_pickle(os.path.join(output_dir, 'patient_data_sequences.pkl'))

    logger.info("Data aggregation complete. Processed data saved.")

# ------------------------------
# Main Execution
# ------------------------------

def main():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(script_dir, 'Data')
    output_dir = data_dir  # same directory for outputs
    os.makedirs(output_dir, exist_ok=True)

    # Load data
    patients, encounters = load_data(data_dir)

    # Aggregate codes
    codes, code_to_id, id_to_code = aggregate_codes(data_dir)

    # Build patient sequences
    patient_sequences = build_patient_sequences(encounters, codes)

    # Save processed data
    save_processed_data(patients, patient_sequences, code_to_id, output_dir)

if __name__ == '__main__':
    main()
# elixhauser_comorbidity.py
# Author: Imran Feisal
# Date: 21/01/2025
# Description:
# This script computes the Elixhauser Comorbidity Index (ECI) from SNOMED codes.
# It expects a conditions dataframe with SNOMED-CT codes and patient IDs.
# The output is a DataFrame with patient IDs and their computed ECI scores.

import pandas as pd

def assign_eci_weights(ElixhauserCategory):
    """
    Assign weights for the Elixhauser Comorbidity Index based on the Van Walraven method.
    """
    category_to_weight = {
        'Congestive heart failure': 7,
        'Cardiac arrhythmias': 5,
        'Valvular disease': 4,
        'Pulmonary circulation disorders': 6,
        'Peripheral vascular disorders': 2,
        'Hypertension, uncomplicated': -1,
        'Hypertension, complicated': 0,
        'Paralysis': 7,
        'Other neurological disorders': 6,
        'Chronic pulmonary disease': 3,
        'Diabetes, uncomplicated': 0,
        'Diabetes, complicated': 7,
        'Hypothyroidism': 0,
        'Renal failure': 5,
        'Liver disease': 11,
        'Peptic ulcer disease': 0,
        'AIDS/HIV': 0,
        'Lymphoma': 9,
        'Metastatic cancer': 14,
        'Solid tumour without metastasis': 8,
        'Rheumatoid arthritis/collagen vascular diseases': 4,
        'Coagulopathy': 11,
        'Obesity': 0,
        'Weight loss': 6,
        'Fluid and electrolyte disorders': 5,
        'Blood loss anaemia': 3,
        'Deficiency anaemias': 0,
        'Alcohol abuse': 0,
        'Drug abuse': 0,
        'Psychoses': 0,
        'Depression': -3
    }
    return category_to_weight.get(ElixhauserCategory, 0)

def compute_eci(conditions):
    """
    Compute the Elixhauser Comorbidity Index for each patient from a DataFrame of SNOMED-CT codes.
    
    Args:
        conditions (pd.DataFrame): Must include ['PATIENT', 'CODE'] columns where
                                   CODE is a SNOMED code (int or str).
    
    Returns:
        pd.DataFrame: A DataFrame with columns ['PATIENT', 'ElixhauserIndex'].
                      If a patient has no mapped comorbidities, ElixhauserIndex = 0.
    """

    # Define the SNOMED to Elixhauser category mapping dictionary
    SNOMED_TO_ELIXHAUSER = {
        # Congestive Heart Failure
        88805009: "Congestive heart failure",
        84114007: "Congestive heart failure",

        # Cardiac Arrhythmias
        49436004: "Cardiac arrhythmias",

        # Valvular Disease
        48724000: "Valvular disease",
        91434003: "Pulmonic valve regurgitation",
        79619009: "Mitral valve stenosis",
        111287006: "Tricuspid valve regurgitation",
        49915006: "Tricuspid valve stenosis",
        60573004: "Aortic valve stenosis",
        60234000: "Aortic valve regurgitation",
        
        # Pulmonary Circulation Disorders
        65710008: "Pulmonary circulation disorders",
        706870000: "Acute pulmonary embolism",
        67782005: "Acute respiratory distress syndrome",

        # Peripheral Vascular Disorders
        698754002: "Peripheral vascular disorders",

        # Hypertension
        59621000: "Hypertension, uncomplicated",

        # Paralysis
        698754002: "Paralysis",
        128188000: "Paralysis",

        # Other Neurological Disorders
        69896004: "Other neurological disorders",
        128613002: "Seizure disorder",

        # Chronic Pulmonary Disease
        195967001: "Chronic pulmonary disease",
        233678006: "Chronic pulmonary disease",

        # Diabetes, Complicated
        368581000119106: "Diabetes, complicated",
        422034002: "Diabetes, complicated",
        90781000119102: "Diabetes, complicated",

        # Diabetes, Uncomplicated
        44054006: "Diabetes, uncomplicated",

        # Renal Failure
        129721000119106: "Renal failure",
        433144002: "Renal failure",

        # Liver Disease
        128302006: "Liver disease",
        61977001: "Liver disease",

        # Peptic Ulcer Disease
        # (Not identified in the dataset)

        # AIDS/HIV
        62479008: "AIDS/HIV",
        86406008: "AIDS/HIV",

        # Lymphoma
        93143009: "Lymphoma",

        # Metastatic Cancer
        94503003: "Metastatic cancer",
        94260004: "Metastatic cancer",

        # Solid Tumour Without Metastasis
        126906006: "Solid tumour without metastasis",
        254637007: "Solid tumour without metastasis",

        # Rheumatoid Arthritis / Collagen Vascular Diseases
        69896004: "Rheumatoid arthritis/collagen vascular diseases",
        200936003: "Rheumatoid arthritis/collagen vascular diseases",

        # Coagulopathy
        234466008: "Coagulopathy",

        # Obesity
        408512008: "Obesity",
        162864005: "Obesity",

        # Weight Loss
        278860009: "Weight loss",

        # Fluid and Electrolyte Disorders
        389087006: "Fluid and electrolyte disorders",

        # Blood Loss Anaemia
        # (Not identified in the dataset)

        # Deficiency Anaemias
        271737000: "Deficiency anaemias",

        # Alcohol Abuse
        7200002: "Alcohol abuse",

        # Drug Abuse
        6525002: "Drug abuse",

        # Psychoses
        47505003: "Psychoses",

        # Depression
        370143000: "Depression",
        36923009: "Depression",
    }


    # Map SNOMED codes to Elixhauser categories
    conditions['ElixhauserCategory'] = conditions['CODE'].map(SNOMED_TO_ELIXHAUSER)

    # Assign weights based on categories
    conditions['ECI_Weight'] = conditions['ElixhauserCategory'].apply(assign_eci_weights)

    # For each patient, sum the unique weights for each category
    patient_eci = (
        conditions
        .groupby(['PATIENT', 'ElixhauserCategory'])['ECI_Weight']
        .max()
        .reset_index()
    )

    patient_eci_sum = (
        patient_eci
        .groupby('PATIENT')['ECI_Weight']
        .sum()
        .reset_index()
    )

    # Rename the result column for clarity
    patient_eci_sum.rename(columns={'ECI_Weight': 'ElixhauserIndex'}, inplace=True)

    return patient_eci_sum

# health_index.py
# Author: Imran Feisal
# Date: 31/10/2024
# Description:
# Calculate the composite health index for each patient by grouping SNOMED CT codes
# into clinically meaningful categories, assigning weights, and calculating a health index.

import pandas as pd
import numpy as np
import os
import logging
from sklearn.decomposition import PCA

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------
# 1. Load Processed Data
# ------------------------------

def load_processed_data(output_dir):
    """
    Load the processed patient data and code mappings.

    Args:
        output_dir (str): Directory where processed data is stored.

    Returns:
        patient_data (pd.DataFrame): Patient data with sequences.
        code_mappings (pd.DataFrame): Code mappings.
    """
    patient_data = pd.read_pickle(os.path.join(output_dir, 'patient_data_sequences.pkl'))
    code_mappings = pd.read_csv(os.path.join(output_dir, 'code_mappings.csv'))
    logger.info("Processed data loaded successfully.")
    return patient_data, code_mappings

# ------------------------------
# 2. Calculate Health Indicators
# ------------------------------

def calculate_health_indicators(patient_data, data_dir):
    """
    Calculate health indicators for each patient.

    Args:
        patient_data (pd.DataFrame): Patient data with sequences.
        data_dir (str): Directory where raw data is stored.

    Returns:
        patient_data (pd.DataFrame): Patient data with health indicators.
    """
    # Load additional data needed
    conditions = pd.read_csv(os.path.join(data_dir, 'conditions.csv'), usecols=[
        'PATIENT', 'CODE', 'DESCRIPTION'
    ])
    encounters = pd.read_csv(os.path.join(data_dir, 'encounters.csv'), usecols=[
        'Id', 'PATIENT', 'ENCOUNTERCLASS'
    ])
    medications = pd.read_csv(os.path.join(data_dir, 'medications.csv'), usecols=[
        'PATIENT', 'CODE', 'DESCRIPTION'
    ])
    observations = pd.read_csv(os.path.join(data_dir, 'observations.csv'), usecols=[
        'PATIENT', 'DESCRIPTION', 'VALUE', 'UNITS'
    ])

    # -----------------------------------------
    # 2.1 Calculate Comorbidity Score using SNOMED CT groups
    # -----------------------------------------

    # Define SNOMED CT code groups with actual codes
    snomed_groups = {
        'Cardiovascular Diseases': ['53741008', '445118002', '59621000', '22298006', '56265001'],
        'Respiratory Diseases': ['19829001', '233604007', '118940003', '409622000', '13645005'],
        'Diabetes': ['44054006', '73211009', '46635009', '190330002'],
        'Cancer': ['363346000', '254637007', '363406005', '254632001'],
        'Chronic Kidney Disease': ['709044004', '90708001', '46177005'],
        'Neurological Disorders': ['230690007', '26929004', '193003'],
        # Add more groups and codes as needed
    }

    # Assign weights to groups based on clinical significance
    group_weights = {
        'Cardiovascular Diseases': 3,
        'Respiratory Diseases': 2,
        'Diabetes': 2,
        'Cancer': 3,
        'Chronic Kidney Disease': 2,
        'Neurological Disorders': 1.5,
        'Other': 1  # Assign a default weight to other conditions
        # Adjust weights as appropriate
    }

    # Function to find group for a given code
    def find_group(code, snomed_groups):
        for group, codes in snomed_groups.items():
            if str(code) in codes:
                return group
        return 'Other'

    # Map codes to groups
    conditions['Group'] = conditions['CODE'].apply(lambda x: find_group(x, snomed_groups))

    # Assign weights to conditions
    conditions['Group_Weight'] = conditions['Group'].map(group_weights)
    conditions['Group_Weight'] = conditions['Group_Weight'].fillna(1)  # Assign default weight if not found

    # Sum comorbidity weights per patient
    comorbidity_scores = conditions.groupby('PATIENT')['Group_Weight'].sum().reset_index()
    comorbidity_scores.rename(columns={'Group_Weight': 'Comorbidity_Score'}, inplace=True)

    # -----------------------------------------
    # 2.2 Calculate Hospitalizations Count
    # -----------------------------------------
    # Filter encounters for inpatient class
    hospitalizations = encounters[encounters['ENCOUNTERCLASS'] == 'inpatient']
    hospitalizations_count = hospitalizations.groupby('PATIENT').size().reset_index(name='Hospitalizations_Count')

    # -----------------------------------------
    # 2.3 Calculate Medications Count
    # -----------------------------------------
    medications_count = medications.groupby('PATIENT')['CODE'].nunique().reset_index(name='Medications_Count')

    # -----------------------------------------
    # 2.4 Calculate Abnormal Observations Count
    # -----------------------------------------
    # Define thresholds for abnormal observations based on clinical guidelines
    observation_thresholds = {
        'Systolic Blood Pressure': {'min': 90, 'max': 120},
        'Diastolic Blood Pressure': {'min': 60, 'max': 80},
        'Body Mass Index': {'min': 18.5, 'max': 24.9},
        'Blood Glucose Level': {'min': 70, 'max': 99},
        'Heart Rate': {'min': 60, 'max': 100},
        # Add more observations with thresholds as needed
    }

    # Map observation descriptions to standardized names
    observation_mappings = {
        'Systolic Blood Pressure': ['Systolic Blood Pressure'],
        'Diastolic Blood Pressure': ['Diastolic Blood Pressure'],
        'Body Mass Index': ['Body mass index (BMI) [Ratio]'],
        'Blood Glucose Level': ['Glucose [Mass/volume] in Blood'],
        'Heart Rate': ['Heart rate'],
        # Add more mappings as needed
    }

    # Normalize observation descriptions
    observations['DESCRIPTION'] = observations['DESCRIPTION'].str.strip()

    # Convert 'VALUE' to numeric, coercing errors to NaN
    observations['VALUE'] = pd.to_numeric(observations['VALUE'], errors='coerce')

    # Initialize abnormal flag to zero
    observations['IS_ABNORMAL'] = 0

    # Iterate through each standardized observation and thresholds
    for standard_desc, desc_list in observation_mappings.items():
        thresholds = observation_thresholds.get(standard_desc, {})
        min_val = thresholds.get('min', -np.inf)
        max_val = thresholds.get('max', np.inf)
        mask = observations['DESCRIPTION'].isin(desc_list)

        # Apply threshold checks
        observations.loc[
            mask & observations['VALUE'].notna() & (
                (observations['VALUE'] < min_val) |
                (observations['VALUE'] > max_val)
            ),
            'IS_ABNORMAL'
        ] = 1

    # Group by patient to count abnormal observations
    abnormal_observations_count = observations.groupby('PATIENT')['IS_ABNORMAL'].sum().reset_index()
    abnormal_observations_count.rename(columns={'IS_ABNORMAL': 'Abnormal_Observations_Count'}, inplace=True)

    # -----------------------------------------
    # 2.5 Merge Counts into patient_data
    # -----------------------------------------
    # Set the index of patient_data to 'Id' for efficient merging
    patient_data.set_index('Id', inplace=True)

    # Merge Comorbidity Score
    patient_data = patient_data.merge(comorbidity_scores.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Merge Hospitalizations Count
    patient_data = patient_data.merge(hospitalizations_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Merge Medications Count
    patient_data = patient_data.merge(medications_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Merge Abnormal Observations Count
    patient_data = patient_data.merge(abnormal_observations_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Reset index to have 'Id' as a column again
    patient_data.reset_index(inplace=True)

    # -----------------------------------------
    # 2.6 Fill NaN values appropriately
    # -----------------------------------------
    indicators = ['Comorbidity_Score', 'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count']
    patient_data[indicators] = patient_data[indicators].fillna(0)

    logger.info("Health indicators calculated successfully.")

    return patient_data

# ------------------------------
# 3. Calculate Composite Health Index
# ------------------------------

def calculate_health_index(patient_data):
    """
    Calculate the composite health index using PCA for weights.

    Args:
        patient_data (pd.DataFrame): Patient data with health indicators.

    Returns:
        patient_data (pd.DataFrame): Patient data with health index.
    """
    # Define indicators
    indicators = ['AGE', 'Comorbidity_Score', 'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count']

    # Normalize indicators using Robust Scaler to reduce the influence of outliers
    from sklearn.preprocessing import RobustScaler
    scaler = RobustScaler()
    scaled_indicators = scaler.fit_transform(patient_data[indicators])

    # Perform PCA to determine weights
    pca = PCA(n_components=1)
    principal_components = pca.fit_transform(scaled_indicators)
    weights = pca.components_[0]
    weights = weights / np.sum(np.abs(weights))  # Normalize weights

    # Check PCA explained variance
    explained_variance = pca.explained_variance_ratio_[0]
    logger.info(f"PCA explained variance ratio: {explained_variance:.4f}")

    # Calculate health index
    patient_data['Health_Index'] = np.dot(scaled_indicators, weights)

    # Scale Health_Index to range 1 to 10
    min_hi = patient_data['Health_Index'].min()
    max_hi = patient_data['Health_Index'].max()
    patient_data['Health_Index'] = 1 + 9 * (patient_data['Health_Index'] - min_hi) / (max_hi - min_hi + 1e-8)

    logger.info("Composite health index calculated successfully.")

    return patient_data

# ------------------------------
# 4. Save Health Index
# ------------------------------

def save_health_index(patient_data, output_dir):
    """
    Save the patient data with health index.

    Args:
        patient_data (pd.DataFrame): Patient data with health index.
        output_dir (str): Directory to save the data.
    """
    patient_data.to_pickle(os.path.join(output_dir, 'patient_data_with_health_index.pkl'))
    logger.info("Health index calculated and saved.")

# ------------------------------
# Main Execution
# ------------------------------

def main():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(script_dir, 'Data')
    output_dir = data_dir  # same directory for outputs
    os.makedirs(output_dir, exist_ok=True)

    # Load processed data
    patient_data, code_mappings = load_processed_data(output_dir)

    # Calculate health indicators
    patient_data = calculate_health_indicators(patient_data, data_dir)

    # Calculate health index
    patient_data = calculate_health_index(patient_data)

    # Save health index
    save_health_index(patient_data, output_dir)

if __name__ == '__main__':
    main()
import os

# List of directories to read .py files from
directories = [
    '.',
    'vitai_scripts',
    'Finals',
    'Validations',
    'Explain_Xai'
]

# Initialize an empty string to hold the combined content
combined_content = ""

# Iterate over each directory in the list
for directory in directories:
    # Iterate over each file in the directory
    for file_name in os.listdir(directory):
        # Check if the file is a .py file
        if file_name.endswith('.py'):
            file_path = os.path.join(directory, file_name)
            print(f"Reading file {file_path}...")
            # Check if the file exists
            if os.path.exists(file_path):
                # Open the file and read its content
                with open(file_path, 'r', encoding='utf-8') as file:
                    combined_content += file.read() + "\n"
            else:
                print(f"File {file_path} does not exist.")

# Save the combined content to a text file
with open('scripts.txt', 'w', encoding='utf-8') as output_file:
    output_file.write(combined_content)

import pandas as pd

# Replace 'your_file.pkl' with the path to your .pkl file
file_path = 'Data/patient_data_with_health_index_cci.pkl'

# Read the .pkl file into a DataFrame
df = pd.read_pickle(file_path)


#save top 5 rows of the dataframe to a csv file named temp_composite_none_tabnet_20250113_001634.csv
df.head().to_csv('Data/hghgh.csv')

# tabnet_model.py
# Author: Imran Feisal
# Date: 31/10/2024
# Description:
# This script builds and trains a TabNet model using hyperparameter tuning,
# includes cross-validation, extracts feature importances, and saves the
# trained model and results. 
# Now accepts an output_prefix param to avoid overwriting artifacts,
# and target_col param to decide which column to predict (Health_Index or CharlsonIndex).

import numpy as np
import pandas as pd
import joblib
import os
import torch
from pytorch_tabnet.tab_model import TabNetRegressor
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, r2_score
import logging
import json
import optuna
from sklearn.preprocessing import LabelEncoder, StandardScaler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_data(output_dir, input_file):
    data_path = os.path.join(output_dir, input_file)
    if not os.path.exists(data_path):
        logger.error(f"Data file not found at {data_path}")
        raise FileNotFoundError(f"Data file not found at {data_path}")
    patient_data = pd.read_pickle(data_path)
    logger.info("Patient data loaded.")
    return patient_data

def prepare_data(patient_data, target_col='Health_Index'):
    """
    Prepare the dataset for TabNet:
      - features: columns that define the model inputs
      - target: the column we want to predict (Health_Index or CharlsonIndex)
    """
    # Feature columns
    features = patient_data[[
        'AGE','DECEASED','GENDER','RACE','ETHNICITY','MARITAL',
        'HEALTHCARE_EXPENSES','HEALTHCARE_COVERAGE','INCOME',
        'Hospitalizations_Count','Medications_Count','Abnormal_Observations_Count'
    ]].copy()

    # Target column is chosen based on `target_col`
    if target_col not in patient_data.columns:
        raise KeyError(f"Column '{target_col}' not found in patient_data!")
    target = patient_data[target_col]

    # Setup categorical columns
    categorical_columns = ['DECEASED','GENDER','RACE','ETHNICITY','MARITAL']
    cat_idxs = [i for i,col in enumerate(features.columns) if col in categorical_columns]
    cat_dims = []

    for col in categorical_columns:
        le = LabelEncoder()
        features[col] = le.fit_transform(features[col].astype(str))
        cat_dims.append(features[col].nunique())

    # Scale continuous columns
    continuous_columns = [col for col in features.columns if col not in categorical_columns]
    scaler = StandardScaler()
    features[continuous_columns] = scaler.fit_transform(features[continuous_columns])
    joblib.dump(scaler, 'tabnet_scaler.joblib')

    # Handle missing
    features.fillna(0, inplace=True)

    X = features.values
    y = target.values.reshape(-1, 1)
    logger.info(f"Data prepared for TabNet (target_col='{target_col}').")

    return X, y, cat_idxs, cat_dims, features.columns.tolist()

def objective(trial, X, y, cat_idxs, cat_dims):
    params = {
        'n_d': trial.suggest_int('n_d', 8, 64),
        'n_a': trial.suggest_int('n_a', 8, 64),
        'n_steps': trial.suggest_int('n_steps', 3, 10),
        'gamma': trial.suggest_float('gamma', 1.0, 2.0),
        'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-6, 1e-3, log=True),
        'optimizer_fn': torch.optim.Adam,
        'optimizer_params': dict(lr=trial.suggest_float('lr',1e-4,1e-2,log=True)),
        'cat_emb_dim': trial.suggest_int('cat_emb_dim',1,5),
        'n_shared': trial.suggest_int('n_shared',1,5),
        'n_independent': trial.suggest_int('n_independent',1,5),
        'device_name': 'cuda',
        'verbose': 0,
    }
    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    mse_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train_fold, X_valid_fold = X[train_idx], X[valid_idx]
        y_train_fold, y_valid_fold = y[train_idx], y[valid_idx]

        model = TabNetRegressor(cat_idxs=cat_idxs, cat_dims=cat_dims, **params)
        model.fit(
            X_train=X_train_fold, y_train=y_train_fold,
            eval_set=[(X_valid_fold, y_valid_fold)],
            eval_metric=['rmse'],
            max_epochs=50,
            patience=10,
            batch_size=4096,
            virtual_batch_size=512
        )
        preds = model.predict(X_valid_fold)
        mse = mean_squared_error(y_valid_fold, preds)
        mse_list.append(mse)
    return np.mean(mse_list)

def hyperparameter_tuning(X, y, cat_idxs, cat_dims):
    study = optuna.create_study(direction='minimize')
    study.optimize(lambda trial: objective(trial, X, y, cat_idxs, cat_dims), n_trials=7)
    logger.info(f"Best trial: {study.best_trial.params}")
    return study.best_trial.params

def train_tabnet(X_train, y_train, X_valid, y_valid, cat_idxs, cat_dims, best_params, output_prefix='tabnet'):
    optimizer_fn = torch.optim.Adam
    optimizer_params = {'lr': best_params.pop('lr')}
    best_params.update({
        'optimizer_fn': optimizer_fn,
        'optimizer_params': optimizer_params,
        'device_name': 'cuda',
        'verbose': 1
    })
    regressor = TabNetRegressor(cat_idxs=cat_idxs, cat_dims=cat_dims, **best_params)
    regressor.fit(
        X_train=X_train,
        y_train=y_train,
        eval_set=[(X_valid, y_valid)],
        eval_metric=['rmse'],
        max_epochs=200,
        patience=20,
        batch_size=8192,
        virtual_batch_size=1024
    )
    regressor.save_model(f'{output_prefix}_model')
    logger.info(f"TabNet model trained and saved -> {output_prefix}_model.zip (among others).")
    return regressor

def main(input_file='patient_data_with_health_index.pkl',
         output_prefix='tabnet',
         target_col='Health_Index'):
    """
    Args:
        input_file (str): Pickle file containing patient data
        output_prefix (str): Unique prefix to avoid overwriting model artifacts
        target_col (str): Which column to predict ('Health_Index' or 'CharlsonIndex')
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, 'Data')
    patient_data = load_data(output_dir, input_file)

    # Prepare data for the specified target column
    X, y, cat_idxs, cat_dims, feature_columns = prepare_data(patient_data, target_col=target_col)

    # Train/valid/test splits
    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)

    # Hyperparameter tuning
    best_params = hyperparameter_tuning(X_train, y_train, cat_idxs, cat_dims)
    regressor = train_tabnet(X_train, y_train, X_valid, y_valid, cat_idxs, cat_dims, best_params, output_prefix=output_prefix)

    # Evaluate on test set
    test_preds = regressor.predict(X_test)
    test_mse = mean_squared_error(y_test, test_preds)
    test_r2 = r2_score(y_test, test_preds)
    logger.info(f"Test MSE: {test_mse:.4f}")
    logger.info(f"Test R2: {test_r2:.4f}")

    # Save predictions
    # If 'Id' is present in the DF, we can map it; otherwise we do a simple index-based approach
    num_test = len(X_test)
    pred_col_name = ("Predicted_Health_Index" if target_col == "Health_Index" 
                     else "Predicted_CharlsonIndex")

    if 'Id' in patient_data.columns:
        # Take the last 'num_test' rows as test IDs
        test_ids = patient_data.iloc[-num_test:]['Id'].values
    else:
        # Fallback if not present
        test_ids = np.arange(num_test)

    predictions_df = pd.DataFrame({
        'Id': test_ids,
        pred_col_name: test_preds.flatten()
    })
    pred_csv = f'{output_prefix}_predictions.csv'
    predictions_df.to_csv(pred_csv, index=False)
    logger.info(f"TabNet predictions saved -> {pred_csv}")

    # Save metrics
    metrics = {
        "test_mse": test_mse,
        "test_r2": test_r2
    }
    metrics_file = f"{output_prefix}_metrics.json"
    with open(metrics_file, "w") as f:
        json.dump(metrics, f)
    logger.info(f"TabNet metrics saved -> {metrics_file}")

if __name__ == '__main__':
    main()

import pandas as pd

# Read the CSV file
df = pd.read_csv('Data/patients.csv')

# Get the first 5 rows
sample_df = df.head(5)

# Save the sample data to a new CSV file
sample_df.to_csv('sample_data.csv', index=False)
# vae_model.py
# Author: Imran Feisal
# Date: 31/10/2024
# Description:
# Updated script to address variable duplication warnings in TensorFlow.
# This script trains a VAE model, now accepts an input_file and output_prefix parameters
# so as to avoid overwriting model artifacts. 
# 
# UPDATe 19/01/2025 this script now saves a JSON file with final
# training and validation losses, named <output_prefix>_vae_metrics.json.

import numpy as np
import pandas as pd
import joblib
import os
import json
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import logging
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_data(output_dir, input_file):
    patient_data = pd.read_pickle(os.path.join(output_dir, input_file))
    return patient_data

def prepare_data(patient_data):
    features = patient_data[[
        'AGE', 'GENDER', 'RACE', 'ETHNICITY', 'MARITAL',
        'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME',
        'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count'
    ]].copy()

    patient_ids = patient_data['Id'].values
    categorical_features = ['GENDER', 'RACE', 'ETHNICITY', 'MARITAL']
    continuous_features = [col for col in features.columns if col not in categorical_features]

    embedding_info = {}
    input_data = {}

    for col in categorical_features:
        le = LabelEncoder()
        features[col] = le.fit_transform(features[col].astype(str))
        joblib.dump(le, f'label_encoder_{col}.joblib')
        vocab_size = features[col].nunique()
        embedding_dim = min(50, (vocab_size + 1)//2)
        embedding_info[col] = {'vocab_size': vocab_size, 'embedding_dim': embedding_dim}
        input_data[col] = features[col].values

    scaler = StandardScaler()
    scaled_continuous = scaler.fit_transform(features[continuous_features])
    joblib.dump(scaler, 'scaler_vae.joblib')
    input_data['continuous'] = scaled_continuous

    logger.info("Data prepared for VAE.")
    return input_data, embedding_info, patient_ids, continuous_features, categorical_features

def build_vae(input_dim, embedding_info, continuous_dim, latent_dim=20):
    inputs = {}
    encoded_features = []

    # Embeddings for categorical
    for col, info in embedding_info.items():
        input_cat = keras.Input(shape=(1,), name=f'input_{col}')
        embedding_layer = layers.Embedding(
            input_dim=info['vocab_size'], 
            output_dim=info['embedding_dim'], 
            name=f'embedding_{col}'
        )(input_cat)
        flat_embedding = layers.Flatten()(embedding_layer)
        inputs[f'input_{col}'] = input_cat
        encoded_features.append(flat_embedding)

    # Continuous input
    input_cont = keras.Input(shape=(continuous_dim,), name='input_continuous')
    inputs['input_continuous'] = input_cont
    encoded_features.append(input_cont)

    concatenated_features = layers.concatenate(encoded_features)
    h = layers.Dense(256, activation='relu')(concatenated_features)
    h = layers.Dense(128, activation='relu')(h)
    z_mean = layers.Dense(latent_dim, name='z_mean')(h)
    z_log_var = layers.Dense(latent_dim, name='z_log_var')(h)

    def sampling(args):
        z_mean, z_log_var = args
        epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], latent_dim))
        return z_mean + tf.exp(0.5*z_log_var)*epsilon

    z = layers.Lambda(sampling, name='z')([z_mean, z_log_var])

    # Decoder
    decoder_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')
    h_decoder = layers.Dense(128, activation='relu')(decoder_inputs)
    h_decoder = layers.Dense(256, activation='relu')(h_decoder)
    reconstructed = layers.Dense(input_dim, activation='linear')(h_decoder)

    encoder = keras.Model(inputs=inputs, outputs=[z_mean, z_log_var, z], name='encoder')
    decoder = keras.Model(inputs=decoder_inputs, outputs=reconstructed, name='decoder')

    outputs = decoder(encoder(inputs)[2])
    vae = keras.Model(inputs=inputs, outputs=outputs, name='vae')

    reconstruction_loss = tf.reduce_mean(tf.square(concatenated_features - outputs))
    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))
    vae_loss = reconstruction_loss + kl_loss
    vae.add_loss(vae_loss)
    vae.compile(optimizer='adam')

    logger.info("VAE model built.")
    return vae, encoder, decoder

def train_vae(vae, input_data, output_prefix='vae'):
    x_train = {
        f'input_{key}': value for key, value in input_data.items() 
        if key != 'continuous'
    }
    x_train['input_continuous'] = input_data['continuous']

    early_stopping = keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=10, restore_best_weights=True
    )
    checkpoint = keras.callbacks.ModelCheckpoint(
        f'{output_prefix}_best_model.h5', 
        monitor='val_loss', 
        save_best_only=True
    )

    # Fit returns a History object with training & validation losses
    history = vae.fit(
        x_train, 
        epochs=100, 
        batch_size=512, 
        validation_split=0.2, 
        callbacks=[early_stopping, checkpoint],
        verbose=1
    )
    vae.save(f'{output_prefix}_model', save_format='tf')
    logger.info(f"VAE trained and saved with prefix={output_prefix}.")

    # Extract final losses from history
    # Because of early stopping, 'val_loss' might not correspond to the final epoch
    # We take the minimal val_loss across epochs as a reference
    final_train_loss = float(history.history['loss'][-1])  # last epoch's training loss
    final_val_loss = float(min(history.history['val_loss']))  # best validation loss

    # Save them to a JSON for easier retrieval
    metrics_json = {
        "final_train_loss": final_train_loss,
        "best_val_loss": final_val_loss
    }
    with open(f"{output_prefix}_vae_metrics.json", "w") as f:
        json.dump(metrics_json, f, indent=2)
    logger.info(f"[METRICS] VAE training/validation losses saved to {output_prefix}_vae_metrics.json")

def save_latent_features(encoder, input_data, patient_ids, output_prefix='vae'):
    x_pred = {
        f'input_{key}': value for key, value in input_data.items() 
        if key != 'continuous'
    }
    x_pred['input_continuous'] = input_data['continuous']
    z_mean, _, _ = encoder.predict(x_pred)

    df = pd.DataFrame(z_mean)
    df['Id'] = patient_ids
    csv_name = f'{output_prefix}_latent_features.csv'
    df.to_csv(csv_name, index=False)
    logger.info(f"Latent features saved to {csv_name}.")

def main(input_file='patient_data_with_health_index.pkl', output_prefix='vae'):
    """
    Args:
        input_file (str): Name of the input pickle file containing patient data.
        output_prefix (str): A unique prefix for saving model artifacts 
                             (latent CSV, model files, etc.).
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, 'Data')
    patient_data = load_data(output_dir, input_file)
    input_data, embedding_info, patient_ids, continuous_features, categorical_features = prepare_data(patient_data)

    input_dim = sum(info['embedding_dim'] for info in embedding_info.values()) + len(continuous_features)
    continuous_dim = len(continuous_features)
    vae, encoder, decoder = build_vae(input_dim, embedding_info, continuous_dim)

    train_vae(vae, input_data, output_prefix=output_prefix)
    encoder.save(f'{output_prefix}_encoder', save_format='tf')
    decoder.save(f'{output_prefix}_decoder', save_format='tf')

    save_latent_features(encoder, input_data, patient_ids, output_prefix=output_prefix)

if __name__ == '__main__':
    main()

# vitai_scripts/cluster_utils.py
# Author: Imran Feisal
# Date: 21/01/2025
#
# Description:
#   Memory-optimised clustering logic and visualisation
#   for each experiment config, returning metrics for
#   K-Means and DBSCAN.

import os
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.cluster import KMeans, DBSCAN
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score
)
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
import umap.umap_ as umap

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def cluster_and_visualise(
    merged_df: pd.DataFrame,
    config_id: str,
    plots_folder: str
) -> dict:
    """
    Clusters the data using K-Means(6..9) to find the best K, plus DBSCAN.
    Saves t-SNE and UMAP plots in 'plots_folder'.
    Returns a dict with final & DBSCAN metrics.
    """
    os.makedirs(plots_folder, exist_ok=True)

    # Exclude any columns that shouldn't be used for clustering
    exclude_cols = {"Id", "Predicted_Health_Index", "Predicted_CharlsonIndex", "Predicted_ElixhauserIndex"}
    X_cols = [c for c in merged_df.columns if c not in exclude_cols]
    X = merged_df[X_cols].values
    n_rows, n_feats = X.shape
    if n_rows < 2 or n_feats < 2:
        logger.warning(f"[{config_id}] Not enough rows/features ({n_rows}x{n_feats}) -> skipping clustering.")
        return {}

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # K-Means
    best_k = None
    best_sil = -1
    for k in range(6, 10):
        km = KMeans(n_clusters=k, random_state=42)
        labels = km.fit_predict(X_scaled)
        sil = silhouette_score(X_scaled, labels)
        if sil > best_sil:
            best_sil = sil
            best_k = k

    final_km = KMeans(n_clusters=best_k, random_state=42).fit(X_scaled)
    final_labels = final_km.predict(X_scaled)
    merged_df["Cluster"] = final_labels

    final_sil = silhouette_score(X_scaled, final_labels)
    final_cal = calinski_harabasz_score(X_scaled, final_labels)
    final_dav = davies_bouldin_score(X_scaled, final_labels)

    # DBSCAN
    db = DBSCAN(eps=0.5, min_samples=5).fit(X_scaled)
    db_labels = db.labels_
    if len(set(db_labels)) < 2:
        db_sil = np.nan
        db_cal = np.nan
        db_dav = np.nan
    else:
        db_sil = silhouette_score(X_scaled, db_labels)
        db_cal = calinski_harabasz_score(X_scaled, db_labels)
        db_dav = davies_bouldin_score(X_scaled, db_labels)

    # Possibly create a 'Severity_Index' if "Predicted_Health_Index" is there
    hue_col = "Cluster"
    if "Predicted_Health_Index" in merged_df.columns:
        cluster_mean = (
            merged_df
            .groupby("Cluster")["Predicted_Health_Index"]
            .mean()
            .sort_values()
            .reset_index()
        )
        cluster_mean["Severity_Index"] = range(1, len(cluster_mean)+1)
        c_map = dict(zip(cluster_mean["Cluster"], cluster_mean["Severity_Index"]))
        merged_df["Severity_Index"] = merged_df["Cluster"].map(c_map)
        hue_col = "Severity_Index"

    # t-SNE
    tsne_2d = TSNE(n_components=2, random_state=42)
    X_tsne = tsne_2d.fit_transform(X_scaled)
    plt.figure(figsize=(7,5))
    sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1],
                    hue=merged_df[hue_col], palette="viridis")
    plt.title(f"t-SNE ({config_id}) - K={best_k}")
    tsne_path = os.path.join(plots_folder, f"tsne2d_{config_id}.png")
    plt.savefig(tsne_path, bbox_inches="tight")
    plt.close()

    # UMAP
    reducer = umap.UMAP(n_components=2, random_state=42)
    X_umap = reducer.fit_transform(X_scaled)
    plt.figure(figsize=(7,5))
    sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1],
                    hue=merged_df[hue_col], palette="viridis")
    plt.title(f"UMAP ({config_id}) - K={best_k}")
    umap_path = os.path.join(plots_folder, f"umap2d_{config_id}.png")
    plt.savefig(umap_path, bbox_inches="tight")
    plt.close()

    return {
        "config_id": config_id,
        "chosen_k": best_k,
        "final_silhouette": final_sil,
        "final_calinski": final_cal,
        "final_davies_bouldin": final_dav,
        "dbscan_silhouette": db_sil,
        "dbscan_calinski": db_cal,
        "dbscan_davies_bouldin": db_dav
    }

# vitai_scripts/data_prep.py
# Author: Imran Feisal 
# Date: 21/01/2025
#
# Description:
#   Ensures the following pickles exist in the Data/ folder:
#     1) patient_data_sequences.pkl
#     2) patient_data_with_health_index.pkl
#   Then merges:
#     - Charlson Comorbidity Index
#     - Elixhauser Comorbidity Index
#   into a single file:
#     patient_data_with_all_indices.pkl
#   containing 'Health_Index', 'CharlsonIndex', 'ElixhauserIndex', etc.
#
#   This uses:
#     data_preprocessing.py -> Preprocess
#     health_index.py       -> Compute Health Index
#     charlson_comorbidity.py
#     elixhauser_comorbidity.py
#
#   The final pickle is 'patient_data_with_all_indices.pkl'.

import os
import logging
import pandas as pd
import gc
import sys

root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(root_path)
# Root-level modules
from data_preprocessing import main as preprocess_main
from health_index import main as health_main
from charlson_comorbidity import load_cci_mapping, compute_cci
from elixhauser_comorbidity import compute_eci

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def ensure_preprocessed_data(data_dir: str) -> None:
    """
    Ensures these files exist:
      1) patient_data_sequences.pkl
      2) patient_data_with_health_index.pkl
    Then merges both Charlson & Elixhauser Indices into a single
    'patient_data_with_all_indices.pkl'.
    """
    seq_path = os.path.join(data_dir, "patient_data_sequences.pkl")
    hi_path  = os.path.join(data_dir, "patient_data_with_health_index.pkl")
    final_path = os.path.join(data_dir, "patient_data_with_all_indices.pkl")

    # 1) data_preprocessing
    if not os.path.exists(seq_path):
        logger.info("Missing patient_data_sequences.pkl -> Running data_preprocessing.")
        preprocess_main()
    else:
        logger.info("Found patient_data_sequences.pkl.")

    # 2) health_index
    if not os.path.exists(hi_path):
        logger.info("Missing patient_data_with_health_index.pkl -> Running health_index.")
        health_main()
    else:
        logger.info("Found patient_data_with_health_index.pkl.")

    # 3) If final file already exists, skip
    if os.path.exists(final_path):
        logger.info(f"Found {final_path}, skipping further merges.")
        return

    logger.info(f"Creating {final_path} by merging Charlson & Elixhauser.")
    # Load base data
    df = pd.read_pickle(hi_path)

    # Merge Charlson
    conditions_csv = os.path.join(data_dir, "conditions.csv")
    if not os.path.exists(conditions_csv):
        raise FileNotFoundError("conditions.csv not found. Cannot compute Charlson/Elixhauser.")
    conditions = pd.read_csv(conditions_csv, usecols=["PATIENT","CODE","DESCRIPTION"])

    cci_map = load_cci_mapping(data_dir)  # Provided by charlson_comorbidity
    patient_cci = compute_cci(conditions, cci_map)
    merged_cci = df.merge(patient_cci, how="left", left_on="Id", right_on="PATIENT")
    merged_cci.drop(columns="PATIENT", inplace=True)
    merged_cci["CharlsonIndex"] = merged_cci["CharlsonIndex"].fillna(0.0)
    del df, patient_cci
    gc.collect()

    # Merge Elixhauser
    eci_df = compute_eci(conditions)
    merged_eci = merged_cci.merge(eci_df, how="left", left_on="Id", right_on="PATIENT")
    merged_eci.drop(columns="PATIENT", inplace=True, errors="ignore")
    merged_eci["ElixhauserIndex"] = merged_eci["ElixhauserIndex"].fillna(0.0)
    del conditions, eci_df, merged_cci
    gc.collect()

    # Save final
    merged_eci.to_pickle(final_path)
    logger.info(f"[DataPrep] Created {final_path}.")
    del merged_eci
    gc.collect()

# vitai_scripts/feature_utils.py
# Author: Imran Feisal
# Date: 21/01/2025
#
# Description:
#   Provides functions for selecting relevant features from
#   the patient DataFrame based on the chosen configuration:
#     - composite (Health_Index)
#     - cci (CharlsonIndex)
#     - eci (ElixhauserIndex)
#     - combined (Health_Index + CharlsonIndex)
#     - combined_eci (Health_Index + ElixhauserIndex)
#
#   You may add your own extra combos if you wish,
#   e.g. "all_indices" for all three.

import pandas as pd

def select_features(df: pd.DataFrame, feature_config: str) -> pd.DataFrame:
    """
    For each feature_config, returns a subset of columns:
      - 'composite' => uses Health_Index
      - 'cci'       => uses CharlsonIndex
      - 'eci'       => uses ElixhauserIndex
      - 'combined'  => uses Health_Index + CharlsonIndex
      - 'combined_eci' => uses Health_Index + ElixhauserIndex
      (You can extend further if needed.)

    Also includes base demographics & hospital/med counts.
    """
    base_cols = [
        "Id", "GENDER", "RACE", "ETHNICITY", "MARITAL",
        "HEALTHCARE_EXPENSES", "HEALTHCARE_COVERAGE", "INCOME",
        "AGE", "DECEASED",
        "Hospitalizations_Count", "Medications_Count", "Abnormal_Observations_Count"
    ]

    # Verify these exist; some might be absent if your data lacks them, so handle carefully.
    for col in base_cols:
        if col not in df.columns:
            df[col] = 0  # fill with 0 or something sensible

    if feature_config == "composite":
        if "Health_Index" not in df.columns:
            raise KeyError("Missing 'Health_Index' for 'composite'.")
        needed = base_cols + ["Health_Index"]

    elif feature_config == "cci":
        if "CharlsonIndex" not in df.columns:
            raise KeyError("Missing 'CharlsonIndex' for 'cci'.")
        needed = base_cols + ["CharlsonIndex"]

    elif feature_config == "eci":
        if "ElixhauserIndex" not in df.columns:
            raise KeyError("Missing 'ElixhauserIndex' for 'eci'.")
        needed = base_cols + ["ElixhauserIndex"]

    elif feature_config == "combined":
        # Health + Charlson
        if ("Health_Index" not in df.columns) or ("CharlsonIndex" not in df.columns):
            raise KeyError("Need both 'Health_Index' & 'CharlsonIndex' for 'combined'.")
        needed = base_cols + ["Health_Index","CharlsonIndex"]

    elif feature_config == "combined_eci":
        # Health + Elixhauser
        if ("Health_Index" not in df.columns) or ("ElixhauserIndex" not in df.columns):
            raise KeyError("Need both 'Health_Index' & 'ElixhauserIndex' for 'combined_eci'.")
        needed = base_cols + ["Health_Index","ElixhauserIndex"]
        
    elif feature_config == "combined_all":
        # Health + Charlson + Elixhauser
        if ("Health_Index" not in df.columns) or ("CharlsonIndex" not in df.columns) or ("ElixhauserIndex" not in df.columns):
            raise KeyError("Need 'Health_Index', 'CharlsonIndex' & 'ElixhauserIndex' for 'combined_all'.")
        needed = base_cols + ["Health_Index","CharlsonIndex","ElixhauserIndex"]

    else:
        raise ValueError(f"Invalid feature_config: {feature_config}")

    return df[needed].copy()

# vitai_scripts/model_utils.py
# Author: Imran Feisal
# Date: 21/01/2025
#
# Description:
#   Provides utility functions to run the VAE and TabNet models,
#   plus gather their metrics from JSON files.

import os
import glob
import json
import logging
import numpy as np
import pandas as pd
import sys

root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(root_path)
# Root-level models
from vae_model import main as vae_main
from tabnet_model import main as tabnet_main

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def run_vae(input_pkl: str, output_prefix: str) -> str:
    """
    Runs VAE on 'input_pkl' and saves results with prefix=output_prefix.
    Returns the path to the latent CSV or None if missing.
    """
    logger.info(f"[VAE] Starting with prefix={output_prefix}")
    vae_main(input_file=input_pkl, output_prefix=output_prefix)
    latent_csv = f"{output_prefix}_latent_features.csv"
    if os.path.exists(latent_csv) and os.path.getsize(latent_csv) > 10:
        return latent_csv
    logger.warning("[VAE] Latent features CSV not found or too small.")
    return None

def run_tabnet(input_pkl: str, output_prefix: str, target_col: str = "Health_Index") -> str:
    """
    Runs TabNet on 'input_pkl' with the specified 'target_col',
    returns the path to the predictions CSV or None if missing.
    """
    logger.info(f"[TabNet] Starting with prefix={output_prefix}, target={target_col}")
    tabnet_main(input_file=input_pkl, output_prefix=output_prefix, target_col=target_col)
    preds_csv = f"{output_prefix}_predictions.csv"
    if os.path.exists(preds_csv) and os.path.getsize(preds_csv) > 10:
        return preds_csv
    logger.warning("[TabNet] Predictions CSV not found or too small.")
    return None

def gather_tabnet_metrics(prefix: str) -> dict:
    """
    Reads TabNet metrics from the JSON file if present.
    Returns a dict with: tabnet_mse, tabnet_r2
    """
    mf = f"{prefix}_metrics.json"
    out = {"tabnet_mse": np.nan, "tabnet_r2": np.nan}
    if not os.path.exists(mf) or os.path.getsize(mf) < 2:
        return out

    try:
        with open(mf, "r") as f:
            data = json.load(f)
        out["tabnet_mse"] = float(data.get("test_mse", np.nan))
        out["tabnet_r2"]  = float(data.get("test_r2", np.nan))
    except Exception as e:
        logger.warning(f"[TabNet] Error reading metrics JSON: {e}")
    return out

def gather_vae_metrics(prefix: str) -> dict:
    """
    Reads VAE metrics from the JSON file if present.
    Returns a dict with: vae_final_train_loss, vae_best_val_loss
    """
    pattern = f"{prefix}_vae_metrics.json"
    if not os.path.exists(pattern):
        maybe = glob.glob(prefix + "_vae_metrics.json")
        if not maybe:
            return {"vae_final_train_loss": np.nan, "vae_best_val_loss": np.nan}
        pattern = maybe[0]

    out = {"vae_final_train_loss": np.nan, "vae_best_val_loss": np.nan}
    if os.path.getsize(pattern) < 2:
        return out

    try:
        with open(pattern, "r") as f:
            data = json.load(f)
        out["vae_final_train_loss"] = float(data.get("final_train_loss", np.nan))
        out["vae_best_val_loss"]    = float(data.get("best_val_loss", np.nan))
    except Exception as e:
        logger.warning(f"[VAE] Error reading metrics JSON: {e}")

    return out

# vitai_scripts/run_vitai_tests_main.py
# Author: Imran Feisal
# Date: 21/01/2025
#
# Description:
#   A single master script to:
#     1) Ensure the data is prepared (including Elixhauser).
#     2) For each (feature_config, subset_type, model_approach),
#        run VAE/TabNet as needed, merge outputs, do clustering.
#     3) Gather model metrics & cluster metrics into ONE final CSV.
#
# Usage Example:
#   python run_vitai.py \
#       --data-dir Data \
#       --output-file final_vitai_results.csv \
#       --feature-configs composite cci eci combined combined_eci combined_all\
#       --subset-types none diabetes ckd \
#       --model-approaches vae tabnet hybrid

import os
import sys
import gc
import argparse
import logging
import datetime
import itertools
import pandas as pd
import numpy as np
from tqdm import tqdm

# vitai_scripts modules
from data_prep import ensure_preprocessed_data
from subset_utils import filter_subpopulation
from feature_utils import select_features
from model_utils import run_vae, run_tabnet, gather_vae_metrics, gather_tabnet_metrics
from cluster_utils import cluster_and_visualise

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def file_is_fully_written(file_path: str, min_size=1, max_age=30) -> bool:
    """
    Checks if a file is large enough and not modified in the last 'max_age' seconds.
    Used to avoid partial files.
    """
    if not os.path.exists(file_path):
        return False
    if os.path.getsize(file_path) < min_size:
        return False
    import time
    mtime = os.path.getmtime(file_path)
    now = time.time()
    if (now - mtime) < max_age:
        return False
    return True

def vae_done(vae_prefix: str) -> bool:
    latent_csv = f"{vae_prefix}_latent_features.csv"
    return file_is_fully_written(latent_csv, min_size=10)

def tabnet_done(tabnet_prefix: str) -> bool:
    preds_csv = f"{tabnet_prefix}_predictions.csv"
    metrics_json = f"{tabnet_prefix}_metrics.json"
    preds_ok = file_is_fully_written(preds_csv, min_size=10)
    metrics_ok = file_is_fully_written(metrics_json, min_size=2)
    return preds_ok and metrics_ok

def cluster_done(config_id: str, config_folder: str) -> bool:
    plots_folder = os.path.join(config_folder, "plots")
    tsne_path = os.path.join(plots_folder, f"tsne2d_{config_id}.png")
    umap_path = os.path.join(plots_folder, f"umap2d_{config_id}.png")
    tsne_ok = file_is_fully_written(tsne_path, min_size=1000)
    umap_ok = file_is_fully_written(umap_path, min_size=1000)
    return (tsne_ok and umap_ok)

def all_done(fc, ss, ma, config_folder, run_ts):
    """
    Checks if VAE, TabNet, and clustering are all done for this combo.
    """
    config_id = f"{fc}_{ss}_{ma}"
    vae_prefix = os.path.join(config_folder, f"{config_id}_{run_ts}_vae")
    tabnet_prefix = os.path.join(config_folder, f"{config_id}_{run_ts}_tabnet")

    if ma == "vae":
        return vae_done(vae_prefix) and cluster_done(config_id, config_folder)
    elif ma == "tabnet":
        return tabnet_done(tabnet_prefix) and cluster_done(config_id, config_folder)
    elif ma == "hybrid":
        return vae_done(vae_prefix) and tabnet_done(tabnet_prefix) and cluster_done(config_id, config_folder)
    return False

def run_vitai_pipeline(data_dir, output_file, feature_configs, subset_types, model_approaches):
    data_dir = os.path.abspath(data_dir)
    out_csv  = os.path.join(data_dir, output_file)

    logger.info(f"Data directory: {data_dir}")
    logger.info(f"Output CSV: {out_csv}")
    logger.info(f"Feature configs: {feature_configs}")
    logger.info(f"Subset types: {subset_types}")
    logger.info(f"Model approaches: {model_approaches}")

    # 1) Ensure data is fully prepped (including Elixhauser)
    ensure_preprocessed_data(data_dir)

    # 2) Load the combined pickle with all indices
    final_pkl = os.path.join(data_dir, "patient_data_with_all_indices.pkl")
    if not os.path.exists(final_pkl):
        raise FileNotFoundError("No patient_data_with_all_indices.pkl after data prep.")
    full_df = pd.read_pickle(final_pkl)
    logger.info(f"[Main] Loaded final dataset shape={full_df.shape}")

    # 3) Build combos
    combos = list(itertools.product(feature_configs, subset_types, model_approaches))
    total = len(combos)

    # We'll store results for each combo, then write to a single CSV
    all_results = []
    run_ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    for (fc, ss, ma) in tqdm(combos, total=total, desc="Overall progress"):
        config_id = f"{fc}_{ss}_{ma}"
        logger.info(f"\n===== Running {config_id} =====")
        config_folder = os.path.join(data_dir, "Experiments", config_id)
        os.makedirs(config_folder, exist_ok=True)

        # If all done, skip
        if all_done(fc, ss, ma, config_folder, run_ts):
            logger.info(f"[Skip] All done for {config_id}.")
            continue

        # Subset
        sub_df = filter_subpopulation(full_df, ss, data_dir)
        # Feature selection
        feats_df = select_features(sub_df, fc)

        # Save temp
        temp_pkl = os.path.join(config_folder, f"temp_{config_id}_{run_ts}.pkl")
        feats_df.to_pickle(temp_pkl)
        del sub_df, feats_df
        gc.collect()

        # Model prefixes
        vae_prefix = os.path.join(config_folder, f"{config_id}_{run_ts}_vae")
        tabnet_prefix = os.path.join(config_folder, f"{config_id}_{run_ts}_tabnet")

        # Possibly run VAE
        if ma in ["vae","hybrid"] and not vae_done(vae_prefix):
            run_vae(temp_pkl, vae_prefix)

        # Possibly run TabNet
        if ma in ["tabnet","hybrid"] and not tabnet_done(tabnet_prefix):
            # Decide target_col
            if fc == "cci":
                run_tabnet(temp_pkl, tabnet_prefix, target_col="CharlsonIndex")
            elif fc == "eci":
                run_tabnet(temp_pkl, tabnet_prefix, target_col="ElixhauserIndex")
            else:
                # composite, combined, combined_eci -> typically predict Health_Index
                run_tabnet(temp_pkl, tabnet_prefix, target_col="Health_Index")

        # Gather partial results (model metrics)
        result_dict = {"config_id": config_id}
        if ma in ["tabnet","hybrid"]:
            tb_metrics = gather_tabnet_metrics(tabnet_prefix)
            result_dict.update(tb_metrics)
        if ma in ["vae","hybrid"]:
            vae_metrics = gather_vae_metrics(vae_prefix)
            result_dict.update(vae_metrics)

        # Merge outputs to do clustering
        frames = []
        vae_csv = f"{vae_prefix}_latent_features.csv"
        if os.path.exists(vae_csv) and os.path.getsize(vae_csv) > 10:
            frames.append(pd.read_csv(vae_csv))
        tabnet_csv = f"{tabnet_prefix}_predictions.csv"
        if os.path.exists(tabnet_csv) and os.path.getsize(tabnet_csv) > 10:
            frames.append(pd.read_csv(tabnet_csv))

        if frames:
            temp_data = pd.read_pickle(temp_pkl)
            merged_df = temp_data
            for fdf in frames:
                merged_df = merged_df.merge(fdf, on="Id", how="inner")

            # One-hot encode typical demographics
            cat_cols = ["GENDER","RACE","ETHNICITY","MARITAL"]
            existing_cats = [c for c in cat_cols if c in merged_df.columns]
            for c in existing_cats:
                merged_df[c] = merged_df[c].astype(str)
            if existing_cats:
                merged_df = pd.get_dummies(merged_df, columns=existing_cats, drop_first=True)

            # Clustering
            if not cluster_done(config_id, config_folder):
                clus_res = cluster_and_visualise(merged_df, config_id, os.path.join(config_folder, "plots"))
                result_dict.update(clus_res)
            else:
                logger.info(f"[Skip] Clustering for {config_id} already done.")
            del merged_df, temp_data
        else:
            logger.info(f"[{config_id}] No model output to cluster, skipping.")

        # Clean up
        if os.path.exists(temp_pkl):
            os.remove(temp_pkl)

        all_results.append(result_dict)

    # 4) Write everything to one final CSV
    if all_results:
        df_final = pd.DataFrame(all_results)
        # If file doesn't exist, write header
        write_header = not os.path.exists(out_csv)
        df_final.to_csv(out_csv, mode='a', header=write_header, index=False)
        logger.info(f"Saved final results to {out_csv}")
    else:
        logger.info("No new results to write. All done.")

def main():
    parser = argparse.ArgumentParser(description="Run the entire VITAI pipeline (incl. Elixhauser) from one script.")
    parser.add_argument("--data-dir", type=str, default="Data",
                        help="Path to the Data folder.")
    parser.add_argument("--output-file", type=str, default="vitai_final_results.csv",
                        help="Name of the final CSV to produce.")
    parser.add_argument("--feature-configs", nargs="+",
                        default=["composite","cci","eci","combined","combined_eci", "combined_all"],
                        help="Which feature configs to test.")
    parser.add_argument("--subset-types", nargs="+",
                        default=["none","diabetes","ckd"],
                        help="Which subpopulations to test.")
    parser.add_argument("--model-approaches", nargs="+",
                        default=["vae","tabnet","hybrid"],
                        help="Which model approaches to test.")
    args = parser.parse_args()

    run_vitai_pipeline(args.data_dir, args.output_file, args.feature_configs, args.subset_types, args.model_approaches)

if __name__ == "__main__":
    main()
# vitai_scripts/subset_utils.py
# Author: Imran Feisal
# Date: 21/01/2025
#
# Description:
#   Provides logic for filtering the patient DataFrame
#   into specific sub-populations (none, diabetes, ckd).

import os
import logging
import pandas as pd

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def _load_conditions(data_dir: str) -> pd.DataFrame:
    cond_path = os.path.join(data_dir, "conditions.csv")
    if not os.path.exists(cond_path):
        raise FileNotFoundError(f"Cannot find conditions.csv at {cond_path}")
    return pd.read_csv(cond_path, usecols=["PATIENT","CODE","DESCRIPTION"])

def subset_diabetes(df: pd.DataFrame, data_dir: str) -> pd.DataFrame:
    conditions = _load_conditions(data_dir)
    mask = conditions["DESCRIPTION"].str.lower().str.contains("diabetes", na=False)
    diabetic_patients = conditions.loc[mask, "PATIENT"].unique()
    sub = df[df["Id"].isin(diabetic_patients)].copy()
    logger.info(f"Subset 'diabetes' shape: {sub.shape}")
    return sub

def subset_ckd(df: pd.DataFrame, data_dir: str) -> pd.DataFrame:
    conditions = _load_conditions(data_dir)
    ckd_codes = {431855005, 431856006, 433144002, 431857002, 46177005}
    code_mask = conditions["CODE"].isin(ckd_codes)
    text_mask = conditions["DESCRIPTION"].str.lower().str.contains("chronic kidney disease", na=False)
    ckd_patients = conditions.loc[code_mask | text_mask, "PATIENT"].unique()
    sub = df[df["Id"].isin(ckd_patients)].copy()
    logger.info(f"Subset 'ckd' shape: {sub.shape}")
    return sub

def filter_subpopulation(df: pd.DataFrame, subset_type: str, data_dir: str) -> pd.DataFrame:
    """
    Subset the DataFrame by 'none', 'diabetes', or 'ckd'.
    If unknown subset, returns the full data.
    """
    st = subset_type.lower().strip()
    if st == "none":
        return df
    elif st == "diabetes":
        return subset_diabetes(df, data_dir)
    elif st == "ckd":
        return subset_ckd(df, data_dir)
    else:
        logger.warning(f"Unknown subset='{subset_type}', returning full dataset.")
        return df

# final_three_tabnet.py
# Author: You
# Date: 21/01/2025
#
# Description:
#   A script that runs our three final TabNet models on the entire dataset,
#   each restricted to one subpopulation:
#     1) "combined_diabetes_tabnet"   -> subset: diabetes,      feature_config: "combined"
#     2) "combined_all_ckd_tabnet"    -> subset: ckd,           feature_config: "combined_all"
#     3) "combined_none_tabnet"       -> subset: none,          feature_config: "combined"
#   We tune hyperparameters briefly, then train a final model with bigger max_epochs.
#   We do clustering (K-Means + optional t-SNE/UMAP) on the final predicted subpopulation.
#   Outputs are saved under Data/finals/<model_id>/.
#
# Usage:
#   python final_three_tabnet.py

import os
import json
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import sys 

#---------------------------------------------------
# Add the root directory to PYTHONPATH
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, ".."))
sys.path.append(ROOT_DIR)

# Add `vitai_scripts` to PYTHONPATH
VITAI_SCRIPTS_DIR = os.path.join(ROOT_DIR, "vitai_scripts")
sys.path.append(VITAI_SCRIPTS_DIR)



#---------------------------------------------------

# TabNet, tuning, training
import torch
from pytorch_tabnet.tab_model import TabNetRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score
)
from sklearn.manifold import TSNE
import umap.umap_ as umap

# Import  existing logic
# Adjust these paths to match your actual project structure
from tabnet_model import load_data, prepare_data, hyperparameter_tuning
from subset_utils import filter_subpopulation
from feature_utils import select_features

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(SCRIPT_DIR, "..", "Data")   # Root-level "Data" folder
FINALS_DIR = os.path.join(DATA_DIR, "finals")
os.makedirs(FINALS_DIR, exist_ok=True)

# We’ll load all patients from the final combined file
INPUT_PICKLE = "patient_data_with_all_indices.pkl"

# Here are the three final configs we want:
FINAL_CONFIGS = [
    {
        "model_id": "combined_diabetes_tabnet",
        "subset": "diabetes",
        "feature_config": "combined"
    },
    {
        "model_id": "combined_all_ckd_tabnet",
        "subset": "ckd",
        "feature_config": "combined_all"
    },
    {
        "model_id": "combined_none_tabnet",
        "subset": "none",
        "feature_config": "combined"
    },
]

# The target column for all three is your Health Index
TARGET_COL = "Health_Index"

# Final training epochs
FINAL_MAX_EPOCHS = 400
FINAL_PATIENCE = 50

def run_final_model(model_id: str, subset_type: str, feature_config: str, full_df: pd.DataFrame):
    """
    1) Filter the DataFrame by subpopulation (subset_type)
    2) Select features by feature_config
    3) Hyperparameter tune with moderate epochs
    4) Re-train with final large epochs
    5) Predict on subpopulation
    6) Clustering on that subpopulation
    7) Save artifacts to Data/finals/<model_id>/
    """
    logger.info(f"[{model_id}] Starting final run. subset={subset_type}, features={feature_config}")

    # Prepare subfolder
    out_dir = os.path.join(FINALS_DIR, model_id)
    os.makedirs(out_dir, exist_ok=True)

    # 1) Filter subpopulation
    sub_df = filter_subpopulation(full_df, subset_type, DATA_DIR)
    if sub_df.empty:
        logger.warning(f"[{model_id}] No patients in subset={subset_type}. Skipping.")
        return

    # 2) Pick the right features
    feats_df = select_features(sub_df, feature_config)

    # 3) Prepare data for TabNet
    #    (We assume prepare_data can accept a 'target_col' for the final col)
    X, y, cat_idxs, cat_dims, feature_columns = prepare_data(feats_df, target_col=TARGET_COL)

    # Train/test split
    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)

    # 4) Hyperparameter tuning (short-run)
    best_params = hyperparameter_tuning(X_train, y_train, cat_idxs, cat_dims)
    logger.info(f"[{model_id}] Best hyperparams from tuning: {best_params}")

    # 5) Build final regressor with big epochs
    saved_lr = best_params.pop("lr")
    best_params.update({
        "optimizer_fn": torch.optim.Adam,
        "optimizer_params": dict(lr=saved_lr),
        "device_name": "cuda" if torch.cuda.is_available() else "cpu",
        "verbose": 1
    })

    regressor = TabNetRegressor(cat_idxs=cat_idxs, cat_dims=cat_dims, **best_params)
    regressor.fit(
        X_train=X_train,
        y_train=y_train,
        eval_set=[(X_valid, y_valid)],
        eval_metric=["rmse"],
        max_epochs=FINAL_MAX_EPOCHS,
        patience=FINAL_PATIENCE,
        batch_size=8192,
        virtual_batch_size=1024
    )

    # Save the model
    regressor.save_model(os.path.join(out_dir, f"{model_id}_model"))
    logger.info(f"[{model_id}] Final TabNet model saved.")

    # Evaluate on test set
    test_preds = regressor.predict(X_test).flatten()
    test_mse = mean_squared_error(y_test, test_preds)
    test_r2 = r2_score(y_test, test_preds)
    logger.info(f"[{model_id}] Test MSE: {test_mse:.6f}")
    logger.info(f"[{model_id}] Test R2:  {test_r2:.6f}")

    # Save final metrics to JSON
    metrics_dict = {
        "test_mse": float(test_mse),
        "test_r2": float(test_r2)
    }
    with open(os.path.join(out_dir, f"{model_id}_metrics.json"), "w") as f:
        json.dump(metrics_dict, f, indent=2)

    # 6) Predict on entire subpopulation (for clustering)
    all_preds = regressor.predict(X).flatten()
    # If 'Id' in feats_df, use it; else fallback
    if "Id" in feats_df.columns:
        pid = feats_df["Id"].values
    else:
        pid = np.arange(len(X))

    pred_df = pd.DataFrame({
        "Id": pid,
        f"Predicted_{TARGET_COL}": all_preds
    })
    pred_csv = os.path.join(out_dir, f"{model_id}_predictions.csv")
    pred_df.to_csv(pred_csv, index=False)
    logger.info(f"[{model_id}] Full subpopulation predictions saved.")

    # 7) Clustering
    #    Merge predicted values with feature columns used for clustering
    cluster_df = pd.DataFrame(X, columns=feature_columns)
    cluster_df[f"Predicted_{TARGET_COL}"] = all_preds
    cluster_df["Id"] = pid

    # Scale everything except 'Id'
    cluster_cols = [c for c in cluster_df.columns if c != "Id"]
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(cluster_df[cluster_cols])

    # K-Means [6..9], pick best by silhouette
    best_k, best_sil = None, -1
    for k in range(6, 10):
        km = KMeans(n_clusters=k, random_state=42)
        labels = km.fit_predict(X_scaled)
        s = silhouette_score(X_scaled, labels)
        if s > best_sil:
            best_sil = s
            best_k = k

    final_km = KMeans(n_clusters=best_k, random_state=42)
    final_labels = final_km.fit_predict(X_scaled)
    cluster_df["Cluster"] = final_labels

    # Evaluate cluster metrics
    sil = silhouette_score(X_scaled, final_labels)
    cal = calinski_harabasz_score(X_scaled, final_labels)
    dav = davies_bouldin_score(X_scaled, final_labels)

    # DBSCAN for reference
    db = DBSCAN(eps=0.5, min_samples=5).fit(X_scaled)
    db_labels = db.labels_
    if len(set(db_labels)) < 2:
        db_sil = np.nan
        db_cal = np.nan
        db_dav = np.nan
    else:
        db_sil = silhouette_score(X_scaled, db_labels)
        db_cal = calinski_harabasz_score(X_scaled, db_labels)
        db_dav = davies_bouldin_score(X_scaled, db_labels)

    cluster_metrics = {
        "chosen_k": best_k,
        "silhouette": float(sil),
        "calinski": float(cal),
        "davies_bouldin": float(dav),
        "dbscan_silhouette": float(db_sil),
        "dbscan_calinski": float(db_cal),
        "dbscan_davies_bouldin": float(db_dav)
    }
    with open(os.path.join(out_dir, f"{model_id}_clusters.json"), "w") as f:
        json.dump(cluster_metrics, f, indent=2)
    logger.info(f"[{model_id}] Clustering metrics: {cluster_metrics}")

    # Save cluster assignments
    clusters_csv = os.path.join(out_dir, f"{model_id}_clusters.csv")
    cluster_df[["Id", f"Predicted_{TARGET_COL}", "Cluster"]].to_csv(clusters_csv, index=False)

    # Optional: t-SNE & UMAP
    # t-SNE
    tsne = TSNE(n_components=2, random_state=42)
    X_tsne = tsne.fit_transform(X_scaled)
    plt.figure(figsize=(7,5))
    sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=cluster_df["Cluster"], palette="viridis")
    plt.title(f"t-SNE ({model_id}) - K={best_k}")
    tsne_path = os.path.join(out_dir, f"{model_id}_tsne.png")
    plt.savefig(tsne_path, bbox_inches="tight")
    plt.close()

    # UMAP
    reducer = umap.UMAP(n_components=2, random_state=42)
    X_umap = reducer.fit_transform(X_scaled)
    plt.figure(figsize=(7,5))
    sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=cluster_df["Cluster"], palette="viridis")
    plt.title(f"UMAP ({model_id}) - K={best_k}")
    umap_path = os.path.join(out_dir, f"{model_id}_umap.png")
    plt.savefig(umap_path, bbox_inches="tight")
    plt.close()

    logger.info(f"[{model_id}] Clustering plots saved.")
    logger.info(f"[{model_id}] Done.\n")


def main():
    # Load the full dataset
    full_path = os.path.join(DATA_DIR, INPUT_PICKLE)
    if not os.path.exists(full_path):
        raise FileNotFoundError(f"Cannot find {full_path} - ensure data prep is complete.")

    full_df = pd.read_pickle(full_path)
    logger.info(f"Loaded dataset: {full_df.shape} rows, from {full_path}")

    # Run each final config in turn
    for cfg in FINAL_CONFIGS:
        run_final_model(
            model_id=cfg["model_id"],
            subset_type=cfg["subset"],
            feature_config=cfg["feature_config"],
            full_df=full_df
        )

    logger.info("[All Done] final_three_tabnet pipeline completed successfully.")


if __name__ == "__main__":
    main()

# validate_final_tabnet_models.py
# Author: Imran Feisal
# Date: 21/01/2025
#
# Description:
#   A validation script that compares each of the three final TabNet models'
#   predictions/clusters against Charlson (CCI) & Elixhauser (ECI) indices.
#
#   Specifically, for each final model:
#     1) Load its final cluster CSV (e.g. "Id", "Predicted_Health_Index", "Cluster").
#     2) Compute Charlson & Elixhauser from "conditions.csv" using your existing
#        "charlson_comorbidity.py" & "elixhauser_comorbidity.py".
#     3) Merge them on patient ID ("Id" vs. "PATIENT").
#     4) Perform correlations, descriptive stats, ANOVA/Kruskal across clusters.
#     5) Generate scatter/box/violin plots.
#     6) Save everything in "Data/validations/<model_id>/".
#
# Usage:
#   python validate_final_tabnet_models.py

import os
import logging
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from scipy.stats import pearsonr, spearmanr, f_oneway, kruskal
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score
)

# Local imports for CCI/ECI. Adjust the path as needed.
import sys
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.join(SCRIPT_DIR, "..")
sys.path.append(PROJECT_ROOT)

import charlson_comorbidity
import elixhauser_comorbidity

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

########################
# CONFIG
########################

DATA_DIR = os.path.join(PROJECT_ROOT, "Data")
VALIDATIONS_DIR = os.path.join(DATA_DIR, "validations")
os.makedirs(VALIDATIONS_DIR, exist_ok=True)

# Where your conditions.csv lives
CONDITIONS_CSV = os.path.join(DATA_DIR, "conditions.csv")

# These are the three final model outputs for "diabetes", "ckd", and "none".
# Adjust names/paths if needed. Each CSV must have: "Id", "Predicted_Health_Index", "Cluster" (optional).
FINAL_MODEL_CSVS = {
    "combined_diabetes_tabnet":    "finals/combined_diabetes_tabnet/combined_diabetes_tabnet_clusters.csv",
    "combined_all_ckd_tabnet":     "finals/combined_all_ckd_tabnet/combined_all_ckd_tabnet_clusters.csv",
    "combined_none_tabnet":        "finals/combined_none_tabnet/combined_none_tabnet_clusters.csv"
}

########################
# FUNCTIONS
########################

def load_and_merge_comorbidity(final_csv: str) -> pd.DataFrame:
    """
    Loads a final model's cluster CSV, merges with Charlson & Elixhauser indices
    computed from conditions.csv, then returns the merged DataFrame.
    """

    # 1) Load final TabNet predictions/clusters
    df_model = pd.read_csv(final_csv)
    logger.info(f"Loaded model output from {final_csv}: shape={df_model.shape}")

    # Check for required columns
    if "Id" not in df_model.columns or "Predicted_Health_Index" not in df_model.columns:
        raise KeyError(f"{final_csv} must contain 'Id' and 'Predicted_Health_Index' columns.")
    # 'Cluster' is optional, but let's check
    has_cluster = ("Cluster" in df_model.columns)

    # 2) Compute Charlson & Elixhauser from conditions
    if not os.path.exists(CONDITIONS_CSV):
        raise FileNotFoundError(f"Cannot find conditions.csv at {CONDITIONS_CSV}")

    conditions = pd.read_csv(CONDITIONS_CSV)
    if "PATIENT" not in conditions.columns or "CODE" not in conditions.columns:
        raise KeyError("conditions.csv must have 'PATIENT' and 'CODE' columns.")

    # Compute Charlson
    cci_map = charlson_comorbidity.load_cci_mapping(DATA_DIR)
    df_cci = charlson_comorbidity.compute_cci(conditions, cci_map)

    # Compute Elixhauser
    df_eci = elixhauser_comorbidity.compute_eci(conditions)

    # Merge
    df_merged = df_model.merge(df_cci, how="left", left_on="Id", right_on="PATIENT")
    df_merged.drop(columns=["PATIENT"], inplace=True, errors="ignore")
    df_merged = df_merged.merge(df_eci, how="left", left_on="Id", right_on="PATIENT")
    df_merged.drop(columns=["PATIENT"], inplace=True, errors="ignore")

    # Fill missing
    df_merged["CharlsonIndex"] = df_merged["CharlsonIndex"].fillna(0)
    df_merged["ElixhauserIndex"] = df_merged["ElixhauserIndex"].fillna(0)

    logger.info(f"Merged shape={df_merged.shape} after adding CCI/ECI.")
    return df_merged


def save_correlations(df: pd.DataFrame, out_path: str):
    """
    Computes Pearson & Spearman correlations among:
      - Predicted_Health_Index
      - CharlsonIndex
      - ElixhauserIndex
    Saves them to a CSV at out_path.
    Also returns the correlation DataFrame.
    """
    pairs = [
        ("Predicted_Health_Index", "CharlsonIndex"),
        ("Predicted_Health_Index", "ElixhauserIndex"),
        ("CharlsonIndex", "ElixhauserIndex"),
    ]
    corrs = []
    for (xcol, ycol) in pairs:
        data_xy = df[[xcol, ycol]].dropna()
        if len(data_xy) < 2:
            corrs.append({
                "VarX": xcol, "VarY": ycol,
                "Pearson_R": np.nan, "Pearson_pvalue": np.nan,
                "Spearman_R": np.nan, "Spearman_pvalue": np.nan
            })
            continue
        pear_r, pear_p = pearsonr(data_xy[xcol], data_xy[ycol])
        spear_r, spear_p = spearmanr(data_xy[xcol], data_xy[ycol])
        corrs.append({
            "VarX": xcol, "VarY": ycol,
            "Pearson_R": pear_r, "Pearson_pvalue": pear_p,
            "Spearman_R": spear_r, "Spearman_pvalue": spear_p
        })

    df_corrs = pd.DataFrame(corrs)
    df_corrs.to_csv(out_path, index=False)
    return df_corrs


def boxplot_by_cluster(df: pd.DataFrame, column: str, cluster_col: str, out_file: str):
    """
    Generates a boxplot of 'column' grouped by 'cluster_col' in df,
    saves it to out_file. If cluster_col not present or only one cluster,
    it does nothing.
    """
    if cluster_col not in df.columns or df[cluster_col].nunique() < 2:
        return

    plt.figure(figsize=(7,5))
    sns.boxplot(data=df, x=cluster_col, y=column, palette="Set2")
    plt.title(f"{column} by {cluster_col}")
    plt.tight_layout()
    plt.savefig(out_file)
    plt.close()


def scatter_plot(df: pd.DataFrame, xcol: str, ycol: str, out_file: str, hue_col: str = None):
    """
    Generates a scatter plot of df[xcol] vs df[ycol], optional hue_col.
    """
    if xcol not in df.columns or ycol not in df.columns:
        return

    plt.figure(figsize=(6,5))
    if hue_col and hue_col in df.columns:
        sns.scatterplot(data=df, x=xcol, y=ycol, hue=hue_col, palette="viridis")
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        sns.scatterplot(data=df, x=xcol, y=ycol)
    plt.title(f"{ycol} vs {xcol}")
    plt.tight_layout()
    plt.savefig(out_file)
    plt.close()


def distribution_plot(df: pd.DataFrame, column: str, out_file: str):
    """
    Generates a simple histogram + KDE of the given column.
    """
    if column not in df.columns:
        return

    plt.figure(figsize=(7,5))
    sns.histplot(df[column], kde=True, color="blue")
    plt.title(f"Distribution of {column}")
    plt.tight_layout()
    plt.savefig(out_file)
    plt.close()


def anova_kruskal_across_clusters(df: pd.DataFrame, cluster_col: str, measure: str):
    """
    If 'cluster_col' is in df, perform one-way ANOVA and Kruskal-Wallis on the
    given 'measure' across cluster groups. Return a dict of stats.
    """
    if cluster_col not in df.columns or df[cluster_col].nunique() < 2:
        return {"Measure": measure, "Test": "ANOVA", "F_stat": np.nan, "p_value": np.nan}

    groups = [grp[measure].dropna().values for _, grp in df.groupby(cluster_col)]
    if len(groups) < 2:
        return {"Measure": measure, "Test": "ANOVA", "F_stat": np.nan, "p_value": np.nan}

    # ANOVA
    f_stat, p_val = f_oneway(*groups)
    # Kruskal
    h_stat, p_val2 = kruskal(*groups)

    return {
        "ANOVA_F": f_stat, "ANOVA_p": p_val,
        "Kruskal_H": h_stat, "Kruskal_p": p_val2
    }


def main():
    # Ensure conditions.csv is present
    if not os.path.exists(CONDITIONS_CSV):
        raise FileNotFoundError(f"Cannot find conditions.csv at {CONDITIONS_CSV}")

    # Loop over each final model CSV
    for model_id, rel_path in FINAL_MODEL_CSVS.items():
        logger.info(f"=== Validating model: {model_id} ===")
        csv_path = os.path.join(DATA_DIR, rel_path)
        if not os.path.exists(csv_path):
            logger.warning(f"[{model_id}] Missing final cluster CSV at {csv_path} - skipping.")
            continue

        # Subfolder for this model's validation outputs
        model_val_dir = os.path.join(VALIDATIONS_DIR, model_id)
        os.makedirs(model_val_dir, exist_ok=True)

        # Merge final predictions/clusters with CCI/ECI
        df_merged = load_and_merge_comorbidity(csv_path)

        # Basic descriptive stats
        numeric_cols = ["Predicted_Health_Index", "CharlsonIndex", "ElixhauserIndex"]
        desc_stats = df_merged[numeric_cols].describe().T
        desc_stats_file = os.path.join(model_val_dir, f"{model_id}_desc_stats.csv")
        desc_stats.to_csv(desc_stats_file)
        logger.info(f"[{model_id}] Descriptive stats saved -> {desc_stats_file}")

        # Correlations
        corr_file = os.path.join(model_val_dir, f"{model_id}_correlations.csv")
        df_corrs = save_correlations(df_merged, corr_file)
        logger.info(f"[{model_id}] Correlations saved -> {corr_file}")

        # ANOVA / Kruskal across clusters
        # (if "Cluster" is present & >1 unique value)
        anova_results = []
        if "Cluster" in df_merged.columns and df_merged["Cluster"].nunique() > 1:
            for measure in ["Predicted_Health_Index", "CharlsonIndex", "ElixhauserIndex"]:
                stats_dict = anova_kruskal_across_clusters(df_merged, "Cluster", measure)
                stats_dict["Measure"] = measure
                anova_results.append(stats_dict)

            df_anova = pd.DataFrame(anova_results)
            anova_file = os.path.join(model_val_dir, f"{model_id}_anova.csv")
            df_anova.to_csv(anova_file, index=False)
            logger.info(f"[{model_id}] ANOVA/Kruskal results -> {anova_file}")

        # Visualisations
        # 1) Distribution of predicted HI, CCI, ECI
        for c in numeric_cols:
            dist_file = os.path.join(model_val_dir, f"{model_id}_dist_{c}.png")
            distribution_plot(df_merged, c, dist_file)

        # 2) Boxplot of each measure by cluster
        if "Cluster" in df_merged.columns and df_merged["Cluster"].nunique() > 1:
            for c in numeric_cols:
                box_file = os.path.join(model_val_dir, f"{model_id}_box_{c}.png")
                boxplot_by_cluster(df_merged, c, "Cluster", box_file)

        # 3) Scatter plots (HI vs. Charlson, HI vs. ECI, Charlson vs. ECI)
        scatter_pairs = [
            ("Predicted_Health_Index", "CharlsonIndex"),
            ("Predicted_Health_Index", "ElixhauserIndex"),
            ("CharlsonIndex", "ElixhauserIndex")
        ]
        for xcol, ycol in scatter_pairs:
            scatter_path = os.path.join(model_val_dir, f"{model_id}_scatter_{xcol}_vs_{ycol}.png")
            scatter_plot(df_merged, xcol, ycol, scatter_path, hue_col="Cluster")

        logger.info(f"[{model_id}] Validation visuals saved in {model_val_dir}\n")

    logger.info("[All Done] Validation across final TabNet models completed.")


if __name__ == "__main__":
    main()
