{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder with Enhanced Clustering for Health Severity Analysis\n",
    "This notebook implements an improved Variational Autoencoder (VAE) on structured health data to identify patient clusters corresponding to different levels of health severity. We've incorporated various architecture styles, expanded hyperparameter tuning, and optimized the code for better performance and resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install keras-tuner\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and deep learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Check if TensorFlow is using the GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Enable memory growth for GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Enabled memory growth on GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. The code will run on CPU, which might be slower.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load preprocessed structured data\n",
    "structured_data = pd.read_csv('structured_data_preprocessed.csv')\n",
    "\n",
    "# Separate features\n",
    "X_structured = structured_data.values.astype('float32')  # Use float32 for efficiency\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_structured)\n",
    "\n",
    "# Save scaler for future use\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Clear unnecessary variables\n",
    "del X_structured\n",
    "del X_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sampling Layer\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# VAE Model with adjustable beta parameter\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, input_dim, beta=1.0, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_dim = input_dim\n",
    "        self.beta = beta\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.MeanSquaredError()(inputs, reconstructed)\n",
    "        ) * self.input_dim\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "        )\n",
    "        \n",
    "        # Total loss with beta parameter\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        \n",
    "        self.add_loss(total_loss)\n",
    "        \n",
    "        return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_vae(hp):\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Hyperparameters\n",
    "    num_layers = hp.Int('num_layers', 1, 5)\n",
    "    units = hp.Int('units', 64, 512, step=64)\n",
    "    activation = hp.Choice('activation', ['relu', 'tanh', 'selu', 'leaky_relu'])\n",
    "    l2_reg = hp.Float('l2_reg', 1e-6, 1e-3, sampling='log')\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.0, 0.5, step=0.1)\n",
    "    encoding_dim = hp.Int('encoding_dim', 8, 64, step=8)\n",
    "    learning_rate = hp.Float('learning_rate', 1e-5, 1e-2, sampling='log')\n",
    "    beta = hp.Float('beta', 1.0, 10.0, step=1.0)\n",
    "    \n",
    "    # Encoder\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    x = input_layer\n",
    "    for _ in range(num_layers):\n",
    "        x = layers.Dense(\n",
    "            units,\n",
    "            activation=activation,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Latent Space\n",
    "    z_mean = layers.Dense(encoding_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(encoding_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    # Encoder Model\n",
    "    encoder = tf.keras.Model(inputs=input_layer, outputs=[z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    # Decoder\n",
    "    latent_inputs = layers.Input(shape=(encoding_dim,))\n",
    "    x = latent_inputs\n",
    "    for _ in range(num_layers):\n",
    "        x = layers.Dense(\n",
    "            units,\n",
    "            activation=activation,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "    outputs = layers.Dense(input_dim, activation='linear')(x)\n",
    "    \n",
    "    # Decoder Model\n",
    "    decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "    \n",
    "    # VAE Model\n",
    "    vae = VAE(encoder, decoder, input_dim=input_dim, beta=beta)\n",
    "    \n",
    "    # Compile the model\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return vae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_vae,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,  # Increased from 20 to 50\n",
    "    executions_per_trial=1,\n",
    "    directory='vae_tuning',\n",
    "    project_name='vitai_vae_enhanced'\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Increased patience\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run the hyperparameter search\n",
    "tuner.search(\n",
    "    X_train, X_train,\n",
    "    epochs=100,  # Increased epochs\n",
    "    batch_size=256,  # Increased batch size to utilize more GPU memory\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Clear memory\n",
    "import gc\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the optimal hyperparameters\n",
    "print(f\"\"\"\n",
    "The optimal number of layers is {best_hps.get('num_layers')} encoder and decoder layers.\n",
    "The optimal number of units in each layer: {best_hps.get('units')}.\n",
    "The optimal activation function is {best_hps.get('activation')}.\n",
    "The optimal encoding dimension is {best_hps.get('encoding_dim')}.\n",
    "The optimal dropout rate is {best_hps.get('dropout_rate')}.\n",
    "The optimal L2 regularization is {best_hps.get('l2_reg')}.\n",
    "The optimal beta value is {best_hps.get('beta')}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build and train the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=200,  # Further increased epochs for best model\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "best_model.save('vae_model_enhanced.keras')\n",
    "\n",
    "# Clear session to free memory\n",
    "keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "custom_objects = {'VAE': VAE, 'Sampling': Sampling}\n",
    "best_model = tf.keras.models.load_model('vae_model_enhanced.keras', custom_objects=custom_objects)\n",
    "\n",
    "# Re-compile the model to include custom loss\n",
    "best_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_hps.get('learning_rate')))\n",
    "\n",
    "# Compute reconstruction error for the entire training dataset\n",
    "reconstructed = best_model.predict(X_train, batch_size=256)\n",
    "reconstruction_errors = np.mean(np.square(X_train - reconstructed), axis=1)\n",
    "\n",
    "# Add reconstruction error to the data\n",
    "train_reconstruction_error = pd.DataFrame({'Reconstruction Error': reconstruction_errors})\n",
    "\n",
    "# Save reconstruction errors\n",
    "train_reconstruction_error.to_csv('train_reconstruction_error.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the reconstruction error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstruction_errors, bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.title('Reconstruction Error Distribution (Training Data)')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Use the encoder model directly from the best_model\n",
    "encoder = best_model.encoder\n",
    "\n",
    "# Obtain latent representation (using z_mean for better clustering)\n",
    "z_mean, z_log_var, z = encoder.predict(X_train, batch_size=256)\n",
    "\n",
    "# Use z_mean for clustering\n",
    "latent_features = z_mean\n",
    "\n",
    "# Create a DataFrame for latent features\n",
    "latent_dim = best_hps.get('encoding_dim')\n",
    "latent_features_df = pd.DataFrame(data=latent_features, columns=[f'latent_{i}' for i in range(latent_dim)])\n",
    "\n",
    "# Add reconstruction error to the latent features DataFrame\n",
    "latent_features_df['reconstruction_error'] = reconstruction_errors\n",
    "\n",
    "# Save the latent features\n",
    "latent_features_df.to_csv('latent_features.csv', index=False)\n",
    "\n",
    "# Clear memory\n",
    "del z_mean, z_log_var, z\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Clustering algorithms to try\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "clustering_algorithms = {\n",
    "    'KMeans': KMeans,\n",
    "    'AgglomerativeClustering': AgglomerativeClustering,\n",
    "    'GaussianMixture': GaussianMixture,\n",
    "    'SpectralClustering': SpectralClustering\n",
    "}\n",
    "\n",
    "# For clustering algorithms, test different numbers of clusters\n",
    "range_n_clusters = list(range(2, 11))\n",
    "\n",
    "best_algorithm = None\n",
    "best_score = -1\n",
    "best_labels = None\n",
    "best_n_clusters = None\n",
    "\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    for n_clusters in range_n_clusters:\n",
    "        if name == 'KMeans':\n",
    "            clustering = algorithm(n_clusters=n_clusters, random_state=42)\n",
    "            cluster_labels = clustering.fit_predict(latent_features)\n",
    "        elif name == 'AgglomerativeClustering':\n",
    "            clustering = algorithm(n_clusters=n_clusters)\n",
    "            cluster_labels = clustering.fit_predict(latent_features)\n",
    "        elif name == 'GaussianMixture':\n",
    "            clustering = algorithm(n_components=n_clusters, random_state=42)\n",
    "            cluster_labels = clustering.fit_predict(latent_features)\n",
    "        elif name == 'SpectralClustering':\n",
    "            clustering = algorithm(n_clusters=n_clusters, assign_labels='discretize', random_state=42)\n",
    "            cluster_labels = clustering.fit_predict(latent_features)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Evaluate clustering\n",
    "        silhouette_avg = silhouette_score(latent_features, cluster_labels)\n",
    "        ch_score = calinski_harabasz_score(latent_features, cluster_labels)\n",
    "        db_score = davies_bouldin_score(latent_features, cluster_labels)\n",
    "        \n",
    "        print(f\"For n_clusters = {n_clusters}, Silhouette Score: {silhouette_avg:.4f}, Calinski-Harabasz Score: {ch_score:.2f}, Davies-Bouldin Score: {db_score:.4f}\")\n",
    "        \n",
    "        # Select the best based on Silhouette Score\n",
    "        if silhouette_avg > best_score:\n",
    "            best_score = silhouette_avg\n",
    "            best_algorithm = name\n",
    "            best_labels = cluster_labels\n",
    "            best_n_clusters = n_clusters\n",
    "\n",
    "print(f\"\\nBest algorithm: {best_algorithm} with {best_n_clusters} clusters and Silhouette Score of {best_score:.4f}\")\n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "latent_features_df['cluster'] = best_labels\n",
    "\n",
    "# Save the updated latent features\n",
    "latent_features_df.to_csv('latent_features_with_clusters.csv', index=False)\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Use t-SNE for visualization\n",
    "print(\"Performing t-SNE...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_jobs=-1)\n",
    "latent_2d = tsne.fit_transform(latent_features)\n",
    "\n",
    "# Add to DataFrame\n",
    "latent_features_df['tsne_1'] = latent_2d[:, 0]\n",
    "latent_features_df['tsne_2'] = latent_2d[:, 1]\n",
    "\n",
    "# Plot t-SNE\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='cluster', data=latent_features_df, palette='viridis', legend='full')\n",
    "plt.title('Latent Space Visualization with t-SNE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Use UMAP for visualization\n",
    "!pip install umap-learn\n",
    "\n",
    "import umap\n",
    "\n",
    "print(\"Performing UMAP...\")\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "latent_2d_umap = reducer.fit_transform(latent_features)\n",
    "\n",
    "# Add to DataFrame\n",
    "latent_features_df['umap_1'] = latent_2d_umap[:, 0]\n",
    "latent_features_df['umap_2'] = latent_2d_umap[:, 1]\n",
    "\n",
    "# Plot UMAP\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='umap_1', y='umap_2', hue='cluster', data=latent_features_df, palette='viridis', legend='full')\n",
    "plt.title('Latent Space Visualization with UMAP')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Map clusters to severity scores\n",
    "cluster_severity = {cluster: index for index, cluster in enumerate(sorted(latent_features_df['cluster'].unique()))}\n",
    "latent_features_df['severity_index'] = latent_features_df['cluster'].map(cluster_severity)\n",
    "\n",
    "# Optionally, scale severity index to 0-10 range\n",
    "scaler_severity = MinMaxScaler(feature_range=(0, 10))\n",
    "latent_features_df['severity_index_scaled'] = scaler_severity.fit_transform(latent_features_df[['severity_index']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Combine original data with cluster labels\n",
    "analysis_df = pd.DataFrame(X_train, columns=structured_data.columns)\n",
    "analysis_df['cluster'] = latent_features_df['cluster']\n",
    "analysis_df['severity_index'] = latent_features_df['severity_index_scaled']\n",
    "analysis_df['reconstruction_error'] = latent_features_df['reconstruction_error']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Group by cluster and compute summary statistics\n",
    "cluster_summary = analysis_df.groupby('cluster').mean()\n",
    "print(\"\\nCluster Summary Statistics:\")\n",
    "display(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize reconstruction error by cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='cluster', y='reconstruction_error', data=analysis_df)\n",
    "plt.title('Reconstruction Error by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize key features by cluster\n",
    "key_features = structured_data.columns.tolist()  # List of feature names\n",
    "\n",
    "# Limit to first 10 features for analysis\n",
    "for feature in key_features[:10]:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='cluster', y=feature, data=analysis_df)\n",
    "    plt.title(f'Distribution of {feature} by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Clear variables to free memory\n",
    "del X_train\n",
    "del X_val\n",
    "del latent_features\n",
    "del latent_2d\n",
    "del latent_2d_umap\n",
    "del encoder\n",
    "del best_model\n",
    "gc.collect()\n",
    "\n",
    "print(\"Analysis complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
