{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder with Enhanced Clustering for Health Severity Analysis\n",
    "This notebook implements an improved Variational Autoencoder (VAE) on structured health data to identify patient clusters corresponding to different levels of health severity. We've incorporated various architecture styles, expanded hyperparameter tuning, and optimized the code for better performance and resource utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# Machine learning and deep learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import keras_tuner\n",
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check GPU Availability and Configure Memory Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Enabled memory growth on GPU\n"
     ]
    }
   ],
   "source": [
    "# Check if TensorFlow is using the GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Enable memory growth for GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Enabled memory growth on GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. The code will run on CPU, which might be slower.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Preprocess the Enhanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Patient Data with Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load patient data with sequences\n",
    "patient_data = pd.read_pickle('Data/patient_data_sequences.pkl')\n",
    "print(\"Loaded patient data with sequences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Code Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load code mappings\n",
    "code_mappings = pd.read_csv('Data/code_mappings.csv')\n",
    "code_to_id = dict(zip(code_mappings['UNIQUE_CODE'], code_mappings['CODE_ID']))\n",
    "id_to_code = dict(zip(code_mappings['CODE_ID'], code_mappings['UNIQUE_CODE']))\n",
    "num_codes = len(code_to_id)\n",
    "print(f\"Number of unique codes: {num_codes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Code Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding dimension\n",
    "embedding_dim = 128  # Adjust as needed\n",
    "\n",
    "# Initialize embeddings randomly\n",
    "np.random.seed(42)\n",
    "code_embeddings = np.random.normal(size=(num_codes, embedding_dim))\n",
    "\n",
    "# Function to get embedding for a code ID\n",
    "def get_code_embedding(code_id):\n",
    "    return code_embeddings[code_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate Embeddings for Each Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate embeddings for a patient\n",
    "def aggregate_patient_embeddings(visits):\n",
    "    all_code_ids = [code_id for visit in visits for code_id in visit]\n",
    "    if not all_code_ids:\n",
    "        return np.zeros(embedding_dim)\n",
    "    embeddings = np.array([get_code_embedding(code_id) for code_id in all_code_ids])\n",
    "    mean_embedding = embeddings.mean(axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "# Aggregate embeddings for each patient\n",
    "patient_embeddings = np.array([\n",
    "    aggregate_patient_embeddings(row['SEQUENCE']) for _, row in patient_data.iterrows()\n",
    "])\n",
    "\n",
    "print(\"Aggregated embeddings for all patients.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Demographic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select demographic features\n",
    "demographic_features = ['Id', 'AGE', 'DECEASED', 'GENDER', 'RACE', 'ETHNICITY',\n",
    "                        'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME']\n",
    "demographics = patient_data[demographic_features]\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "demographics = pd.get_dummies(demographics, columns=['GENDER', 'RACE', 'ETHNICITY'])\n",
    "\n",
    "# Fill missing values\n",
    "demographics.fillna(0, inplace=True)\n",
    "\n",
    "print(\"Prepared demographic features with patient IDs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Embeddings and Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert demographics to NumPy array (excluding the 'Id' column)\n",
    "demographics_array = demographics.drop(columns=['Id']).values\n",
    "\n",
    "# Concatenate embeddings and demographics\n",
    "X = np.hstack((patient_embeddings, demographics_array))\n",
    "\n",
    "print(f\"Final input shape: {X.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save the scaler for future use\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "print(\"Data standardized and scaler saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split\n",
    " \n",
    "We need to split both the features and the patient IDs so that we can track which patients are in the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = demographics['Id'].values\n",
    "\n",
    "# Split into training and validation sets, including patient IDs\n",
    "X_train, X_val, ids_train, ids_val = train_test_split(\n",
    "    X_scaled, patient_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Clear unnecessary variables\n",
    "del X, X_scaled, demographics_array\n",
    "gc.collect()\n",
    "# # Get patient IDs\n",
    "# patient_ids = demographics['Id'].values\n",
    "\n",
    "# # Split into training and validation sets, including patient IDs\n",
    "# X_train, X_val, ids_train, ids_val = train_test_split(\n",
    "#     X, patient_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the data after splitting\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "\n",
    "# # Save the scaler for future use\n",
    "# joblib.dump(scaler, 'scaler.joblib')\n",
    "# print(\"Data standardized and scaler saved.\")\n",
    "\n",
    "# # Clear unnecessary variables\n",
    "# del X, patient_embeddings, demographics_array\n",
    "# gc.collect()\n",
    "# Get patient IDs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Sampling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling Layer\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Sampling, self).get_config()\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Variational Autoencoder (VAE) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Model with adjustable beta parameter\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, input_dim, beta=1.0, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_dim = input_dim\n",
    "        self.beta = beta\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.MeanSquaredError()(inputs, reconstructed)\n",
    "        ) * self.input_dim\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "        )\n",
    "        \n",
    "        # Total loss with beta parameter\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        \n",
    "        self.add_loss(total_loss)\n",
    "        \n",
    "        return reconstructed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(VAE, self).get_config()\n",
    "        config.update({\n",
    "            'encoder': self.encoder.get_config(),\n",
    "            'decoder': self.decoder.get_config(),\n",
    "            'input_dim': self.input_dim,\n",
    "            'beta': self.beta,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        encoder_config = config.pop('encoder')\n",
    "        decoder_config = config.pop('decoder')\n",
    "\n",
    "        encoder = tf.keras.Model.from_config(encoder_config)\n",
    "        decoder = tf.keras.Model.from_config(decoder_config)\n",
    "\n",
    "        return cls(encoder=encoder, decoder=decoder, **config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model Builder Function for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vae(hp):\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Hyperparameters\n",
    "    num_layers = hp.Int('num_layers', 1, 3)\n",
    "    units = hp.Int('units', 64, 512, step=64)\n",
    "    activation = hp.Choice('activation', ['relu', 'tanh', 'selu', 'elu'])\n",
    "    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "    encoding_dim = hp.Int('encoding_dim', 8, 64, step=8)\n",
    "    learning_rate = hp.Float('learning_rate', 1e-5, 1e-3, sampling='log')\n",
    "    beta = hp.Float('beta', 1.0, 5.0, step=0.5)\n",
    "    \n",
    "    # Activation function\n",
    "    activation_fn = activation\n",
    "    \n",
    "    # Encoder\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    x = input_layer\n",
    "    for _ in range(num_layers):\n",
    "        x = layers.Dense(\n",
    "            units,\n",
    "            activation=activation_fn,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Latent Space\n",
    "    z_mean = layers.Dense(encoding_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(encoding_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    # Encoder Model\n",
    "    encoder = tf.keras.Model(inputs=input_layer, outputs=[z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    # Decoder\n",
    "    latent_inputs = layers.Input(shape=(encoding_dim,))\n",
    "    x = latent_inputs\n",
    "    for _ in range(num_layers):\n",
    "        x = layers.Dense(\n",
    "            units,\n",
    "            activation=activation_fn,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "    outputs = layers.Dense(input_dim, activation='linear')(x)\n",
    "    \n",
    "    # Decoder Model\n",
    "    decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "    \n",
    "    # VAE Model\n",
    "    vae = VAE(encoder, decoder, input_dim=input_dim, beta=beta)\n",
    "    \n",
    "    # Compile the model\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return vae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up the Hyperparameter Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BayesianOptimization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ReduceLROnPlateau\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Set up the tuner\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tuner \u001b[38;5;241m=\u001b[39m \u001b[43mBayesianOptimization\u001b[49m(\n\u001b[0;32m      5\u001b[0m     build_vae,\n\u001b[0;32m      6\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m      8\u001b[0m     executions_per_trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      9\u001b[0m     directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvae_tuning\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvitai_vae_enhanced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[0;32m     16\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[0;32m     17\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     19\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     20\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BayesianOptimization' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_vae,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='vae_tuning',\n",
    "    project_name='vitai_vae_enhanced'\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter search\n",
    "tuner.search(\n",
    "    X_train, X_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the optimal hyperparameters\n",
    "print(f\"\"\"\n",
    "The optimal number of layers is {best_hps.get('num_layers')} encoder and decoder layers.\n",
    "The optimal number of units in each layer: {best_hps.get('units')}.\n",
    "The optimal activation function is {best_hps.get('activation')}.\n",
    "The optimal encoding dimension is {best_hps.get('encoding_dim')}.\n",
    "The optimal dropout rate is {best_hps.get('dropout_rate')}.\n",
    "The optimal L2 regularization is {best_hps.get('l2_reg')}.\n",
    "The optimal beta value is {best_hps.get('beta')}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and Train the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save the weights\n",
    "best_model.save_weights('vae_weights.h5')\n",
    "\n",
    "# Save the best hyperparameters\n",
    "import pickle\n",
    "with open('best_hyperparameters.pkl', 'wb') as f:\n",
    "    pickle.dump(best_hps.values, f)\n",
    "\n",
    "# Clear session to free memory\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Training and Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Best Model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best hyperparameters\n",
    "with open('best_hyperparameters.pkl', 'rb') as f:\n",
    "        best_hps_values = pickle.load(f)\n",
    "\n",
    "# Reconstruct the hyperparameters object\n",
    "best_hps = keras_tuner.HyperParameters()\n",
    "best_hps.values = best_hps_values\n",
    "\n",
    "# Rebuild the model architecture\n",
    "best_model = build_vae(best_hps)\n",
    "\n",
    "# Compile the model\n",
    "best_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_hps.get('learning_rate')))\n",
    "\n",
    "# Build the model by calling it on some data\n",
    "dummy_input = tf.zeros((1, X_train.shape[1]))\n",
    "best_model(dummy_input)\n",
    "\n",
    "# Load the weights\n",
    "best_model.load_weights('vae_weights.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reconstruction error for the entire training dataset\n",
    "reconstructed = best_model.predict(X_train, batch_size=256)\n",
    "reconstruction_errors = np.mean(np.square(X_train - reconstructed), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Reconstruction Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reconstruction error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstruction_errors, bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.title('Reconstruction Error Distribution (Training Data)')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain Latent Features from the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the encoder model directly from the best_model\n",
    "encoder = best_model.encoder\n",
    "\n",
    "# Obtain latent representation (using z_mean for better clustering)\n",
    "z_mean, z_log_var, z = encoder.predict(X_train, batch_size=256)\n",
    "\n",
    "# Use z_mean for clustering\n",
    "latent_features = z_mean\n",
    "\n",
    "# Create a DataFrame for latent features, including patient IDs\n",
    "latent_dim = best_hps.get('encoding_dim')\n",
    "latent_features_df = pd.DataFrame(\n",
    "    data=latent_features,\n",
    "    columns=[f'latent_{i}' for i in range(latent_dim)]\n",
    ")\n",
    "latent_features_df['Id'] = ids_train  # Add patient IDs\n",
    "\n",
    "# Add reconstruction error to the latent features DataFrame\n",
    "latent_features_df['reconstruction_error'] = reconstruction_errors\n",
    "\n",
    "# Save the latent features\n",
    "latent_features_df.to_csv('latent_features.csv', index=False)\n",
    "\n",
    "# Clear memory\n",
    "del z_mean, z_log_var, z\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Clustering on Latent Features\n",
    "Note: Some clustering algorithms can consume a significant amount of memory, especially with large datasets. To mitigate memory issues, we'll adjust our approach by using more memory-efficient algorithms like MiniBatchKMeans and sample the data for computing evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of clusters to 10\n",
    "n_clusters = 10\n",
    "\n",
    "# Clustering algorithms to try\n",
    "clustering_algorithms = {\n",
    "    'MiniBatchKMeans': MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=1024),\n",
    "    'GaussianMixture': GaussianMixture(n_components=n_clusters, random_state=42),\n",
    "    'AgglomerativeClustering': AgglomerativeClustering(n_clusters=n_clusters),\n",
    "}\n",
    "\n",
    "best_algorithm = None\n",
    "best_score = -1\n",
    "best_labels = None\n",
    "\n",
    "# Sample a subset of the data for computing evaluation metrics\n",
    "sample_size = min(10000, latent_features.shape[0])  # Use up to 10,000 samples\n",
    "if latent_features.shape[0] > sample_size:\n",
    "    latent_features_sample, _, indices_sample, _ = train_test_split(\n",
    "        latent_features, np.arange(latent_features.shape[0]),\n",
    "        train_size=sample_size, random_state=42)\n",
    "else:\n",
    "    latent_features_sample = latent_features\n",
    "    indices_sample = np.arange(latent_features.shape[0])\n",
    "\n",
    "# Perform clustering\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    try:\n",
    "        cluster_labels = algorithm.fit_predict(latent_features)\n",
    "        \n",
    "        # Evaluate clustering\n",
    "        cluster_labels_sample = cluster_labels[indices_sample]\n",
    "        silhouette_avg = silhouette_score(latent_features_sample, cluster_labels_sample)\n",
    "        print(f\"{name} Silhouette Score: {silhouette_avg:.4f}\")\n",
    "        \n",
    "        # Store the best algorithm based on Silhouette Score\n",
    "        if silhouette_avg > best_score:\n",
    "            best_score = silhouette_avg\n",
    "            best_algorithm = name\n",
    "            best_labels = cluster_labels\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nBest algorithm: {best_algorithm} with Silhouette Score of {best_score:.4f}\")\n",
    "\n",
    "# Update latent_features_df with the best cluster labels\n",
    "latent_features_df['cluster'] = best_labels\n",
    "\n",
    "# Save the updated latent features\n",
    "latent_features_df.to_csv('latent_features_with_clusters.csv', index=False)\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Clusters Using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use t-SNE for visualization\n",
    "print(\"Performing t-SNE...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_jobs=-1)\n",
    "latent_2d = tsne.fit_transform(latent_features_sample)\n",
    "\n",
    "# Add to DataFrame\n",
    "latent_features_df_sample = latent_features_df.iloc[indices_sample].copy()\n",
    "latent_features_df_sample['tsne_1'] = latent_2d[:, 0]\n",
    "latent_features_df_sample['tsne_2'] = latent_2d[:, 1]\n",
    "\n",
    "# Plot t-SNE\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='cluster', data=latent_features_df_sample, palette='viridis', legend='full')\n",
    "plt.title('Latent Space Visualization with t-SNE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Clusters Using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UMAP for visualization\n",
    "print(\"Performing UMAP...\")\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "latent_2d_umap = reducer.fit_transform(latent_features_sample)\n",
    "\n",
    "# Add to DataFrame\n",
    "latent_features_df_sample['umap_1'] = latent_2d_umap[:, 0]\n",
    "latent_features_df_sample['umap_2'] = latent_2d_umap[:, 1]\n",
    "\n",
    "# Plot UMAP\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='umap_1', y='umap_2', hue='cluster', data=latent_features_df_sample, palette='viridis', legend='full')\n",
    "plt.title('Latent Space Visualization with UMAP')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Clusters to Severity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map clusters to severity scores\n",
    "cluster_severity = {cluster: index for index, cluster in enumerate(sorted(latent_features_df['cluster'].unique()))}\n",
    "latent_features_df['severity_index'] = latent_features_df['cluster'].map(cluster_severity)\n",
    "\n",
    "# Optionally, scale severity index to 0-10 range\n",
    "scaler_severity = MinMaxScaler(feature_range=(0, 10))\n",
    "latent_features_df['severity_index_scaled'] = scaler_severity.fit_transform(latent_features_df[['severity_index']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Latent Features with Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge latent features with demographics using patient IDs\n",
    "final_df = latent_features_df.merge(\n",
    "    demographics, on='Id', how='left'\n",
    ")\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final_df.to_csv('patient_clusters.csv', index=False)\n",
    "\n",
    "print(\"Final DataFrame with patient IDs and clusters saved to 'patient_clusters.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Clusters and Visualize Key Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Original Data with Cluster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding column names\n",
    "embedding_columns = [f'embedding_{i}' for i in range(patient_embeddings.shape[1])]\n",
    "\n",
    "# Get demographic column names (excluding 'Id')\n",
    "demographics_columns = demographics.drop(columns=['Id']).columns.tolist()\n",
    "\n",
    "# Combine embedding and demographic column names\n",
    "feature_columns = embedding_columns + demographics_columns\n",
    "\n",
    "# Create DataFrame with the correct column names\n",
    "analysis_df = pd.DataFrame(X_train, columns=feature_columns)\n",
    "\n",
    "# Ensure that the indices align between analysis_df and latent_features_df\n",
    "analysis_df.reset_index(drop=True, inplace=True)\n",
    "latent_features_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add cluster labels and other information\n",
    "analysis_df['cluster'] = latent_features_df['cluster'].values\n",
    "analysis_df['severity_index'] = latent_features_df['severity_index_scaled'].values\n",
    "analysis_df['reconstruction_error'] = latent_features_df['reconstruction_error'].values\n",
    "\n",
    "# Save analysis DataFrame\n",
    "analysis_df.to_csv(\"analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and compute summary statistics\n",
    "cluster_summary = analysis_df.groupby('cluster').mean()\n",
    "print(\"\\nCluster Summary Statistics:\")\n",
    "display(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Reconstruction Error by Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction error by cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='cluster', y='reconstruction_error', data=analysis_df)\n",
    "plt.title('Reconstruction Error by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Key Features by Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the error by excluding 'Id' from the features\n",
    "key_features = demographics.drop(columns=['Id']).columns.tolist()  # Exclude 'Id'\n",
    "\n",
    "# Merge analysis_df with demographics for plotting\n",
    "'''\n",
    "Fix the error by excluding 'Id' from the features\n",
    "We ensure only valid columns are included in the plotting function\n",
    "Additionally, we merge the analysis_df with demographics to include the necessary features for plotting\n",
    "'''\n",
    "plot_df = analysis_df.merge(demographics[['Id'] + key_features], left_index=True, right_index=True)\n",
    "\n",
    "# Limit to first 10 features for analysis\n",
    "for feature in key_features[:10]:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='cluster', y=feature, data=plot_df)\n",
    "    plt.title(f'Distribution of {feature} by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear Variables to Free Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear variables to free memory\n",
    "del X_train\n",
    "del X_val\n",
    "del latent_features\n",
    "del latent_2d\n",
    "del latent_2d_umap\n",
    "del encoder\n",
    "del best_model\n",
    "gc.collect()\n",
    "\n",
    "print(\"Analysis complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
