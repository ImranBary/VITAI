# data_preprocessing.py
# Author: Imran Feisal 
# Date: 31/10/2024
# Description:
# This script loads Synthea data from CSV files, 
# enhances feature extraction from patient demographics,
# handles missing data more robustly,
# aggregates codes from conditions, medications, procedures, and observations, 
# builds sequences of visits for each patient, and saves the processed data for modeling.

import pandas as pd
import numpy as np
from datetime import datetime
import os
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------
# 1. Load Data
# ------------------------------

def load_data(data_dir):
    """
    Load Synthea data from CSV files and preprocess patient demographics.

    Args:
        data_dir (str): Directory where Synthea CSV files are stored.

    Returns:
        patients (pd.DataFrame): Processed patient demographics data.
        encounters (pd.DataFrame): Processed encounters data.
    """
    # Load Patients Data
    patients = pd.read_csv(os.path.join(data_dir, 'patients.csv'), usecols=[
        'Id', 'BIRTHDATE', 'DEATHDATE', 'GENDER', 'RACE', 'ETHNICITY',
        'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME', 'MARITAL'
    ])

    # Convert 'BIRTHDATE' and 'DEATHDATE' to datetime format
    patients['BIRTHDATE'] = pd.to_datetime(patients['BIRTHDATE'], errors='coerce')
    patients['DEATHDATE'] = pd.to_datetime(patients['DEATHDATE'], errors='coerce')

    # Check for future birthdates and deaths before births
    patients = patients[patients['BIRTHDATE'] <= patients['BIRTHDATE'].max()]
    patients = patients[(patients['DEATHDATE'].isnull()) | (patients['DEATHDATE'] >= patients['BIRTHDATE'])]

    # Calculate Age using the latest date in encounters as reference
    encounters_dates = pd.read_csv(os.path.join(data_dir, 'encounters.csv'), usecols=['START', 'STOP'])
    encounters_dates['START'] = pd.to_datetime(encounters_dates['START']).dt.tz_localize(None)
    latest_date = encounters_dates['START'].max()
    patients['AGE'] = (latest_date - patients['BIRTHDATE']).dt.days / 365.25
    patients['AGE'] = patients['AGE'].fillna(0)

    # Calculate if patient is deceased
    patients['DECEASED'] = patients['DEATHDATE'].notnull().astype(int)

    # Calculate age at death
    patients['AGE_AT_DEATH'] = ((patients['DEATHDATE'] - patients['BIRTHDATE']).dt.days / 365.25).fillna(patients['AGE'])

    # Drop unnecessary columns
    patients.drop(columns=['BIRTHDATE', 'DEATHDATE'], inplace=True)

    # Handle missing data using median imputation for numerical features
    numerical_features = ['HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME']
    for col in numerical_features:
        patients[col].fillna(patients[col].median(), inplace=True)

    # Handle missing data for categorical features by creating an 'Unknown' category
    categorical_features = ['GENDER', 'RACE', 'ETHNICITY', 'MARITAL']
    for col in categorical_features:
        patients[col].fillna('Unknown', inplace=True)
        patients[col] = patients[col].replace('', 'Unknown')

    # Load Encounters Data (Visits)
    encounters = pd.read_csv(os.path.join(data_dir, 'encounters.csv'), usecols=[
        'Id', 'PATIENT', 'ENCOUNTERCLASS', 'START', 'STOP', 'REASONCODE', 'REASONDESCRIPTION'
    ])

    # Convert START and STOP to datetime without timezone
    encounters['START'] = pd.to_datetime(encounters['START']).dt.tz_localize(None)
    encounters['STOP'] = pd.to_datetime(encounters['STOP']).dt.tz_localize(None) 

    # Sort encounters by patient and start date
    encounters.sort_values(by=['PATIENT', 'START'], inplace=True)

    logger.info("Data loaded and preprocessed successfully.")

    return patients, encounters

# ------------------------------
# 2. Prepare Visit-Level Data
# ------------------------------

def aggregate_codes(data_dir):
    """
    Aggregate codes from conditions, medications, procedures, and observations.

    Args:
        data_dir (str): Directory where Synthea CSV files are stored.

    Returns:
        codes (pd.DataFrame): Aggregated codes with unified code system.
        code_to_id (dict): Mapping from UNIQUE_CODE to integer IDs.
        id_to_code (dict): Reverse mapping from IDs to UNIQUE_CODE.
    """
    # Load data
    conditions = pd.read_csv(os.path.join(data_dir, 'conditions.csv'), usecols=[
        'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION'
    ])
    medications = pd.read_csv(os.path.join(data_dir, 'medications.csv'), usecols=[
        'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION'
    ])
    procedures = pd.read_csv(os.path.join(data_dir, 'procedures.csv'), usecols=[
        'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION'
    ])
    observations = pd.read_csv(os.path.join(data_dir, 'observations.csv'), usecols=[
        'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION', 'VALUE', 'UNITS'
    ])

    # Combine all codes into a single DataFrame
    conditions['TYPE'] = 'condition'
    medications['TYPE'] = 'medication'
    procedures['TYPE'] = 'procedure'
    observations['TYPE'] = 'observation'

    codes = pd.concat([conditions, medications, procedures, observations], ignore_index=True)

    # Handle missing codes
    codes['CODE'] = codes['CODE'].fillna('UNKNOWN')

    # Create a unified code system
    codes['UNIQUE_CODE'] = codes['TYPE'] + '_' + codes['CODE'].astype(str)

    # Generate a mapping from UNIQUE_CODE to integer IDs
    unique_codes = codes['UNIQUE_CODE'].unique()
    code_to_id = {code: idx for idx, code in enumerate(unique_codes)}
    id_to_code = {idx: code for code, idx in code_to_id.items()}

    # Map codes to IDs
    codes['CODE_ID'] = codes['UNIQUE_CODE'].map(code_to_id)

    logger.info("Codes aggregated successfully.")

    return codes, code_to_id, id_to_code

# ------------------------------
# 3. Build Patient Sequences
# ------------------------------

def build_patient_sequences(encounters, codes):
    """
    Build sequences of visits for each patient.

    Args:
        encounters (pd.DataFrame): Encounters data.
        codes (pd.DataFrame): Aggregated codes.

    Returns:
        patient_sequences (dict): Mapping of patient IDs to sequences of visits.
    """
    # Create a mapping from ENCOUNTER to CODE_IDs
    encounter_code_map = codes.groupby('ENCOUNTER')['CODE_ID'].apply(list)

    # Merge encounters with codes
    encounters_with_codes = encounters[['Id', 'PATIENT']].merge(encounter_code_map, left_on='Id', right_on='ENCOUNTER', how='left')

    # Group by patient and collect sequences
    patient_sequences = encounters_with_codes.groupby('PATIENT')['CODE_ID'].apply(list).to_dict()

    logger.info("Patient sequences built successfully.")

    return patient_sequences

# ------------------------------
# 4. Save Processed Data
# ------------------------------

def save_processed_data(patients, patient_sequences, code_to_id, output_dir):
    """
    Save the processed data for modeling.

    Args:
        patients (pd.DataFrame): Patient demographics data.
        patient_sequences (dict): Patient sequences data.
        code_to_id (dict): Code to ID mapping.
        output_dir (str): Directory to save processed data.
    """
    # Convert patient_sequences to a DataFrame
    patient_sequence_df = pd.DataFrame([
        {'PATIENT': patient_id, 'SEQUENCE': visits}
        for patient_id, visits in patient_sequences.items()
    ])

    # Merge with patient demographics
    patient_data = patients.merge(patient_sequence_df, how='inner', left_on='Id', right_on='PATIENT')

    # Drop redundant 'PATIENT' column
    patient_data.drop(columns=['PATIENT'], inplace=True)

    # Save code mappings
    code_mappings = pd.DataFrame(list(code_to_id.items()), columns=['UNIQUE_CODE', 'CODE_ID'])
    code_mappings.to_csv(os.path.join(output_dir, 'code_mappings.csv'), index=False)

    # Save patient data with sequences
    patient_data.to_pickle(os.path.join(output_dir, 'patient_data_sequences.pkl'))

    logger.info("Data aggregation complete. Processed data saved.")

# ------------------------------
# Main Execution
# ------------------------------

def main():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(script_dir, 'Data')
    output_dir = data_dir  # same directory for outputs
    os.makedirs(output_dir, exist_ok=True)

    # Load data
    patients, encounters = load_data(data_dir)

    # Aggregate codes
    codes, code_to_id, id_to_code = aggregate_codes(data_dir)

    # Build patient sequences
    patient_sequences = build_patient_sequences(encounters, codes)

    # Save processed data
    save_processed_data(patients, patient_sequences, code_to_id, output_dir)

if __name__ == '__main__':
    main()





















'''
import pandas as pd
import numpy as np
from datetime import datetime
import os
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------
# 1. Load Data
# ------------------------------

def load_data(data_dir):
    """Load Synthea data from CSV files."""
    # Load Patients Data
    patients = pd.read_csv(os.path.join(data_dir, 'patients.csv'), usecols=[
        'Id', 'BIRTHDATE', 'DEATHDATE', 'GENDER', 'RACE', 'ETHNICITY',
        'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME', 'MARITAL'
    ])

    # Convert 'BIRTHDATE' and 'DEATHDATE' to datetime format
    patients['BIRTHDATE'] = pd.to_datetime(patients['BIRTHDATE'], errors='coerce')
    patients['DEATHDATE'] = pd.to_datetime(patients['DEATHDATE'], errors='coerce')

    # Check for future birthdates and deaths before births
    patients = patients[patients['BIRTHDATE'] <= datetime.now()]
    patients = patients[(patients['DEATHDATE'].isnull()) | (patients['DEATHDATE'] >= patients['BIRTHDATE'])]

    # Calculate Age
    current_date = datetime.now()
    patients['AGE'] = (current_date - patients['BIRTHDATE']).dt.days / 365.25
    patients['AGE'] = patients['AGE'].fillna(0)

    # Calculate if patient is deceased
    patients['DECEASED'] = patients['DEATHDATE'].notnull().astype(int)

    # Calculate age at death
    patients['AGE_AT_DEATH'] = ((patients['DEATHDATE'] - patients['BIRTHDATE']).dt.days / 365.25).fillna(patients['AGE'])

    # Drop unnecessary columns
    patients.drop(columns=['BIRTHDATE', 'DEATHDATE'], inplace=True)

    # Handle missing data using mean imputation for numerical features
    numerical_features = ['HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME']
    for col in numerical_features:
        patients[col].fillna(patients[col].mean(), inplace=True)

    # Handle missing data for categorical features by creating an 'Unknown' category
    categorical_features = ['GENDER', 'RACE', 'ETHNICITY', 'MARITAL']
    for col in categorical_features:
        patients[col].fillna('Unknown', inplace=True)

    # Load Encounters Data (Visits)
    encounters = pd.read_csv(os.path.join(data_dir, 'encounters.csv'), usecols=[
        'Id', 'PATIENT', 'ENCOUNTERCLASS', 'START', 'STOP', 'REASONCODE', 'REASONDESCRIPTION'
    ])

    # Convert START and STOP to datetime without timezone
    encounters['START'] = pd.to_datetime(encounters['START']).dt.tz_localize(None)
    encounters['STOP'] = pd.to_datetime(encounters['STOP']).dt.tz_localize(None) 

    # Sort encounters by patient and start date
    encounters.sort_values(by=['PATIENT', 'START'], inplace=True)

    logger.info("Data loaded and preprocessed successfully.")

    return patients, encounters

# ------------------------------
# 2. Prepare Visit-Level Data
# ------------------------------

def aggregate_codes(data_dir):
    """Aggregate codes from conditions, medications, procedures, and observations."""
    # Load data
    conditions = pd.read_csv(os.path.join(data_dir, 'conditions.csv'), usecols=[
        'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION'
    ])
    medications = pd.read_csv(os.path.join(data_dir, 'medications.csv'), usecols=[
        'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION'
    ])
    procedures = pd.read_csv(os.path.join(data_dir, 'procedures.csv'), usecols=[
        'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION'
    ])
    observations = pd.read_csv(os.path.join(data_dir, 'observations.csv'), usecols=[
        'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION', 'VALUE', 'UNITS'
    ])

    # Combine all codes into a single DataFrame
    codes = pd.concat([
        conditions.assign(TYPE='condition'),
        medications.assign(TYPE='medication'),
        procedures.assign(TYPE='procedure'),
        observations.assign(TYPE='observation')
    ], ignore_index=True)

    # Handle missing codes
    codes['CODE'] = codes['CODE'].fillna('UNKNOWN')

    # Create a unified code system
    codes['UNIQUE_CODE'] = codes['TYPE'] + '_' + codes['CODE'].astype(str)

    # Generate a mapping from UNIQUE_CODE to integer IDs
    unique_codes = codes['UNIQUE_CODE'].unique()
    code_to_id = {code: idx for idx, code in enumerate(unique_codes)}
    id_to_code = {idx: code for code, idx in code_to_id.items()}

    # Map codes to IDs
    codes['CODE_ID'] = codes['UNIQUE_CODE'].map(code_to_id)

    logger.info("Codes aggregated successfully.")

    return codes, code_to_id, id_to_code

# ------------------------------
# 3. Build Patient Sequences
# ------------------------------

def build_patient_sequences(encounters, codes):
    """Build sequences of visits for each patient."""
    # Create a mapping from ENCOUNTER to CODE_IDs
    encounter_code_map = codes.groupby('ENCOUNTER')['CODE_ID'].apply(list).to_dict()

    # Create a mapping from PATIENT to ENCOUNTER IDs
    patient_encounter_map = encounters.groupby('PATIENT')['Id'].apply(list).to_dict()

    # Build sequences
    patient_sequences = {}
    for patient_id, encounter_ids in patient_encounter_map.items():
        patient_visits = []
        for visit_id in encounter_ids:
            visit_codes = encounter_code_map.get(visit_id, [])
            if visit_codes:
                patient_visits.append(visit_codes)
        if patient_visits:
            patient_sequences[patient_id] = patient_visits

    logger.info("Patient sequences built successfully.")

    return patient_sequences

# ------------------------------
# 4. Save Processed Data
# ------------------------------

def save_processed_data(patients, patient_sequences, code_to_id, output_dir):
    """Save the processed data for modeling."""
    # Convert patient_sequences to a DataFrame
    patient_sequence_df = pd.DataFrame([
        {'PATIENT': patient_id, 'SEQUENCE': visits}
        for patient_id, visits in patient_sequences.items()
    ])

    # Merge with patient demographics
    patient_data = patients.merge(patient_sequence_df, how='inner', left_on='Id', right_on='PATIENT')

    # Drop redundant 'PATIENT' column
    patient_data.drop(columns=['PATIENT'], inplace=True)

    # Save code mappings
    code_mappings = pd.DataFrame(list(code_to_id.items()), columns=['UNIQUE_CODE', 'CODE_ID'])
    code_mappings.to_csv(os.path.join(output_dir, 'code_mappings.csv'), index=False)

    # Save patient data with sequences
    patient_data.to_pickle(os.path.join(output_dir, 'patient_data_sequences.pkl'))

    logger.info("Data aggregation complete. Processed data saved.")

# ------------------------------
# Main Execution
# ------------------------------

def main():
    data_dir = r'E:\DataGen\synthea\output\csv'  
    output_dir = 'Data'
    os.makedirs(output_dir, exist_ok=True)

    # Load data
    patients, encounters = load_data(data_dir)

    # Aggregate codes
    codes, code_to_id, id_to_code = aggregate_codes(data_dir)

    # Build patient sequences
    patient_sequences = build_patient_sequences(encounters, codes)

    # Save processed data
    save_processed_data(patients, patient_sequences, code_to_id, output_dir)

if __name__ == '__main__':
    main()
'''
# health_index.py
# Author: Imran Feisal
# Date: 31/10/2024
# Description:
# Calculate the composite health index for each patient by grouping SNOMED CT codes
# into clinically meaningful categories, assigning weights, and calculating a health index.

import pandas as pd
import numpy as np
import os
import logging
from sklearn.decomposition import PCA

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------
# 1. Load Processed Data
# ------------------------------

def load_processed_data(output_dir):
    """
    Load the processed patient data and code mappings.

    Args:
        output_dir (str): Directory where processed data is stored.

    Returns:
        patient_data (pd.DataFrame): Patient data with sequences.
        code_mappings (pd.DataFrame): Code mappings.
    """
    patient_data = pd.read_pickle(os.path.join(output_dir, 'patient_data_sequences.pkl'))
    code_mappings = pd.read_csv(os.path.join(output_dir, 'code_mappings.csv'))
    logger.info("Processed data loaded successfully.")
    return patient_data, code_mappings

# ------------------------------
# 2. Calculate Health Indicators
# ------------------------------

def calculate_health_indicators(patient_data, data_dir):
    """
    Calculate health indicators for each patient.

    Args:
        patient_data (pd.DataFrame): Patient data with sequences.
        data_dir (str): Directory where raw data is stored.

    Returns:
        patient_data (pd.DataFrame): Patient data with health indicators.
    """
    # Load additional data needed
    conditions = pd.read_csv(os.path.join(data_dir, 'conditions.csv'), usecols=[
        'PATIENT', 'CODE', 'DESCRIPTION'
    ])
    encounters = pd.read_csv(os.path.join(data_dir, 'encounters.csv'), usecols=[
        'Id', 'PATIENT', 'ENCOUNTERCLASS'
    ])
    medications = pd.read_csv(os.path.join(data_dir, 'medications.csv'), usecols=[
        'PATIENT', 'CODE', 'DESCRIPTION'
    ])
    observations = pd.read_csv(os.path.join(data_dir, 'observations.csv'), usecols=[
        'PATIENT', 'DESCRIPTION', 'VALUE', 'UNITS'
    ])

    # -----------------------------------------
    # 2.1 Calculate Comorbidity Score using SNOMED CT groups
    # -----------------------------------------

    # Define SNOMED CT code groups with actual codes
    snomed_groups = {
        'Cardiovascular Diseases': ['53741008', '445118002', '59621000', '22298006', '56265001'],
        'Respiratory Diseases': ['19829001', '233604007', '118940003', '409622000', '13645005'],
        'Diabetes': ['44054006', '73211009', '46635009', '190330002'],
        'Cancer': ['363346000', '254637007', '363406005', '254632001'],
        'Chronic Kidney Disease': ['709044004', '90708001', '46177005'],
        'Neurological Disorders': ['230690007', '26929004', '193003'],
        # Add more groups and codes as needed
    }

    # Assign weights to groups based on clinical significance
    group_weights = {
        'Cardiovascular Diseases': 3,
        'Respiratory Diseases': 2,
        'Diabetes': 2,
        'Cancer': 3,
        'Chronic Kidney Disease': 2,
        'Neurological Disorders': 1.5,
        'Other': 1  # Assign a default weight to other conditions
        # Adjust weights as appropriate
    }

    # Function to find group for a given code
    def find_group(code, snomed_groups):
        for group, codes in snomed_groups.items():
            if str(code) in codes:
                return group
        return 'Other'

    # Map codes to groups
    conditions['Group'] = conditions['CODE'].apply(lambda x: find_group(x, snomed_groups))

    # Assign weights to conditions
    conditions['Group_Weight'] = conditions['Group'].map(group_weights)
    conditions['Group_Weight'] = conditions['Group_Weight'].fillna(1)  # Assign default weight if not found

    # Sum comorbidity weights per patient
    comorbidity_scores = conditions.groupby('PATIENT')['Group_Weight'].sum().reset_index()
    comorbidity_scores.rename(columns={'Group_Weight': 'Comorbidity_Score'}, inplace=True)

    # -----------------------------------------
    # 2.2 Calculate Hospitalizations Count
    # -----------------------------------------
    # Filter encounters for inpatient class
    hospitalizations = encounters[encounters['ENCOUNTERCLASS'] == 'inpatient']
    hospitalizations_count = hospitalizations.groupby('PATIENT').size().reset_index(name='Hospitalizations_Count')

    # -----------------------------------------
    # 2.3 Calculate Medications Count
    # -----------------------------------------
    medications_count = medications.groupby('PATIENT')['CODE'].nunique().reset_index(name='Medications_Count')

    # -----------------------------------------
    # 2.4 Calculate Abnormal Observations Count
    # -----------------------------------------
    # Define thresholds for abnormal observations based on clinical guidelines
    observation_thresholds = {
        'Systolic Blood Pressure': {'min': 90, 'max': 120},
        'Diastolic Blood Pressure': {'min': 60, 'max': 80},
        'Body Mass Index': {'min': 18.5, 'max': 24.9},
        'Blood Glucose Level': {'min': 70, 'max': 99},
        'Heart Rate': {'min': 60, 'max': 100},
        # Add more observations with thresholds as needed
    }

    # Map observation descriptions to standardized names
    observation_mappings = {
        'Systolic Blood Pressure': ['Systolic Blood Pressure'],
        'Diastolic Blood Pressure': ['Diastolic Blood Pressure'],
        'Body Mass Index': ['Body mass index (BMI) [Ratio]'],
        'Blood Glucose Level': ['Glucose [Mass/volume] in Blood'],
        'Heart Rate': ['Heart rate'],
        # Add more mappings as needed
    }

    # Normalize observation descriptions
    observations['DESCRIPTION'] = observations['DESCRIPTION'].str.strip()

    # Convert 'VALUE' to numeric, coercing errors to NaN
    observations['VALUE'] = pd.to_numeric(observations['VALUE'], errors='coerce')

    # Initialize abnormal flag to zero
    observations['IS_ABNORMAL'] = 0

    # Iterate through each standardized observation and thresholds
    for standard_desc, desc_list in observation_mappings.items():
        thresholds = observation_thresholds.get(standard_desc, {})
        min_val = thresholds.get('min', -np.inf)
        max_val = thresholds.get('max', np.inf)
        mask = observations['DESCRIPTION'].isin(desc_list)

        # Apply threshold checks
        observations.loc[
            mask & observations['VALUE'].notna() & (
                (observations['VALUE'] < min_val) |
                (observations['VALUE'] > max_val)
            ),
            'IS_ABNORMAL'
        ] = 1

    # Group by patient to count abnormal observations
    abnormal_observations_count = observations.groupby('PATIENT')['IS_ABNORMAL'].sum().reset_index()
    abnormal_observations_count.rename(columns={'IS_ABNORMAL': 'Abnormal_Observations_Count'}, inplace=True)

    # -----------------------------------------
    # 2.5 Merge Counts into patient_data
    # -----------------------------------------
    # Set the index of patient_data to 'Id' for efficient merging
    patient_data.set_index('Id', inplace=True)

    # Merge Comorbidity Score
    patient_data = patient_data.merge(comorbidity_scores.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Merge Hospitalizations Count
    patient_data = patient_data.merge(hospitalizations_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Merge Medications Count
    patient_data = patient_data.merge(medications_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Merge Abnormal Observations Count
    patient_data = patient_data.merge(abnormal_observations_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Reset index to have 'Id' as a column again
    patient_data.reset_index(inplace=True)

    # -----------------------------------------
    # 2.6 Fill NaN values appropriately
    # -----------------------------------------
    indicators = ['Comorbidity_Score', 'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count']
    patient_data[indicators] = patient_data[indicators].fillna(0)

    logger.info("Health indicators calculated successfully.")

    return patient_data

# ------------------------------
# 3. Calculate Composite Health Index
# ------------------------------

def calculate_health_index(patient_data):
    """
    Calculate the composite health index using PCA for weights.

    Args:
        patient_data (pd.DataFrame): Patient data with health indicators.

    Returns:
        patient_data (pd.DataFrame): Patient data with health index.
    """
    # Define indicators
    indicators = ['AGE', 'Comorbidity_Score', 'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count']

    # Normalize indicators using Robust Scaler to reduce the influence of outliers
    from sklearn.preprocessing import RobustScaler
    scaler = RobustScaler()
    scaled_indicators = scaler.fit_transform(patient_data[indicators])

    # Perform PCA to determine weights
    pca = PCA(n_components=1)
    principal_components = pca.fit_transform(scaled_indicators)
    weights = pca.components_[0]
    weights = weights / np.sum(np.abs(weights))  # Normalize weights

    # Check PCA explained variance
    explained_variance = pca.explained_variance_ratio_[0]
    logger.info(f"PCA explained variance ratio: {explained_variance:.4f}")

    # Calculate health index
    patient_data['Health_Index'] = np.dot(scaled_indicators, weights)

    # Scale Health_Index to range 1 to 10
    min_hi = patient_data['Health_Index'].min()
    max_hi = patient_data['Health_Index'].max()
    patient_data['Health_Index'] = 1 + 9 * (patient_data['Health_Index'] - min_hi) / (max_hi - min_hi + 1e-8)

    logger.info("Composite health index calculated successfully.")

    return patient_data

# ------------------------------
# 4. Save Health Index
# ------------------------------

def save_health_index(patient_data, output_dir):
    """
    Save the patient data with health index.

    Args:
        patient_data (pd.DataFrame): Patient data with health index.
        output_dir (str): Directory to save the data.
    """
    patient_data.to_pickle(os.path.join(output_dir, 'patient_data_with_health_index.pkl'))
    logger.info("Health index calculated and saved.")

# ------------------------------
# Main Execution
# ------------------------------

def main():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(script_dir, 'Data')
    output_dir = data_dir  # same directory for outputs
    os.makedirs(output_dir, exist_ok=True)

    # Load processed data
    patient_data, code_mappings = load_processed_data(output_dir)

    # Calculate health indicators
    patient_data = calculate_health_indicators(patient_data, data_dir)

    # Calculate health index
    patient_data = calculate_health_index(patient_data)

    # Save health index
    save_health_index(patient_data, output_dir)

if __name__ == '__main__':
    main()


'''
import pandas as pd
import numpy as np
import os
import logging
from sklearn.decomposition import PCA

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------
# 1. Load Processed Data
# ------------------------------

def load_processed_data(output_dir):
    """Load the processed patient data and code mappings."""
    patient_data = pd.read_pickle(os.path.join(output_dir, 'patient_data_sequences.pkl'))
    code_mappings = pd.read_csv(os.path.join(output_dir, 'code_mappings.csv'))
    logger.info("Processed data loaded successfully.")
    return patient_data, code_mappings

# ------------------------------
# 2. Calculate Health Indicators
# ------------------------------

def calculate_health_indicators(patient_data, data_dir):
    """Calculate health indicators for each patient."""
    # Load additional data needed
    conditions = pd.read_csv(os.path.join(data_dir, 'conditions.csv'), usecols=[
        'PATIENT', 'CODE', 'DESCRIPTION'
    ])
    encounters = pd.read_csv(os.path.join(data_dir, 'encounters.csv'), usecols=[
        'Id', 'PATIENT', 'ENCOUNTERCLASS'
    ])
    medications = pd.read_csv(os.path.join(data_dir, 'medications.csv'), usecols=[
        'PATIENT', 'CODE', 'DESCRIPTION'
    ])
    observations = pd.read_csv(os.path.join(data_dir, 'observations.csv'), usecols=[
        'PATIENT', 'DESCRIPTION', 'VALUE', 'UNITS'
    ])

    # -----------------------------------------
    # 2.1 Calculate Comorbidity Score using SNOMED CT groups
    # -----------------------------------------

    # Define SNOMED CT code groups (Replace with actual codes from your dataset)
    snomed_groups = {
        'Cardiovascular Diseases': ['53741008', '445118002', '59621000'],  # Example SNOMED CT codes
        'Respiratory Diseases': ['19829001', '233604007', '118940003'],
        'Diabetes': ['44054006', '73211009'],
        'Cancer': ['363346000', '254637007'],
        'Chronic Kidney Disease': ['709044004', '90708001'],
        'Neurological Disorders': ['230690007', '26929004'],
        # Add more groups and codes as needed
    }

    # Assign weights to groups based on clinical significance
    group_weights = {
        'Cardiovascular Diseases': 2,
        'Respiratory Diseases': 1.5,
        'Diabetes': 1.5,
        'Cancer': 2,
        'Chronic Kidney Disease': 1.5,
        'Neurological Disorders': 1,
        'Other': 0.5  # Assign a small weight to other conditions
        # Adjust weights as appropriate
    }

    # Function to find group for a given code
    def find_group(code, snomed_groups):
        for group, codes in snomed_groups.items():
            if str(code) in codes:
                return group
        return 'Other'

    # Map codes to groups
    conditions['Group'] = conditions['CODE'].apply(lambda x: find_group(x, snomed_groups))

    # Assign weights to conditions
    conditions['Group_Weight'] = conditions['Group'].map(group_weights)
    conditions['Group_Weight'] = conditions['Group_Weight'].fillna(0)  # Assign 0 weight if not found

    # Sum comorbidity weights per patient
    comorbidity_scores = conditions.groupby('PATIENT')['Group_Weight'].sum().reset_index()
    comorbidity_scores.rename(columns={'Group_Weight': 'Comorbidity_Score'}, inplace=True)

    # -----------------------------------------
    # 2.2 Calculate Hospitalizations Count
    # -----------------------------------------
    # Filter encounters for inpatient class
    hospitalizations = encounters[encounters['ENCOUNTERCLASS'] == 'inpatient']
    hospitalizations_count = hospitalizations.groupby('PATIENT').size().reset_index(name='Hospitalizations_Count')

    # -----------------------------------------
    # 2.3 Calculate Medications Count
    # -----------------------------------------
    medications_count = medications.groupby('PATIENT')['CODE'].nunique().reset_index(name='Medications_Count')

    # -----------------------------------------
    # 2.4 Calculate Abnormal Observations Count
    # -----------------------------------------
    # Define thresholds for abnormal observations based on clinical guidelines
    observation_thresholds = {
        'Systolic Blood Pressure': {'min': 90, 'max': 120},
        'Diastolic Blood Pressure': {'min': 60, 'max': 80},
        'Body mass index (BMI) [Ratio]': {'min': 18.5, 'max': 24.9},
        'Glucose [Mass/volume] in Blood': {'min': 70, 'max': 99},
        'Heart rate': {'min': 60, 'max': 100},
        # Add more observations with thresholds as needed
    }

    # Convert 'VALUE' to numeric, coercing errors to NaN
    observations['VALUE'] = pd.to_numeric(observations['VALUE'], errors='coerce')

    # Initialize abnormal flag to zero
    observations['IS_ABNORMAL'] = 0

    # Iterate through each observation description and threshold
    for obs_desc, thresholds in observation_thresholds.items():
        # Create a mask for the current observation
        mask = observations['DESCRIPTION'] == obs_desc

        # Apply threshold checks only on rows with numeric 'VALUE'
        observations.loc[
            mask & observations['VALUE'].notna() & (
                (observations['VALUE'] < thresholds['min']) |
                (observations['VALUE'] > thresholds['max'])
            ),
            'IS_ABNORMAL'
        ] = 1

    # Group by patient to count abnormal observations
    abnormal_observations_count = observations.groupby('PATIENT')['IS_ABNORMAL'].sum().reset_index()
    abnormal_observations_count.rename(columns={'IS_ABNORMAL': 'Abnormal_Observations_Count'}, inplace=True)

    # -----------------------------------------
    # 2.5 Merge Counts into patient_data
    # -----------------------------------------
    # Ensure that the 'Id' column in patient_data matches 'PATIENT' in counts

    # Set the index of patient_data to 'Id' for efficient merging
    patient_data.set_index('Id', inplace=True)

    # Merge Comorbidity Score
    patient_data = patient_data.merge(comorbidity_scores.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Merge Hospitalizations Count
    patient_data = patient_data.merge(hospitalizations_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Merge Medications Count
    patient_data = patient_data.merge(medications_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Merge Abnormal Observations Count
    patient_data = patient_data.merge(abnormal_observations_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Reset index to have 'Id' as a column again
    patient_data.reset_index(inplace=True)

    # -----------------------------------------
    # 2.6 Fill NaN values with zeros
    # -----------------------------------------
    indicators = ['Comorbidity_Score', 'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count']
    patient_data[indicators] = patient_data[indicators].fillna(0)

    logger.info("Health indicators calculated successfully.")

    return patient_data

# ------------------------------
# 3. Calculate Composite Health Index
# ------------------------------

def calculate_health_index(patient_data):
    """Calculate the composite health index using PCA for weights."""
    # Define indicators
    indicators = ['AGE', 'Comorbidity_Score', 'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count']

    # Normalize indicators using Robust Scaler to reduce the influence of outliers
    from sklearn.preprocessing import RobustScaler
    scaler = RobustScaler()
    patient_data[indicators] = scaler.fit_transform(patient_data[indicators])

    # Perform PCA to determine weights
    pca = PCA(n_components=1)
    pca.fit(patient_data[indicators])
    weights = pca.components_[0]
    weights = weights / np.sum(np.abs(weights))  # Normalize weights

    # Calculate health index
    patient_data['Health_Index'] = np.dot(patient_data[indicators], weights)

    # Scale Health_Index to range 1 to 10
    min_hi = patient_data['Health_Index'].min()
    max_hi = patient_data['Health_Index'].max()
    patient_data['Health_Index'] = 1 + 9 * (patient_data['Health_Index'] - min_hi) / (max_hi - min_hi + 1e-8)

    logger.info("Composite health index calculated successfully.")

    return patient_data

# ------------------------------
# 4. Save Health Index
# ------------------------------

def save_health_index(patient_data, output_dir):
    """Save the patient data with health index."""
    patient_data.to_pickle(os.path.join(output_dir, 'patient_data_with_health_index.pkl'))
    logger.info("Health index calculated and saved.")

# ------------------------------
# Main Execution
# ------------------------------

def main():
    output_dir = 'Data'
    data_dir = r'E:\DataGen\synthea\output\csv'

    # Load processed data
    patient_data, code_mappings = load_processed_data(output_dir)

    # Calculate health indicators
    patient_data = calculate_health_indicators(patient_data, data_dir)

    # Calculate health index
    patient_data = calculate_health_index(patient_data)

    # Save health index
    save_health_index(patient_data, output_dir)

if __name__ == '__main__':
    main()

'''
'''
# health_index.py
# Author: Imran Feisal
# Date: 31/10/2024
# Description: Calculate the composite health index for each patient and save it for use in modeling.

import pandas as pd
import numpy as np
import os

# ------------------------------
# 1. Load Processed Data
# ------------------------------

def load_processed_data(output_dir):
    """Load the processed patient data and code mappings."""
    patient_data = pd.read_pickle(os.path.join(output_dir, 'patient_data_sequences.pkl'))
    code_mappings = pd.read_csv(os.path.join(output_dir, 'code_mappings.csv'))
    return patient_data, code_mappings

# ------------------------------
# 2. Calculate Health Indicators
# ------------------------------

def calculate_health_indicators(patient_data, data_dir):
    """Calculate health indicators for each patient."""
    # Load additional data needed
    conditions = pd.read_csv(os.path.join(data_dir, 'conditions.csv'), usecols=[
        'PATIENT', 'CODE', 'DESCRIPTION'
    ])
    encounters = pd.read_csv(os.path.join(data_dir, 'encounters.csv'), usecols=[
        'Id', 'PATIENT', 'ENCOUNTERCLASS'
    ])
    medications = pd.read_csv(os.path.join(data_dir, 'medications.csv'), usecols=[
        'PATIENT', 'CODE', 'DESCRIPTION'
    ])
    observations = pd.read_csv(os.path.join(data_dir, 'observations.csv'), usecols=[
        'PATIENT', 'DESCRIPTION', 'VALUE', 'UNITS'
    ])

    # -----------------------------------------
    # 2.1 Calculate Chronic Conditions Count
    # -----------------------------------------
    # Define a list of chronic conditions based on your data
    chronic_conditions_list = [
        # From your unique condition descriptions, extract chronic conditions
        'Asthma',
        'Chronic sinusitis (disorder)',
        'Chronic obstructive lung disease',
        'Chronic kidney disease stage 1 (disorder)',
        'Chronic kidney disease stage 2 (disorder)',
        'Chronic kidney disease stage 3 (disorder)',
        'Chronic kidney disease stage 4 (disorder)',
        'Chronic neck pain (finding)',
        'Chronic low back pain (finding)',
        'Chronic congestive heart failure (disorder)',
        'Chronic pain',
        'Chronic obstructive bronchitis (disorder)',
        'Chronic hepatitis C (disorder)',
        'Chronic intractable migraine without aura',
        'Chronic type B viral hepatitis (disorder)',
        'Diabetes mellitus type 2 (disorder)',
        'Essential hypertension (disorder)',
        'Heart failure (disorder)',
        'Human immunodeficiency virus infection (disorder)',
        'Ischemic heart disease (disorder)',
        'Metabolic syndrome X (disorder)',
        'Multiple myeloma (disorder)',
        'Osteoarthritis of knee',
        'Osteoarthritis of hip',
        'Pulmonary emphysema (disorder)',
        'Rheumatoid arthritis',
        'Chronic pain (finding)',
        'Epilepsy (disorder)',
        'Fibromyalgia (disorder)',
        'Obstructive sleep apnea syndrome (disorder)',
        'Seizure disorder',
        'Sleep apnea (disorder)',
        'Cerebral palsy (disorder)',
        'Spasticity (finding)',
        'Stroke',
        'Depression',
        'Anemia (disorder)',
        'Gout',
        'Hypoxemia (disorder)',
        'Neuropathy due to type 2 diabetes mellitus (disorder)',
        'Retinopathy due to type 2 diabetes mellitus (disorder)',
        'Chronic renal failure (disorder)',
        'Chronic respiratory failure (disorder)',
        # Add more conditions as needed
    ]

    # Map condition descriptions to chronic conditions
    conditions['IS_CHRONIC'] = conditions['DESCRIPTION'].isin(chronic_conditions_list).astype(int)
    chronic_conditions_count = conditions.groupby('PATIENT')['IS_CHRONIC'].sum().reset_index()
    chronic_conditions_count.rename(columns={'IS_CHRONIC': 'Chronic_Conditions_Count'}, inplace=True)

    # -----------------------------------------
    # 2.2 Calculate Hospitalizations Count
    # -----------------------------------------
    # Filter encounters for inpatient class
    hospitalizations = encounters[encounters['ENCOUNTERCLASS'] == 'inpatient']
    hospitalizations_count = hospitalizations.groupby('PATIENT').size().reset_index(name='Hospitalizations_Count')

    # -----------------------------------------
    # 2.3 Calculate Medications Count
    # -----------------------------------------
    medications_count = medications.groupby('PATIENT')['CODE'].nunique().reset_index(name='Medications_Count')

    # -----------------------------------------
    # 2.4 Calculate Abnormal Observations Count
    # -----------------------------------------
    # Define abnormal observations based on descriptions
    abnormal_observations_list = [
        # From your unique observation descriptions, identify those indicating abnormal results
        'Hemoglobin A1c/Hemoglobin.total in Blood',
        'Glucose [Mass/volume] in Blood',
        'Cholesterol [Mass/volume] in Serum or Plasma',
        'Triglycerides',
        'Low Density Lipoprotein Cholesterol',
        'Cholesterol in HDL [Mass/volume] in Serum or Plasma',
        'Blood Pressure',
        'Body mass index (BMI) [Ratio]',
        'Oxygen saturation in Arterial blood',
        'Troponin I.cardiac [Mass/volume] in Serum or Plasma by High sensitivity method',
        'C reactive protein [Mass/volume] in Serum or Plasma',
        'Prothrombin time (PT)',
        'INR in Platelet poor plasma by Coagulation assay',
        'Creatinine [Mass/volume] in Blood',
        'Urea nitrogen [Mass/volume] in Blood',
        'Glomerular filtration rate/1.73 sq M.predicted',
        # Add more observations as needed
    ]

    # Assume any recorded value for these observations indicates an abnormal result
    observations['IS_ABNORMAL'] = observations['DESCRIPTION'].isin(abnormal_observations_list).astype(int)
    abnormal_observations_count = observations.groupby('PATIENT')['IS_ABNORMAL'].sum().reset_index()
    abnormal_observations_count.rename(columns={'IS_ABNORMAL': 'Abnormal_Observations_Count'}, inplace=True)

    # -----------------------------------------
    # 2.5 Merge Counts into patient_data
    # -----------------------------------------
    # Ensure that the 'Id' column in patient_data matches 'PATIENT' in counts
    # So, we'll merge on 'Id' and 'PATIENT' appropriately

    # First, set the index of patient_data to 'Id' for efficient merging
    patient_data.set_index('Id', inplace=True)

    # Merge Chronic Conditions Count
    patient_data = patient_data.merge(chronic_conditions_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Merge Hospitalizations Count
    patient_data = patient_data.merge(hospitalizations_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Merge Medications Count
    patient_data = patient_data.merge(medications_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Merge Abnormal Observations Count
    patient_data = patient_data.merge(abnormal_observations_count.set_index('PATIENT'), left_index=True, right_index=True, how='left')

    # Reset index to have 'Id' as a column again
    patient_data.reset_index(inplace=True)

    # -----------------------------------------
    # 2.6 Fill NaN values with zeros
    # -----------------------------------------
    indicators = ['Chronic_Conditions_Count', 'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count']
    patient_data[indicators] = patient_data[indicators].fillna(0)

    return patient_data

# ------------------------------
# 3. Calculate Composite Health Index
# ------------------------------

def calculate_health_index(patient_data):
    """Calculate the composite health index."""
    # Define weights
    weights = {
        'AGE': 0.2,
        'Chronic_Conditions_Count': 0.3,
        'Hospitalizations_Count': 0.25,
        'Medications_Count': 0.15,
        'Abnormal_Observations_Count': 0.1
    }

    # Normalize indicators
    indicators = ['AGE', 'Chronic_Conditions_Count', 'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count']
    for col in indicators:
        if patient_data[col].max() != patient_data[col].min():
            # Min-Max scale with a small epsilon to avoid division by zero
            epsilon = 1e-8
            patient_data[col] = (patient_data[col] - patient_data[col].min()) / (patient_data[col].max() - patient_data[col].min() + epsilon)

            #patient_data[col] = (patient_data[col] - patient_data[col].min()) / (patient_data[col].max() - patient_data[col].min())
        else:
            patient_data[col] = 0  # If all values are the same, set normalized value to 0

    # Calculate health index
    patient_data['Health_Index'] = (
        patient_data['AGE'] * weights['AGE'] +
        patient_data['Chronic_Conditions_Count'] * weights['Chronic_Conditions_Count'] +
        patient_data['Hospitalizations_Count'] * weights['Hospitalizations_Count'] +
        patient_data['Medications_Count'] * weights['Medications_Count'] +
        patient_data['Abnormal_Observations_Count'] * weights['Abnormal_Observations_Count']
    )

    return patient_data

# ------------------------------
# 4. Save Health Index
# ------------------------------

def save_health_index(patient_data, output_dir):
    """Save the patient data with health index."""
    patient_data.to_pickle(os.path.join(output_dir, 'patient_data_with_health_index.pkl'))
    print("Health index calculated and saved.")

# ------------------------------
# Main Execution
# ------------------------------

def main():
    output_dir = 'Data'
    data_dir = r'E:\DataGen\synthea\output\csv' 

    # Load processed data
    patient_data, code_mappings = load_processed_data(output_dir)

    # Calculate health indicators
    patient_data = calculate_health_indicators(patient_data, data_dir)

    # Calculate health index
    patient_data = calculate_health_index(patient_data)

    # Save health index
    save_health_index(patient_data, output_dir)

if __name__ == '__main__':
    main()


Comments on improvements:
> Abnormal observations Calculations:
. For a more accurate count of abnormal observations, consider implementing logic to compare observation values against normal reference ranges.
. This would require defining normal ranges for each observation and checking if the patient's value falls outside that range.
. Implementing this would involve more detailed data analysis and possibly domain expertise.

> Medical Categorisation:
. You might consider categorising medications into classes (e.g., antihypertensives, antidiabetics) and counting medications in specific categories.
. This could provide more insight into the medication burden related to chronic conditions.

> Encounter Data:
. Besides hospitalisations, consider analyzing emergency visits or other encounter types that may indicate poor health status.
'''
# charlson_comorbidity.py
# Author: Your Name
# Date: DD/MM/YYYY
# Description:
# This script computes the Charlson Comorbidity Index (CCI) from SNOMED codes.
# It expects a CSV mapping file with columns:
# code, coding_system, description, entity, list_name, upload_date, medcode, snomedctdescriptionid, CharlsonCategory
# It also expects a conditions dataframe with SNOMED-CT codes.
#
# The output is a DataFrame with patient IDs and their computed CCI scores.

import pandas as pd
import os

def load_cci_mapping(data_dir):
    """
    Load the Charlson mapping file.
    Expects a CSV with columns: ['code', 'CharlsonCategory', ...].
    If no CSV or the file is missing, this function might raise FileNotFoundError
    or produce an empty DataFrame (depending on your needs).
    """
    cci_filepath = os.path.join(data_dir, 'res195-comorbidity-cci-snomed.csv')
    cci_df = pd.read_csv(cci_filepath)
    # Basic cleanup or renaming if needed
    # e.g. rename columns to something standard
    # cci_df.rename(columns={'code': 'SNOMED', 'CharlsonCategory': 'CharlsonCategory'}, inplace=True)
    return cci_df

def assign_cci_weights(CharlsonCategory):
    """
    Assign Charlson weights based on category.
    As per the original Charlson Comorbidity Index:
      - Myocardial infarction, Congestive heart failure, Peripheral vascular disease,
        Cerebrovascular disease, Dementia, Chronic pulmonary disease, Connective tissue disease,
        Ulcer disease, Mild liver disease, Diabetes without end-organ damage => weight 1
      - Hemiplegia, Moderate/severe kidney disease, Diabetes with end-organ damage,
        Any tumour (solid tumor), leukemia, lymphoma => weight 2
      - Moderate or severe liver disease => weight 3
      - Metastatic solid tumour, AIDS => weight 6
    """
    category_to_weight = {
        'Myocardial infarction': 1,
        'Congestive heart failure': 1,
        'Peripheral vascular disease': 1,
        'Cerebrovascular disease': 1,
        'Dementia': 1,
        'Chronic pulmonary disease': 1,
        'Connective tissue disease': 1,
        'Ulcer disease': 1,
        'Mild liver disease': 1,
        'Diabetes without end-organ damage': 1,
        'Hemiplegia': 2,
        'Moderate or severe kidney disease': 2,
        'Diabetes with end-organ damage': 2,
        'Any tumour, leukaemia, lymphoma': 2,
        'Moderate or severe liver disease': 3,
        'Metastatic solid tumour': 6,
        'AIDS/HIV': 6
    }
    return category_to_weight.get(CharlsonCategory, 0)

def compute_cci(conditions, cci_mapping):
    """
    Compute the Charlson Comorbidity Index for each patient from a DataFrame of SNOMED-CT codes.
    
    Args:
        conditions (pd.DataFrame): Must include ['PATIENT', 'CODE'] columns where
                                   CODE is a SNOMED code (int or str).
        cci_mapping (pd.DataFrame): A CSV-based lookup with at least:
                                   ['code', 'CharlsonCategory']
                                   (Loaded from load_cci_mapping).
                                   Some codes may be missing from the CSV.
    
    Returns:
        pd.DataFrame: A DataFrame with columns ['PATIENT', 'CharlsonIndex'].
                      If a patient has no mapped comorbidities, CharlsonIndex = 0.
    """

    # 1. Define a fallback dictionary for SNOMED -> CharlsonCategory
    SNOMED_TO_CHARLSON = {
        #
        # MYOCARDIAL INFARCTION (weight 1)
        #
        22298006: "Myocardial infarction",      # "Myocardial infarction (disorder)"
        401303003: "Myocardial infarction",     # "Acute ST segment elevation myocardial infarction"
        401314000: "Myocardial infarction",     # "Acute non-ST segment elevation myocardial infarction"
        129574000: "Myocardial infarction",     # "Postoperative myocardial infarction (disorder)"
        #
        # CONGESTIVE HEART FAILURE (weight 1)
        #
        88805009: "Congestive heart failure",   # "Chronic congestive heart failure (disorder)"
        84114007: "Congestive heart failure",   # "Heart failure (disorder)"
        #
        # PERIPHERAL VASCULAR DISEASE (weight 1)
        #
        # -- None in your list match typical “peripheral vascular disease” codes --
        #
        # CEREBROVASCULAR DISEASE (weight 1)
        #
        230690007: "Cerebrovascular disease",   # "Cerebrovascular accident (disorder)"
        #
        # DEMENTIA (weight 1)
        #
        26929004: "Dementia",                   # "Alzheimer's disease (disorder)"
        230265002: "Dementia",                  # "Familial Alzheimer's disease of early onset (disorder)"
        #
        # CHRONIC PULMONARY DISEASE (weight 1)
        #
        185086009: "Chronic pulmonary disease", # "Chronic obstructive bronchitis (disorder)"
        87433001:  "Chronic pulmonary disease", # "Pulmonary emphysema (disorder)"
        195967001: "Chronic pulmonary disease", # "Asthma (disorder)" – (Some include chronic asthma under COPD)
        233678006: "Chronic pulmonary disease", # "Childhood asthma (disorder)"
        #
        # CONNECTIVE TISSUE DISEASE (weight 1)
        #
        69896004: "Connective tissue disease",  # "Rheumatoid arthritis (disorder)"
        200936003: "Connective tissue disease", # "Lupus erythematosus (disorder)"
        #
        # ULCER DISEASE (weight 1)
        #
        # -- None in your list specifically match “peptic ulcer disease” --
        #
        # MILD LIVER DISEASE (weight 1)
        #
        128302006: "Mild liver disease",        # "Chronic hepatitis C (disorder)" 
        61977001:  "Mild liver disease",        # "Chronic type B viral hepatitis (disorder)"
        #
        # DIABETES WITHOUT END-ORGAN DAMAGE (weight 1)
        #
        44054006: "Diabetes without end-organ damage",  # "Diabetes mellitus type 2 (disorder)"
        #
        # DIABETES WITH END-ORGAN DAMAGE (weight 2)
        #
        368581000119106: "Diabetes with end-organ damage",  # "Neuropathy due to type 2 diabetes mellitus"
        422034002:        "Diabetes with end-organ damage",  # "Retinopathy due to type 2 diabetes mellitus"
        127013003:        "Diabetes with end-organ damage",  # "Disorder of kidney due to diabetes mellitus"
        90781000119102:   "Diabetes with end-organ damage",  # "Microalbuminuria due to type 2 diabetes mellitus"
        157141000119108:  "Diabetes with end-organ damage",  # "Proteinuria due to type 2 diabetes mellitus"
        60951000119105:   "Diabetes with end-organ damage",  # "Blindness due to type 2 diabetes mellitus"
        97331000119101:   "Diabetes with end-organ damage",  # "Macular edema & retinopathy due to T2DM"
        1501000119109:    "Diabetes with end-organ damage",  # "Proliferative retinopathy due to T2DM"
        1551000119108:    "Diabetes with end-organ damage",  # "Nonproliferative retinopathy due to T2DM"
        #
        # HEMIPLEGIA or PARAPLEGIA (weight 2)
        #
        # -- None in your list appear to indicate hemiplegia or paraplegia, 
        #    e.g. “cerebral palsy” is not typically counted as hemiplegia. 
        #
        # MODERATE OR SEVERE KIDNEY DISEASE (weight 2)
        #
        # Some references only count CKD stage 3 or worse. 
        # The user had stage 1 & 2 included, so we’ll keep that approach consistent:
        431855005: "Moderate or severe kidney disease",  # "CKD stage 1 (disorder)"
        431856006: "Moderate or severe kidney disease",  # "CKD stage 2 (disorder)"
        433144002: "Moderate or severe kidney disease",  # "CKD stage 3 (disorder)"
        431857002: "Moderate or severe kidney disease",  # "CKD stage 4 (disorder)"
        46177005:  "Moderate or severe kidney disease",  # "End-stage renal disease (disorder)"
        129721000119106: "Moderate or severe kidney disease",  # "Acute renal failure on dialysis (disorder)"
        #
        # ANY TUMOUR (solid tumor), LEUKEMIA, LYMPHOMA (weight 2)
        #
        254637007: "Any tumour, leukaemia, lymphoma",  # "Non-small cell lung cancer (disorder)"
        254632001: "Any tumour, leukaemia, lymphoma",  # "Small cell carcinoma of lung (disorder)"
        93761005:  "Any tumour, leukaemia, lymphoma",  # "Primary malignant neoplasm of colon"
        363406005: "Any tumour, leukaemia, lymphoma",  # "Malignant neoplasm of colon"
        109838007: "Any tumour, leukaemia, lymphoma",  # "Overlapping malignant neoplasm of colon"
        126906006: "Any tumour, leukaemia, lymphoma",  # "Neoplasm of prostate (disorder)"
        92691004:  "Any tumour, leukaemia, lymphoma",  # "Carcinoma in situ of prostate"
        254837009: "Any tumour, leukaemia, lymphoma",  # "Malignant neoplasm of breast"
        109989006: "Any tumour, leukaemia, lymphoma",  # "Multiple myeloma (disorder)"
        93143009:  "Any tumour, leukaemia, lymphoma",  # "Leukemia disease (disorder)"
        91861009:  "Any tumour, leukaemia, lymphoma",  # "Acute myeloid leukemia (disorder)"
        #
        # MODERATE OR SEVERE LIVER DISEASE (weight 3)
        #
        # -- None in your list mention cirrhosis or advanced hepatic failure 
        #    that we'd classify as 'moderate/severe liver disease'.
        #
        # METASTATIC SOLID TUMOUR (weight 6)
        #
        94503003: "Metastatic solid tumour",    # "Metastatic malignant neoplasm to prostate"
        94260004: "Metastatic solid tumour",    # "Metastatic malignant neoplasm to colon"
        #
        # AIDS/HIV (weight 6)
        #
        62479008: "AIDS/HIV",   # "Acquired immune deficiency syndrome (disorder)"
        86406008: "AIDS/HIV",   # "Human immunodeficiency virus infection (disorder)"
    }


    # 2. Merge conditions with cci_mapping on SNOMED code (left_on='CODE', right_on='code')
    #    This way if a code is present in the CSV, it overrides or supplies CharlsonCategory
    merged = conditions.merge(
        cci_mapping[['code', 'CharlsonCategory']],
        how='left',
        left_on='CODE',
        right_on='code'
    )

    # 3. Fallback: For rows where the CSV didn't provide a CharlsonCategory, try the SNOMED_TO_CHARLSON dict
    #    If neither the CSV nor the dict have it, it remains None/NaN
    def fallback_category(row):
        if pd.notna(row['CharlsonCategory']):
            return row['CharlsonCategory']
        else:
            # Attempt dictionary lookup
            return SNOMED_TO_CHARLSON.get(row['CODE'], None)

    merged['CharlsonCategory'] = merged.apply(fallback_category, axis=1)

    # 4. Compute the Charlson weight for each row
    merged['CCI_Weight'] = merged['CharlsonCategory'].apply(assign_cci_weights)

    # 5. For each patient, sum the unique categories.
    #    i.e. if a patient has multiple codes in the same category, only count once.
    #    We do this by grouping on (PATIENT, CharlsonCategory) and taking the max weight
    #    Then summing across categories for each patient
    patient_cci = (
        merged
        .groupby(['PATIENT', 'CharlsonCategory'])['CCI_Weight']
        .max()
        .reset_index()
    )

    patient_cci_sum = (
        patient_cci
        .groupby('PATIENT')['CCI_Weight']
        .sum()
        .reset_index()
    )

    # Rename column to match the expected return type
    patient_cci_sum.rename(columns={'CCI_Weight': 'CharlsonIndex'}, inplace=True)

    return patient_cci_sum




'''
import pandas as pd
import os

def load_cci_mapping(data_dir):
    """
    Load the Charlson mapping file.
    """
    cci_df = pd.read_csv(os.path.join(data_dir, 'res195-comorbidity-cci-snomed.csv'))
    # Clean and prepare mapping if needed
    return cci_df

def assign_cci_weights(CharlsonCategory):
    """
    Assign Charlson weights based on category.
    As per the original Charlson Comorbidity Index:
    - Myocardial infarction, Congestive heart failure, Peripheral vascular disease, Cerebrovascular disease, Dementia,
      Chronic pulmonary disease, Connective tissue disease, Peptic ulcer disease, Mild liver disease, Diabetes (without end-organ damage)
      all have weight 1.
    - Hemiplegia, Moderate or severe kidney disease, Diabetes with end-organ damage, Any tumor (solid tumor), Leukemia,
      Lymphoma have weight 2.
    - Moderate or severe liver disease has weight 3.
    - Metastatic solid tumor, AIDS have weight 6.
    
    """
    category_to_weight = {
        'Myocardial infarction': 1,
        'Congestive heart failure': 1,
        'Peripheral vascular disease': 1,
        'Cerebrovascular disease': 1,
        'Dementia': 1,
        'Chronic pulmonary disease': 1,
        'Connective tissue disease': 1,
        'Ulcer disease': 1,
        'Mild liver disease': 1,
        'Diabetes without end-organ damage': 1,
        'Hemiplegia': 2,
        'Moderate or severe kidney disease': 2,
        'Diabetes with end-organ damage': 2,
        'Any tumour, leukaemia, lymphoma': 2,
        'Moderate or severe liver disease': 3,
        'Metastatic solid tumour': 6,
        'AIDS/HIV': 6
    }
    return category_to_weight.get(CharlsonCategory, 0)

def compute_cci(conditions, cci_mapping):
    """
    Compute the Charlson Comorbidity Index for each patient.
    
    Args:
        conditions (pd.DataFrame): Conditions data with SNOMED codes.
        cci_mapping (pd.DataFrame): Mapping of SNOMED codes to Charlson categories.

    Returns:
        pd.DataFrame: A DataFrame with ['PATIENT', 'CharlsonIndex'].
    """
    # Merge conditions with cci_mapping on SNOMED code
    merged = conditions.merge(cci_mapping, left_on='CODE', right_on='code', how='left')
    merged['CCI_Weight'] = merged['CharlsonCategory'].apply(assign_cci_weights)

    # For each patient, sum the unique categories (Charlson CCI accounts for comorbidity presence, not frequency)
    # One patient could have multiple codes in the same category, but the category is only counted once.
    # So, we group by patient and category, take max weight for that category, and sum.
    patient_cci = merged.groupby(['PATIENT', 'CharlsonCategory'])['CCI_Weight'].max().reset_index()
    patient_cci_sum = patient_cci.groupby('PATIENT')['CCI_Weight'].sum().reset_index()
    patient_cci_sum.rename(columns={'CCI_Weight':'CharlsonIndex'}, inplace=True)
    return patient_cci_sum
'''
# vae_model.py
# Author: Imran Feisal
# Date: 31/10/2024
# Description:
# Updated script to address variable duplication warnings in TensorFlow.
# This script trains a VAE model, now accepts an input_file and output_prefix parameters
# so as to avoid overwriting model artifacts. 
# 
# UPDATe 19/01/2025 this script now saves a JSON file with final
# training and validation losses, named <output_prefix>_vae_metrics.json.

import numpy as np
import pandas as pd
import joblib
import os
import json
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import logging
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_data(output_dir, input_file):
    patient_data = pd.read_pickle(os.path.join(output_dir, input_file))
    return patient_data

def prepare_data(patient_data):
    features = patient_data[[
        'AGE', 'GENDER', 'RACE', 'ETHNICITY', 'MARITAL',
        'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME',
        'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count'
    ]].copy()

    patient_ids = patient_data['Id'].values
    categorical_features = ['GENDER', 'RACE', 'ETHNICITY', 'MARITAL']
    continuous_features = [col for col in features.columns if col not in categorical_features]

    embedding_info = {}
    input_data = {}

    for col in categorical_features:
        le = LabelEncoder()
        features[col] = le.fit_transform(features[col].astype(str))
        joblib.dump(le, f'label_encoder_{col}.joblib')
        vocab_size = features[col].nunique()
        embedding_dim = min(50, (vocab_size + 1)//2)
        embedding_info[col] = {'vocab_size': vocab_size, 'embedding_dim': embedding_dim}
        input_data[col] = features[col].values

    scaler = StandardScaler()
    scaled_continuous = scaler.fit_transform(features[continuous_features])
    joblib.dump(scaler, 'scaler_vae.joblib')
    input_data['continuous'] = scaled_continuous

    logger.info("Data prepared for VAE.")
    return input_data, embedding_info, patient_ids, continuous_features, categorical_features

def build_vae(input_dim, embedding_info, continuous_dim, latent_dim=20):
    inputs = {}
    encoded_features = []

    # Embeddings for categorical
    for col, info in embedding_info.items():
        input_cat = keras.Input(shape=(1,), name=f'input_{col}')
        embedding_layer = layers.Embedding(
            input_dim=info['vocab_size'], 
            output_dim=info['embedding_dim'], 
            name=f'embedding_{col}'
        )(input_cat)
        flat_embedding = layers.Flatten()(embedding_layer)
        inputs[f'input_{col}'] = input_cat
        encoded_features.append(flat_embedding)

    # Continuous input
    input_cont = keras.Input(shape=(continuous_dim,), name='input_continuous')
    inputs['input_continuous'] = input_cont
    encoded_features.append(input_cont)

    concatenated_features = layers.concatenate(encoded_features)
    h = layers.Dense(256, activation='relu')(concatenated_features)
    h = layers.Dense(128, activation='relu')(h)
    z_mean = layers.Dense(latent_dim, name='z_mean')(h)
    z_log_var = layers.Dense(latent_dim, name='z_log_var')(h)

    def sampling(args):
        z_mean, z_log_var = args
        epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], latent_dim))
        return z_mean + tf.exp(0.5*z_log_var)*epsilon

    z = layers.Lambda(sampling, name='z')([z_mean, z_log_var])

    # Decoder
    decoder_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')
    h_decoder = layers.Dense(128, activation='relu')(decoder_inputs)
    h_decoder = layers.Dense(256, activation='relu')(h_decoder)
    reconstructed = layers.Dense(input_dim, activation='linear')(h_decoder)

    encoder = keras.Model(inputs=inputs, outputs=[z_mean, z_log_var, z], name='encoder')
    decoder = keras.Model(inputs=decoder_inputs, outputs=reconstructed, name='decoder')

    outputs = decoder(encoder(inputs)[2])
    vae = keras.Model(inputs=inputs, outputs=outputs, name='vae')

    reconstruction_loss = tf.reduce_mean(tf.square(concatenated_features - outputs))
    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))
    vae_loss = reconstruction_loss + kl_loss
    vae.add_loss(vae_loss)
    vae.compile(optimizer='adam')

    logger.info("VAE model built.")
    return vae, encoder, decoder

def train_vae(vae, input_data, output_prefix='vae'):
    x_train = {
        f'input_{key}': value for key, value in input_data.items() 
        if key != 'continuous'
    }
    x_train['input_continuous'] = input_data['continuous']

    early_stopping = keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=10, restore_best_weights=True
    )
    checkpoint = keras.callbacks.ModelCheckpoint(
        f'{output_prefix}_best_model.h5', 
        monitor='val_loss', 
        save_best_only=True
    )

    # Fit returns a History object with training & validation losses
    history = vae.fit(
        x_train, 
        epochs=100, 
        batch_size=512, 
        validation_split=0.2, 
        callbacks=[early_stopping, checkpoint],
        verbose=1
    )
    vae.save(f'{output_prefix}_model', save_format='tf')
    logger.info(f"VAE trained and saved with prefix={output_prefix}.")

    # Extract final losses from history
    # Because of early stopping, 'val_loss' might not correspond to the final epoch
    # We take the minimal val_loss across epochs as a reference
    final_train_loss = float(history.history['loss'][-1])  # last epoch's training loss
    final_val_loss = float(min(history.history['val_loss']))  # best validation loss

    # Save them to a JSON for easier retrieval
    metrics_json = {
        "final_train_loss": final_train_loss,
        "best_val_loss": final_val_loss
    }
    with open(f"{output_prefix}_vae_metrics.json", "w") as f:
        json.dump(metrics_json, f, indent=2)
    logger.info(f"[METRICS] VAE training/validation losses saved to {output_prefix}_vae_metrics.json")

def save_latent_features(encoder, input_data, patient_ids, output_prefix='vae'):
    x_pred = {
        f'input_{key}': value for key, value in input_data.items() 
        if key != 'continuous'
    }
    x_pred['input_continuous'] = input_data['continuous']
    z_mean, _, _ = encoder.predict(x_pred)

    df = pd.DataFrame(z_mean)
    df['Id'] = patient_ids
    csv_name = f'{output_prefix}_latent_features.csv'
    df.to_csv(csv_name, index=False)
    logger.info(f"Latent features saved to {csv_name}.")

def main(input_file='patient_data_with_health_index.pkl', output_prefix='vae'):
    """
    Args:
        input_file (str): Name of the input pickle file containing patient data.
        output_prefix (str): A unique prefix for saving model artifacts 
                             (latent CSV, model files, etc.).
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, 'Data')
    patient_data = load_data(output_dir, input_file)
    input_data, embedding_info, patient_ids, continuous_features, categorical_features = prepare_data(patient_data)

    input_dim = sum(info['embedding_dim'] for info in embedding_info.values()) + len(continuous_features)
    continuous_dim = len(continuous_features)
    vae, encoder, decoder = build_vae(input_dim, embedding_info, continuous_dim)

    train_vae(vae, input_data, output_prefix=output_prefix)
    encoder.save(f'{output_prefix}_encoder', save_format='tf')
    decoder.save(f'{output_prefix}_decoder', save_format='tf')

    save_latent_features(encoder, input_data, patient_ids, output_prefix=output_prefix)

if __name__ == '__main__':
    main()


'''
import numpy as np
import pandas as pd
import joblib
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import logging
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------
# 1. Load Data
# ------------------------------

def load_data(output_dir):
    """
    Load patient data with health index.
    """
    patient_data = pd.read_pickle(os.path.join(output_dir, 'patient_data_with_health_index.pkl'))
    return patient_data

# ------------------------------
# 2. Prepare Data for VAE
# ------------------------------

def prepare_data(patient_data):
    """
    Prepare data for training the VAE, using embedding layers for categorical variables.
    """
    # Extract features without the Health Index
    features = patient_data[['AGE', 'GENDER', 'RACE', 'ETHNICITY', 'MARITAL',
                             'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME',
                             'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count']].copy()

    patient_ids = patient_data['Id'].values

    # Identify categorical variables
    categorical_features = ['GENDER', 'RACE', 'ETHNICITY', 'MARITAL']
    continuous_features = [col for col in features.columns if col not in categorical_features]

    # Initialize embedding info
    embedding_info = {}
    input_data = {}

    # Process categorical features
    for col in categorical_features:
        le = LabelEncoder()
        features[col] = le.fit_transform(features[col].astype(str))
        # Save the LabelEncoder
        joblib.dump(le, f'label_encoder_{col}.joblib')

        vocab_size = features[col].nunique()
        embedding_dim = min(50, (vocab_size + 1) // 2)

        embedding_info[col] = {'vocab_size': vocab_size, 'embedding_dim': embedding_dim}
        input_data[col] = features[col].values

    # Process continuous features
    scaler = StandardScaler()
    scaled_continuous = scaler.fit_transform(features[continuous_features])
    joblib.dump(scaler, 'scaler_vae.joblib')
    input_data['continuous'] = scaled_continuous

    logger.info("Data prepared for VAE model.")

    return input_data, embedding_info, patient_ids, continuous_features, categorical_features

# ------------------------------
# 3. Build VAE Model
# ------------------------------

def build_vae(input_dim, embedding_info, continuous_dim, latent_dim=20):
    """
    Build the VAE model without variable duplication.
    """
    # Inputs
    inputs = {}
    encoded_features = []

    # Categorical inputs and embeddings
    for col, info in embedding_info.items():
        input_cat = keras.Input(shape=(1,), name=f'input_{col}')
        embedding_layer = layers.Embedding(
            input_dim=info['vocab_size'],
            output_dim=info['embedding_dim'],
            name=f'embedding_{col}'
        )(input_cat)
        flat_embedding = layers.Flatten()(embedding_layer)
        inputs[f'input_{col}'] = input_cat
        encoded_features.append(flat_embedding)

    # Continuous inputs
    input_cont = keras.Input(shape=(continuous_dim,), name='input_continuous')
    inputs['input_continuous'] = input_cont
    encoded_features.append(input_cont)

    # Concatenate features
    concatenated_features = layers.concatenate(encoded_features)

    # Encoder
    h = layers.Dense(256, activation='relu')(concatenated_features)
    h = layers.Dense(128, activation='relu')(h)
    z_mean = layers.Dense(latent_dim, name='z_mean')(h)
    z_log_var = layers.Dense(latent_dim, name='z_log_var')(h)

    # Sampling Layer
    def sampling(args):
        z_mean, z_log_var = args
        epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], latent_dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

    z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])

    # Decoder
    decoder_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')
    h_decoder = layers.Dense(128, activation='relu')(decoder_inputs)
    h_decoder = layers.Dense(256, activation='relu')(h_decoder)
    reconstructed = layers.Dense(input_dim, activation='linear')(h_decoder)

    # Define encoder and decoder models
    encoder = keras.Model(inputs=inputs, outputs=[z_mean, z_log_var, z], name='encoder')
    decoder = keras.Model(inputs=decoder_inputs, outputs=reconstructed, name='decoder')

    # VAE Model
    outputs = decoder(encoder(inputs)[2])
    vae = keras.Model(inputs=inputs, outputs=outputs, name='vae')

    # Loss
    reconstruction_loss = tf.reduce_mean(tf.square(concatenated_features - outputs))
    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))
    vae_loss = reconstruction_loss + kl_loss
    vae.add_loss(vae_loss)
    vae.compile(optimizer='adam')

    logger.info("VAE model built successfully.")

    return vae, encoder, decoder

# ------------------------------
# 4. Train VAE Model
# ------------------------------

def train_vae(vae, input_data):
    """
    Train the VAE model.
    """
    # Prepare input data for training
    x_train = {f'input_{key}': value for key, value in input_data.items() if key != 'continuous'}
    x_train['input_continuous'] = input_data['continuous']

    # Early stopping and model checkpoint
    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    checkpoint = keras.callbacks.ModelCheckpoint('vae_best_model.h5', monitor='val_loss', save_best_only=True)

    # Since the VAE reconstructs the inputs, we don't need to provide targets
    vae.fit(
        x=x_train,
        epochs=100,
        batch_size=256,
        validation_split=0.2,
        callbacks=[early_stopping, checkpoint]
    )
    vae.save('vae_model', save_format='tf')

    logger.info("VAE model trained and saved successfully.")

# ------------------------------
# 5. Save Latent Features
# ------------------------------

def save_latent_features(encoder, input_data, patient_ids):
    """
    Save the latent features from the encoder.
    """
    # Prepare input data for prediction
    x_pred = {f'input_{key}': value for key, value in input_data.items() if key != 'continuous'}
    x_pred['input_continuous'] = input_data['continuous']

    z_mean, _, _ = encoder.predict(x_pred)
    latent_features_df = pd.DataFrame(z_mean)
    latent_features_df['Id'] = patient_ids
    latent_features_df.to_csv('latent_features_vae.csv', index=False)
    logger.info("Latent features saved.")

# ------------------------------
# Main Execution
# ------------------------------

def main():
    output_dir = 'Data'

    # Load data
    patient_data = load_data(output_dir)

    # Prepare data
    input_data, embedding_info, patient_ids, continuous_features, categorical_features = prepare_data(patient_data)

    # Build VAE model
    input_dim = sum(info['embedding_dim'] for info in embedding_info.values()) + len(continuous_features)
    continuous_dim = len(continuous_features)
    vae, encoder, decoder = build_vae(input_dim, embedding_info, continuous_dim)

    # Train VAE model
    train_vae(vae, input_data)

    # Save encoder and decoder separately
    encoder.save('vae_encoder', save_format='tf')
    decoder.save('vae_decoder', save_format='tf')

    # Save latent features
    save_latent_features(encoder, input_data, patient_ids)

if __name__ == '__main__':
    main()
'''
'''
# vae_model.py
# Author: Imran Feisal
# Date: 31/10/2024
# Description:
# This script loads patient data with health index,
# enhances data preparation by including more features and embedding sequences,
# builds a more complex VAE model with hyperparameter tuning and early stopping,
# trains the model, and saves the entire model for future use.

# Inspiration:
# - Kingma, D. P., & Welling, M. (2014). Auto-Encoding Variational Bayes. International Conference on Learning Representations.
# - Miotto, R., et al. (2016). Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records. Scientific Reports, 6, 26094.

import numpy as np
import pandas as pd
import joblib
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import logging

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------
# 1. Load Data
# ------------------------------

def load_data(output_dir):
    """Load patient data with health index."""
    patient_data = pd.read_pickle(os.path.join(output_dir, 'patient_data_with_health_index.pkl'))
    return patient_data

# ------------------------------
# 2. Prepare Data for VAE
# ------------------------------

def prepare_data(patient_data):
    """Prepare data for training the VAE."""
    # Extract features without the Charlson Comorbidity Score
    features = patient_data[['AGE', 'GENDER', 'RACE', 'ETHNICITY', 'MARITAL', 
                             'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME',
                             'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count']]

    # One-hot encode categorical variables
    categorical_features = ['GENDER', 'RACE', 'ETHNICITY', 'MARITAL']
    features = pd.get_dummies(features, columns=categorical_features)

    # Fill missing values
    features.fillna(0, inplace=True)

    # Standardize features
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X = scaler.fit_transform(features)

    # Save the scaler
    joblib.dump(scaler, 'scaler_vae.joblib')

    logger.info("Data prepared for VAE model.")

    return X

# ------------------------------
# 3. Build VAE Model
# ------------------------------

def build_vae(input_dim, latent_dim=20):
    """Build the VAE model."""
    # Encoder
    inputs = keras.Input(shape=(input_dim,))
    h = layers.Dense(256, activation='relu')(inputs)
    h = layers.Dense(128, activation='relu')(h)
    z_mean = layers.Dense(latent_dim, name='z_mean')(h)
    z_log_var = layers.Dense(latent_dim, name='z_log_var')(h)

    # Sampling Layer
    def sampling(args):
        z_mean, z_log_var = args
        epsilon = tf.keras.backend.random_normal(shape=(tf.keras.backend.shape(z_mean)[0], latent_dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

    z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])

    # Decoder
    latent_inputs = keras.Input(shape=(latent_dim,))
    h_decoder = layers.Dense(128, activation='relu')(latent_inputs)
    h_decoder = layers.Dense(256, activation='relu')(h_decoder)
    outputs = layers.Dense(input_dim, activation='linear')(h_decoder)

    # Define models
    encoder = keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')
    decoder = keras.Model(latent_inputs, outputs, name='decoder')
    outputs = decoder(encoder(inputs)[2])

    # VAE Model
    vae = keras.Model(inputs, outputs, name='vae')

    # Loss
    reconstruction_loss = tf.reduce_mean(tf.square(inputs - outputs))
    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))
    vae_loss = reconstruction_loss + kl_loss
    vae.add_loss(vae_loss)
    vae.compile(optimizer='adam')

    logger.info("VAE model built successfully.")

    return vae, encoder, decoder

# ------------------------------
# 4. Train VAE Model
# ------------------------------

def train_vae(vae, X):
    """Train the VAE model."""
    # Early stopping
    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    vae.fit(X, epochs=100, batch_size=256, validation_split=0.2, callbacks=[early_stopping])
    vae.save('vae_model.h5')

    logger.info("VAE model trained and saved successfully.")

# ------------------------------
# 5. Save Latent Features
# ------------------------------

def save_latent_features(encoder, X, patient_ids):
    """Save the latent features from the encoder."""
    z_mean, _, _ = encoder.predict(X)
    latent_features_df = pd.DataFrame(z_mean)
    latent_features_df['Id'] = patient_ids
    latent_features_df.to_csv('latent_features_vae.csv', index=False)
    logger.info("Latent features saved.")

# ------------------------------
# Main Execution
# ------------------------------

def main():
    output_dir = 'Data'

    # Load data
    patient_data = load_data(output_dir)
    patient_ids = patient_data['Id'].values

    # Prepare data
    X = prepare_data(patient_data)

    # Build VAE model
    input_dim = X.shape[1]
    vae, encoder, decoder = build_vae(input_dim)

    # Train VAE model
    train_vae(vae, X)

    # Save latent features
    save_latent_features(encoder, X, patient_ids)

if __name__ == '__main__':
    main()
'''
# tabnet_model.py
# Author: Imran Feisal
# Date: 31/10/2024
# Description:
# This script builds and trains a TabNet model using hyperparameter tuning,
# includes cross-validation, extracts feature importances, and saves the
# trained model and results. 
# Now accepts an output_prefix param to avoid overwriting artifacts,
# and target_col param to decide which column to predict (Health_Index or CharlsonIndex).

import numpy as np
import pandas as pd
import joblib
import os
import torch
from pytorch_tabnet.tab_model import TabNetRegressor
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, r2_score
import logging
import json
import optuna
from sklearn.preprocessing import LabelEncoder, StandardScaler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_data(output_dir, input_file):
    data_path = os.path.join(output_dir, input_file)
    if not os.path.exists(data_path):
        logger.error(f"Data file not found at {data_path}")
        raise FileNotFoundError(f"Data file not found at {data_path}")
    patient_data = pd.read_pickle(data_path)
    logger.info("Patient data loaded.")
    return patient_data

def prepare_data(patient_data, target_col='Health_Index'):
    """
    Prepare the dataset for TabNet:
      - features: columns that define the model inputs
      - target: the column we want to predict (Health_Index or CharlsonIndex)
    """
    # Feature columns
    features = patient_data[[
        'AGE','DECEASED','GENDER','RACE','ETHNICITY','MARITAL',
        'HEALTHCARE_EXPENSES','HEALTHCARE_COVERAGE','INCOME',
        'Hospitalizations_Count','Medications_Count','Abnormal_Observations_Count'
    ]].copy()

    # Target column is chosen based on `target_col`
    if target_col not in patient_data.columns:
        raise KeyError(f"Column '{target_col}' not found in patient_data!")
    target = patient_data[target_col]

    # Setup categorical columns
    categorical_columns = ['DECEASED','GENDER','RACE','ETHNICITY','MARITAL']
    cat_idxs = [i for i,col in enumerate(features.columns) if col in categorical_columns]
    cat_dims = []

    for col in categorical_columns:
        le = LabelEncoder()
        features[col] = le.fit_transform(features[col].astype(str))
        cat_dims.append(features[col].nunique())

    # Scale continuous columns
    continuous_columns = [col for col in features.columns if col not in categorical_columns]
    scaler = StandardScaler()
    features[continuous_columns] = scaler.fit_transform(features[continuous_columns])
    joblib.dump(scaler, 'tabnet_scaler.joblib')

    # Handle missing
    features.fillna(0, inplace=True)

    X = features.values
    y = target.values.reshape(-1, 1)
    logger.info(f"Data prepared for TabNet (target_col='{target_col}').")

    return X, y, cat_idxs, cat_dims, features.columns.tolist()

def objective(trial, X, y, cat_idxs, cat_dims):
    params = {
        'n_d': trial.suggest_int('n_d', 8, 64),
        'n_a': trial.suggest_int('n_a', 8, 64),
        'n_steps': trial.suggest_int('n_steps', 3, 10),
        'gamma': trial.suggest_float('gamma', 1.0, 2.0),
        'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-6, 1e-3, log=True),
        'optimizer_fn': torch.optim.Adam,
        'optimizer_params': dict(lr=trial.suggest_float('lr',1e-4,1e-2,log=True)),
        'cat_emb_dim': trial.suggest_int('cat_emb_dim',1,5),
        'n_shared': trial.suggest_int('n_shared',1,5),
        'n_independent': trial.suggest_int('n_independent',1,5),
        'device_name': 'cuda',
        'verbose': 0,
    }
    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    mse_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train_fold, X_valid_fold = X[train_idx], X[valid_idx]
        y_train_fold, y_valid_fold = y[train_idx], y[valid_idx]

        model = TabNetRegressor(cat_idxs=cat_idxs, cat_dims=cat_dims, **params)
        model.fit(
            X_train=X_train_fold, y_train=y_train_fold,
            eval_set=[(X_valid_fold, y_valid_fold)],
            eval_metric=['rmse'],
            max_epochs=50,
            patience=10,
            batch_size=4096,
            virtual_batch_size=512
        )
        preds = model.predict(X_valid_fold)
        mse = mean_squared_error(y_valid_fold, preds)
        mse_list.append(mse)
    return np.mean(mse_list)

def hyperparameter_tuning(X, y, cat_idxs, cat_dims):
    study = optuna.create_study(direction='minimize')
    study.optimize(lambda trial: objective(trial, X, y, cat_idxs, cat_dims), n_trials=8)
    logger.info(f"Best trial: {study.best_trial.params}")
    return study.best_trial.params

def train_tabnet(X_train, y_train, X_valid, y_valid, cat_idxs, cat_dims, best_params, output_prefix='tabnet'):
    optimizer_fn = torch.optim.Adam
    optimizer_params = {'lr': best_params.pop('lr')}
    best_params.update({
        'optimizer_fn': optimizer_fn,
        'optimizer_params': optimizer_params,
        'device_name': 'cuda',
        'verbose': 1
    })
    regressor = TabNetRegressor(cat_idxs=cat_idxs, cat_dims=cat_dims, **best_params)
    regressor.fit(
        X_train=X_train,
        y_train=y_train,
        eval_set=[(X_valid, y_valid)],
        eval_metric=['rmse'],
        max_epochs=200,
        patience=20,
        batch_size=8192,
        virtual_batch_size=1024
    )
    regressor.save_model(f'{output_prefix}_model')
    logger.info(f"TabNet model trained and saved -> {output_prefix}_model.zip (among others).")
    return regressor

def main(input_file='patient_data_with_health_index.pkl',
         output_prefix='tabnet',
         target_col='Health_Index'):
    """
    Args:
        input_file (str): Pickle file containing patient data
        output_prefix (str): Unique prefix to avoid overwriting model artifacts
        target_col (str): Which column to predict ('Health_Index' or 'CharlsonIndex')
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, 'Data')
    patient_data = load_data(output_dir, input_file)

    # Prepare data for the specified target column
    X, y, cat_idxs, cat_dims, feature_columns = prepare_data(patient_data, target_col=target_col)

    # Train/valid/test splits
    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)

    # Hyperparameter tuning
    best_params = hyperparameter_tuning(X_train, y_train, cat_idxs, cat_dims)
    regressor = train_tabnet(X_train, y_train, X_valid, y_valid, cat_idxs, cat_dims, best_params, output_prefix=output_prefix)

    # Evaluate on test set
    test_preds = regressor.predict(X_test)
    test_mse = mean_squared_error(y_test, test_preds)
    test_r2 = r2_score(y_test, test_preds)
    logger.info(f"Test MSE: {test_mse:.4f}")
    logger.info(f"Test R2: {test_r2:.4f}")

    # Save predictions
    # If 'Id' is present in the DF, we can map it; otherwise we do a simple index-based approach
    num_test = len(X_test)
    pred_col_name = ("Predicted_Health_Index" if target_col == "Health_Index" 
                     else "Predicted_CharlsonIndex")

    if 'Id' in patient_data.columns:
        # Take the last 'num_test' rows as test IDs
        test_ids = patient_data.iloc[-num_test:]['Id'].values
    else:
        # Fallback if not present
        test_ids = np.arange(num_test)

    predictions_df = pd.DataFrame({
        'Id': test_ids,
        pred_col_name: test_preds.flatten()
    })
    pred_csv = f'{output_prefix}_predictions.csv'
    predictions_df.to_csv(pred_csv, index=False)
    logger.info(f"TabNet predictions saved -> {pred_csv}")

    # Save metrics
    metrics = {
        "test_mse": test_mse,
        "test_r2": test_r2
    }
    metrics_file = f"{output_prefix}_metrics.json"
    with open(metrics_file, "w") as f:
        json.dump(metrics, f)
    logger.info(f"TabNet metrics saved -> {metrics_file}")

if __name__ == '__main__':
    main()

'''
import numpy as np
import pandas as pd
import joblib
import os
import torch
from pytorch_tabnet.tab_model import TabNetRegressor
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import logging
import optuna
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------
# 1. Load Data
# ------------------------------

def load_data(output_dir):
    data_path = os.path.join(output_dir, 'patient_data_with_health_index.pkl')
    if not os.path.exists(data_path):
        logger.error(f"Data file not found at {data_path}")
        raise FileNotFoundError(f"Data file not found at {data_path}")
    patient_data = pd.read_pickle(data_path)
    logger.info("Patient data loaded successfully.")
    return patient_data

# ------------------------------
# 2. Prepare Data for TabNet
# ------------------------------

def prepare_data(patient_data):
    features = patient_data[['AGE', 'DECEASED', 'GENDER', 'RACE', 'ETHNICITY', 'MARITAL',
                             'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME',
                             'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count']].copy()
    target = patient_data['Health_Index']

    categorical_columns = ['DECEASED', 'GENDER', 'RACE', 'ETHNICITY', 'MARITAL']
    cat_idxs = [i for i, col in enumerate(features.columns) if col in categorical_columns]
    cat_dims = []

    for col in categorical_columns:
        le = LabelEncoder()
        features[col] = le.fit_transform(features[col].astype(str))
        cat_dims.append(features[col].nunique())

    continuous_columns = [col for col in features.columns if col not in categorical_columns]
    scaler = StandardScaler()
    features[continuous_columns] = scaler.fit_transform(features[continuous_columns])
    joblib.dump(scaler, 'tabnet_scaler.joblib')

    features.fillna(0, inplace=True)
    X = features.values
    y = target.values.reshape(-1, 1)

    logger.info("Data prepared for TabNet model.")
    return X, y, cat_idxs, cat_dims, features.columns.tolist()

# ------------------------------
# 3. Hyperparameter Tuning with Optuna
# ------------------------------

def objective(trial, X, y, cat_idxs, cat_dims):
    params = {
        'n_d': trial.suggest_int('n_d', 8, 64),
        'n_a': trial.suggest_int('n_a', 8, 64),
        'n_steps': trial.suggest_int('n_steps', 3, 10),
        'gamma': trial.suggest_float('gamma', 1.0, 2.0),
        'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-6, 1e-3, log=True),
        'optimizer_fn': torch.optim.Adam,
        'optimizer_params': dict(lr=trial.suggest_float('lr', 1e-4, 1e-2, log=True)),
        'cat_emb_dim': trial.suggest_int('cat_emb_dim', 1, 5),
        'n_shared': trial.suggest_int('n_shared', 1, 5),
        'n_independent': trial.suggest_int('n_independent', 1, 5),
        'device_name': 'cuda',
        'verbose': 0,
    }

    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    mse_list = []

    for train_index, valid_index in kf.split(X):
        X_train_fold, X_valid_fold = X[train_index], X[valid_index]
        y_train_fold, y_valid_fold = y[train_index], y[valid_index]

        model = TabNetRegressor(
            cat_idxs=cat_idxs,
            cat_dims=cat_dims,
            **params
        )

        model.fit(
            X_train=X_train_fold, y_train=y_train_fold,
            eval_set=[(X_valid_fold, y_valid_fold)],
            eval_metric=['rmse'],
            max_epochs=50,  # Reduce epochs during tuning for faster results
            patience=10,
            batch_size=4096,
            virtual_batch_size=512
        )

        preds = model.predict(X_valid_fold)
        mse = mean_squared_error(y_valid_fold, preds)
        mse_list.append(mse)

    return np.mean(mse_list)

def hyperparameter_tuning(X, y, cat_idxs, cat_dims):
    study = optuna.create_study(direction='minimize')
    study.optimize(lambda trial: objective(trial, X, y, cat_idxs, cat_dims), n_trials=20)  # Reduce trials for faster results
    logger.info(f"Best trial: {study.best_trial.params}")
    return study.best_trial.params

# ------------------------------
# 4. Train TabNet Model
# ------------------------------

def train_tabnet(X_train, y_train, X_valid, y_valid, cat_idxs, cat_dims, best_params):
    optimizer_fn = torch.optim.Adam
    optimizer_params = {'lr': best_params.pop('lr')}
    best_params.update({
        'optimizer_fn': optimizer_fn,
        'optimizer_params': optimizer_params,
        'device_name': 'cuda',
        'verbose': 1,
    })

    regressor = TabNetRegressor(
        cat_idxs=cat_idxs,
        cat_dims=cat_dims,
        **best_params
    )

    regressor.fit(
        X_train=X_train,
        y_train=y_train,
        eval_set=[(X_valid, y_valid)],
        eval_metric=['rmse'],
        max_epochs=200,
        patience=20,
        batch_size=8192,  # Larger batch size for GPU
        virtual_batch_size=1024
    )

    regressor.save_model('tabnet_model')
    logger.info("TabNet model trained and saved successfully.")
    return regressor

# ------------------------------
# 5. Main Execution
# ------------------------------

def main():
    output_dir = 'Data'
    patient_data = load_data(output_dir)
    X, y, cat_idxs, cat_dims, feature_columns = prepare_data(patient_data)

    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)

    best_params = hyperparameter_tuning(X_train, y_train, cat_idxs, cat_dims)
    regressor = train_tabnet(X_train, y_train, X_valid, y_valid, cat_idxs, cat_dims, best_params)

    test_preds = regressor.predict(X_test)
    test_mse = mean_squared_error(y_test, test_preds)
    test_r2 = r2_score(y_test, test_preds)
    logger.info(f"Test MSE: {test_mse:.4f}")
    logger.info(f"Test R2 Score: {test_r2:.4f}")

if __name__ == '__main__':
    main()

'''
'''
# tabnet_model.py
# Author: Imran Feisal
# Date: 31/10/2024
# Description:
# This script builds and trains a TabNet model using hyperparameter tuning,
# includes cross-validation, extracts feature importances, and saves the trained model and results.

import numpy as np
import pandas as pd
import joblib
import os
import torch
from pytorch_tabnet.tab_model import TabNetRegressor
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import logging
import optuna
from sklearn.preprocessing import LabelEncoder

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------
# 1. Load Data
# ------------------------------

def load_data(output_dir):
    """Load patient data with health index."""
    data_path = os.path.join(output_dir, 'patient_data_with_health_index.pkl')
    if not os.path.exists(data_path):
        logger.error(f"Data file not found at {data_path}")
        raise FileNotFoundError(f"Data file not found at {data_path}")
    patient_data = pd.read_pickle(data_path)
    logger.info("Patient data loaded successfully.")
    return patient_data

# ------------------------------
# 2. Prepare Data for TabNet
# ------------------------------

def prepare_data(patient_data):
    """Prepare data for training the TabNet model."""
    # Extract features and target without the Charlson Comorbidity Score
    features = patient_data[['AGE', 'DECEASED', 'GENDER', 'RACE', 'ETHNICITY', 'MARITAL',
                             'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME',
                             'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count']]
    target = patient_data['Health_Index']

    # Identify categorical columns
    categorical_columns = ['DECEASED', 'GENDER', 'RACE', 'ETHNICITY', 'MARITAL']
    cat_idxs = [i for i, col in enumerate(features.columns) if col in categorical_columns]
    
    # Label encode categorical columns and update cat_dims
    cat_dims = []
    for col in categorical_columns:
        le = LabelEncoder()
        features[col] = le.fit_transform(features[col].astype(str))
        cat_dims.append(features[col].nunique())

    # Fill missing values
    features.fillna(0, inplace=True)
    
    # Convert DataFrame to numpy array
    X = features.values
    y = target.values.reshape(-1, 1)  # Reshape target to 2D array

    logger.info("Data prepared for TabNet model.")
    return X, y, cat_idxs, cat_dims

# ------------------------------
# 3. Hyperparameter Tuning with Optuna
# ------------------------------

def objective(trial, X, y, cat_idxs, cat_dims):
    """Objective function for hyperparameter tuning with Optuna."""
    params = {
        'n_d': trial.suggest_int('n_d', 8, 64),
        'n_a': trial.suggest_int('n_a', 8, 64),
        'n_steps': trial.suggest_int('n_steps', 3, 10),
        'gamma': trial.suggest_float('gamma', 1.0, 2.0),
        'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-6, 1e-3, log=True),
        'optimizer_params': dict(lr=trial.suggest_float('lr', 1e-4, 1e-2, log=True)),
        'cat_emb_dim': trial.suggest_int('cat_emb_dim', 1, 5),
        'n_shared': trial.suggest_int('n_shared', 1, 5),
        'n_independent': trial.suggest_int('n_independent', 1, 5),
    }

    # Cross-validation
    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    mse_list = []

    for train_index, valid_index in kf.split(X):
        X_train_fold, X_valid_fold = X[train_index], X[valid_index]
        y_train_fold, y_valid_fold = y[train_index], y[valid_index]

        model = TabNetRegressor(
            cat_idxs=cat_idxs,
            cat_dims=cat_dims,
            **params
        )

        model.fit(
            X_train=X_train_fold, y_train=y_train_fold,
            eval_set=[(X_valid_fold, y_valid_fold)],
            eval_metric=['rmse'],
            max_epochs=100,
            patience=20,
            batch_size=1024,
            virtual_batch_size=128,
            num_workers=0,
            drop_last=False
        )

        preds = model.predict(X_valid_fold)
        mse = mean_squared_error(y_valid_fold, preds)
        mse_list.append(mse)

    return np.mean(mse_list)

def hyperparameter_tuning(X, y, cat_idxs, cat_dims):
    """Perform hyperparameter tuning using Optuna."""
    study = optuna.create_study(direction='minimize')
    study.optimize(lambda trial: objective(trial, X, y, cat_idxs, cat_dims), n_trials=20)

    logger.info(f"Best trial: {study.best_trial.params}")

    return study.best_trial.params

# ------------------------------
# 4. Train TabNet Model with Best Hyperparameters
# ------------------------------

def train_tabnet(X_train, y_train, X_valid, y_valid, cat_idxs, cat_dims, best_params):
    """Train the TabNet model with the best hyperparameters."""
    # Extract 'lr' from best_params and create 'optimizer_params'
    optimizer_params = {'lr': best_params.pop('lr')}
    
    # Now pass optimizer_params to TabNetRegressor
    regressor = TabNetRegressor(
        cat_idxs=cat_idxs,
        cat_dims=cat_dims,
        optimizer_params=optimizer_params,
        **best_params
    )

    regressor.fit(
        X_train=X_train,
        y_train=y_train,
        eval_set=[(X_valid, y_valid)],
        eval_metric=['rmse'],
        max_epochs=100,
        patience=20,
        batch_size=1024,
        virtual_batch_size=128,
        num_workers=0,
        drop_last=False,
    )

    # Save the model
    regressor.save_model('tabnet_model')
    logger.info("TabNet model trained and saved successfully.")

    return regressor


# ------------------------------
# 5. Evaluate Model
# ------------------------------

def evaluate_model(regressor, X_valid, y_valid):
    """Evaluate the TabNet model on the validation set."""
    predictions = regressor.predict(X_valid)
    mse = mean_squared_error(y_valid, predictions)
    mae = mean_absolute_error(y_valid, predictions)
    r2 = r2_score(y_valid, predictions)
    logger.info(f"Validation MSE: {mse:.4f}")
    logger.info(f"Validation MAE: {mae:.4f}")
    logger.info(f"Validation R2 Score: {r2:.4f}")

    # Feature Importances
    feature_importances = regressor.feature_importances_
    logger.info("Feature importances extracted.")
    return feature_importances

# ------------------------------
# 6. Save Predictions and Feature Importances
# ------------------------------

def save_results(regressor, X, patient_ids, feature_columns, feature_importances):
    """Save the predictions and feature importances."""
    predictions = regressor.predict(X)
    predictions = predictions.flatten()  # Flatten predictions to 1D array
    predictions_df = pd.DataFrame({'Id': patient_ids, 'Predicted_Health_Index': predictions})
    predictions_df.to_csv('tabnet_predictions.csv', index=False)
    logger.info("TabNet predictions saved.")

    # Save feature importances
    feature_importances_df = pd.DataFrame({
        'Feature': feature_columns,
        'Importance': feature_importances
    })
    feature_importances_df.to_csv('tabnet_feature_importances.csv', index=False)
    logger.info("Feature importances saved.")

# ------------------------------
# Main Execution
# ------------------------------

def main():
    output_dir = 'Data'

    # Load data
    patient_data = load_data(output_dir)
    patient_ids = patient_data['Id'].values

    # Prepare data
    X, y, cat_idxs, cat_dims = prepare_data(patient_data)
    feature_columns = [
        'AGE', 'DECEASED', 'GENDER', 'RACE', 'ETHNICITY', 'MARITAL',
        'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME',
        'Hospitalizations_Count', 'Medications_Count', 'Abnormal_Observations_Count'
    ]

    # Hyperparameter tuning
    best_params = hyperparameter_tuning(X, y, cat_idxs, cat_dims)

    # Split data
    X_train, X_valid, y_train, y_valid = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Train TabNet model with best hyperparameters
    regressor = train_tabnet(X_train, y_train, X_valid, y_valid, cat_idxs, cat_dims, best_params)

    # Evaluate model
    feature_importances = evaluate_model(regressor, X_valid, y_valid)

    # Save predictions and feature importances
    save_results(regressor, X, patient_ids, feature_columns, feature_importances)

if __name__ == '__main__':
    main()
    '''
"""
comprehensive_testing_mem_optimized_refactored.py
Author: Imran Feisal
Date: 12/01/2025

Description:
This memory-optimized script orchestrates multiple experiments to evaluate:
 - Feature configurations: composite, cci, combined
 - Subsets: none, diabetes, ckd
 - Model approaches: vae, tabnet, hybrid

It references:
 - data_preprocessing.py   -> generating patient_data_sequences.pkl
 - health_index.py         -> computing the composite health index
 - charlson_comorbidity.py -> integrating Charlson Comorbidity Index (CCI)
 - vae_model.py            -> training VAE & saving latent features
 - tabnet_model.py         -> training TabNet & saving predictions

Memory-Saving Approaches:
 - Skips hierarchical clustering entirely (AgglomerativeClustering).
 - Uses the entire dataset (no subsampling) for DBSCAN, t-SNE, UMAP, etc.
 - Employs joblib to dump intermediate DataFrames to disk, free memory, and reload as needed.

Usage:
  python comprehensive_testing_mem_optimized_refactored.py
"""

import os
import sys
import gc
import datetime
import logging
import json
import itertools
import time

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from joblib import dump, load

# Clustering & metrics
from sklearn.cluster import KMeans, DBSCAN
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import (
    silhouette_score, calinski_harabasz_score, davies_bouldin_score
)
from sklearn.preprocessing import StandardScaler

# Dimensionality reduction
from sklearn.manifold import TSNE
import umap.umap_ as umap

# Local imports
from data_preprocessing import main as preprocess_main
from health_index import main as health_main
from charlson_comorbidity import load_cci_mapping, compute_cci
from vae_model import main as vae_main
from tabnet_model import main as tabnet_main

# Basic config
OUTPUT_RESULTS_CSV = "comprehensive_experiments_results.csv"
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

###############################################################################
# 0. Helper Functions for "Resume" Logic
###############################################################################
def file_is_fully_written(file_path, min_size=1, max_age_seconds=30):
    """
    Return True if the file_path exists, is larger than min_size (bytes),
    and hasn't been modified in the last `max_age_seconds` (to avoid partial).
    """
    if not os.path.exists(file_path):
        return False

    # Check file size
    if os.path.getsize(file_path) < min_size:
        return False

    # Check last modification time
    mtime = os.path.getmtime(file_path)
    now = time.time()
    if (now - mtime) < max_age_seconds:
        # If the file was *just* modified, there's a small chance it's incomplete
        # We consider it "not fully written" in this window
        return False

    return True

def step_vae_artifacts_ok(vae_prefix):
    """
    For VAE, the final artifact is typically the latent-features CSV.
    Optionally, you could also check the saved model directories.
    """
    latent_csv = f"{vae_prefix}_latent_features.csv"
    return file_is_fully_written(latent_csv, min_size=10)

def step_tabnet_artifacts_ok(tabnet_prefix):
    """
    For TabNet, we expect both predictions CSV and metrics JSON.
    Optionally, also check the model zip file.
    """
    preds_csv = f"{tabnet_prefix}_predictions.csv"
    metrics_json = f"{tabnet_prefix}_metrics.json"
    preds_ok = file_is_fully_written(preds_csv, min_size=10)
    metrics_ok = file_is_fully_written(metrics_json, min_size=2)
    return preds_ok and metrics_ok

def step_clustering_artifacts_ok(config_id, config_folder):
    """
    We consider clustering "done" if we have both tsne and umap plots.
    If you store more artifacts (e.g. a CSV with cluster labels),
    you can check that as well.
    """
    plots_folder = os.path.join(config_folder, "plots")
    tsne_file = os.path.join(plots_folder, f"tsne2d_{config_id}.png")
    umap_file = os.path.join(plots_folder, f"umap2d_{config_id}.png")
    tsne_ok = file_is_fully_written(tsne_file, min_size=1000)
    umap_ok = file_is_fully_written(umap_file, min_size=1000)
    return (tsne_ok and umap_ok)

def all_steps_done(fc, ss, ma, config_folder, run_timestamp):
    """
    If you want to skip the entire (fc, ss, ma) if *all* final artifacts exist,
    define this function. We'll check VAE, TabNet, and clustering if relevant.
    """
    config_id = f"{fc}_{ss}_{ma}"
    # Build typical prefixes
    vae_prefix = os.path.join(config_folder, f"{config_id}_{run_timestamp}_vae")
    tabnet_prefix = os.path.join(config_folder, f"{config_id}_{run_timestamp}_tabnet")

    # Depending on the approach:
    if ma == "vae":
        # Must have VAE done + clustering done
        if step_vae_artifacts_ok(vae_prefix) and step_clustering_artifacts_ok(config_id, config_folder):
            return True
        else:
            return False

    elif ma == "tabnet":
        # Must have tabnet done + clustering done
        if step_tabnet_artifacts_ok(tabnet_prefix) and step_clustering_artifacts_ok(config_id, config_folder):
            return True
        else:
            return False

    elif ma == "hybrid":
        # Must have both VAE & TabNet done + clustering done
        if (step_vae_artifacts_ok(vae_prefix) and 
            step_tabnet_artifacts_ok(tabnet_prefix) and
            step_clustering_artifacts_ok(config_id, config_folder)):
            return True
        else:
            return False

    # Fallback
    return False

###############################################################################
# 1. Ensure Data Preprocessing & CCI
###############################################################################
def ensure_preprocessed_data(data_dir):
    """
    Ensures we have patient_data_sequences.pkl,
    patient_data_with_health_index.pkl, and
    patient_data_with_health_index_cci.pkl.
    If missing, calls relevant scripts.
    """
    pkl_sequences = os.path.join(data_dir, "patient_data_sequences.pkl")
    pkl_health_index = os.path.join(data_dir, "patient_data_with_health_index.pkl")
    pkl_health_cci = os.path.join(data_dir, "patient_data_with_health_index_cci.pkl")

    # data_preprocessing step
    if not os.path.exists(pkl_sequences):
        logger.info("Missing patient_data_sequences.pkl -> Running data_preprocessing.py")
        preprocess_main()
    else:
        logger.info("Preprocessed sequences found.")

    # health_index step
    if not os.path.exists(pkl_health_index):
        logger.info("Missing patient_data_with_health_index.pkl -> Running health_index.py")
        health_main()
    else:
        logger.info("Health index data found.")

    # CCI step
    if not os.path.exists(pkl_health_cci):
        logger.info("Missing patient_data_with_health_index_cci.pkl -> merging CCI.")
        conditions_csv = os.path.join(data_dir, "conditions.csv")
        if not os.path.exists(conditions_csv):
            raise FileNotFoundError("conditions.csv not found. Cannot compute CCI.")

        conditions = pd.read_csv(conditions_csv, usecols=["PATIENT","CODE","DESCRIPTION"])
        cci_map = load_cci_mapping(data_dir)
        patient_cci = compute_cci(conditions, cci_map)

        patient_data = pd.read_pickle(pkl_health_index)
        merged = patient_data.merge(
            patient_cci, how="left", left_on="Id", right_on="PATIENT"
        )
        merged.drop(columns="PATIENT", inplace=True)
        merged["CharlsonIndex"] = merged["CharlsonIndex"].fillna(0.0)
        merged.to_pickle(pkl_health_cci)
        logger.info("[INFO] CCI merged & saved -> %s", pkl_health_cci)
        del conditions, cci_map, patient_cci, patient_data, merged
        gc.collect()
    else:
        logger.info("CCI data found.")


###############################################################################
# 2. Subset Filtering Logic
###############################################################################
def load_conditions(data_dir):
    cpath = os.path.join(data_dir, "conditions.csv")
    if not os.path.exists(cpath):
        raise FileNotFoundError(f"conditions.csv not found at {cpath}.")
    return pd.read_csv(cpath, usecols=['PATIENT','CODE','DESCRIPTION'])

def subset_diabetes(patient_data, data_dir):
    conditions = load_conditions(data_dir)
    mask = conditions['DESCRIPTION'].str.lower().str.contains('diabetes', na=False)
    diabetes_patients = conditions.loc[mask, 'PATIENT'].unique()
    sub = patient_data[patient_data['Id'].isin(diabetes_patients)].copy()
    logger.info(f"[INFO] Diabetes subset shape: {sub.shape}")
    del conditions
    gc.collect()
    return sub

def subset_ckd(patient_data, data_dir):
    conditions = load_conditions(data_dir)
    ckd_snomed = {431855005, 431856006, 433144002, 431857002, 46177005}
    code_mask = conditions['CODE'].isin(ckd_snomed)
    text_mask = conditions['DESCRIPTION'].str.lower().str.contains('chronic kidney disease', na=False)
    ckd_mask = code_mask | text_mask
    ckd_patients = conditions.loc[ckd_mask, 'PATIENT'].unique()
    sub = patient_data[patient_data['Id'].isin(ckd_patients)].copy()
    logger.info(f"[INFO] CKD subset shape: {sub.shape}")
    del conditions
    gc.collect()
    return sub

def filter_subpopulation(patient_data, subset_type, data_dir):
    if subset_type.lower() == "none":
        return patient_data
    elif subset_type.lower() == "diabetes":
        return subset_diabetes(patient_data, data_dir)
    elif subset_type.lower() == "ckd":
        return subset_ckd(patient_data, data_dir)
    else:
        logger.warning(f"Unknown subset_type={subset_type}, returning full data.")
        return patient_data


###############################################################################
# 3. Feature Selection
###############################################################################
def select_features(patient_data, feature_config="composite"):
    """
    Grabs only the columns relevant for each feature config.
    We'll one-hot encode them later, after merging with the model outputs,
    to avoid numeric-categorical mismatch.
    """
    chosen_cols = ['Id']
    base_demo = [
        'GENDER','RACE','ETHNICITY','MARITAL',
        'HEALTHCARE_EXPENSES','HEALTHCARE_COVERAGE','INCOME',
        'AGE','DECEASED','Hospitalizations_Count','Medications_Count','Abnormal_Observations_Count'
    ]
    chosen_cols.extend(base_demo)

    if feature_config == "composite":
        if 'Health_Index' not in patient_data.columns:
            raise KeyError("Missing 'Health_Index' for feature_config='composite'")
        chosen_cols.append('Health_Index')
    elif feature_config == "cci":
        if 'CharlsonIndex' not in patient_data.columns:
            raise KeyError("Missing 'CharlsonIndex' for feature_config='cci'")
        chosen_cols.append('CharlsonIndex')
    elif feature_config == "combined":
        if 'Health_Index' not in patient_data.columns or 'CharlsonIndex' not in patient_data.columns:
            raise KeyError("Missing 'Health_Index' or 'CharlsonIndex' for feature_config='combined'")
        chosen_cols.extend(['Health_Index', 'CharlsonIndex'])
    else:
        raise ValueError(f"Invalid feature_config: {feature_config}")

    return patient_data[chosen_cols].copy()


###############################################################################
# 4. Model Runners
###############################################################################
def run_vae(pkl_file, output_prefix):
    logger.info(f"Running VAE on {pkl_file} with prefix={output_prefix}.")
    vae_main(input_file=pkl_file, output_prefix=output_prefix)
    latent_csv = f"{output_prefix}_latent_features.csv"
    if not os.path.exists(latent_csv):
        logger.warning("[WARN] latent features CSV missing after VAE.")
        return None
    return latent_csv

def run_tabnet(pkl_file, output_prefix, target_col="Health_Index"):
    """
    Modified runner that passes a dynamic target_col to tabnet_main.
    """
    logger.info(f"Running TabNet on {pkl_file} with prefix={output_prefix}, target={target_col}.")
    tabnet_main(input_file=pkl_file, output_prefix=output_prefix, target_col=target_col)
    preds_csv = f"{output_prefix}_predictions.csv"
    if not os.path.exists(preds_csv):
        logger.warning("[WARN] TabNet predictions CSV missing after training.")
        return None
    return preds_csv


###############################################################################
# 5. Clustering & Visualization
###############################################################################
def memory_optimized_clustering_and_visualization(merged_df, config_id, plots_folder):
    """
    Clusters on the entire dataset, saving 2D t-SNE/UMAP plots to 'plots_folder'.
    One-hot encoding must already be done if we have categorical columns.
    """
    os.makedirs(plots_folder, exist_ok=True)

    # Exclude 'Id' and 'Predicted_Health_Index' from clustering
    X_columns = [c for c in merged_df.columns if c not in ('Id', 'Predicted_Health_Index')]
    X_full = merged_df[X_columns].values
    logger.info(f"[CLUSTER] Data shape: {X_full.shape}")

    if X_full.shape[1] == 0:
        logger.warning(f"[CLUSTER] 0 features for {config_id}. Skipping clustering.")
        return {}

    scaler = StandardScaler()
    X_full_scaled = scaler.fit_transform(X_full)

    # 1) K-Means
    cluster_range = range(6, 10)
    kmeans_results = []
    for n in cluster_range:
        km = KMeans(n_clusters=n, random_state=42)
        labels_km = km.fit_predict(X_full_scaled)
        s = silhouette_score(X_full_scaled, labels_km)
        c = calinski_harabasz_score(X_full_scaled, labels_km)
        d = davies_bouldin_score(X_full_scaled, labels_km)
        kmeans_results.append((n, s, c, d))

    kmeans_df = pd.DataFrame(kmeans_results, columns=['k','silhouette','calinski','davies_bouldin'])
    kmeans_df['method'] = 'KMeans'
    kmeans_df['sil_rank'] = kmeans_df['silhouette'].rank(ascending=False)
    kmeans_df['ch_rank'] = kmeans_df['calinski'].rank(ascending=False)
    kmeans_df['db_rank'] = kmeans_df['davies_bouldin'].rank(ascending=True)
    kmeans_df['avg_rank'] = kmeans_df[['sil_rank','ch_rank','db_rank']].mean(axis=1)

    best_k = int(kmeans_df.loc[kmeans_df['avg_rank'].idxmin(), 'k'])
    best_km = KMeans(n_clusters=best_k, random_state=42)
    best_km.fit(X_full_scaled)
    final_labels_kmeans = best_km.predict(X_full_scaled)
    merged_df['Cluster'] = final_labels_kmeans

    # 2) DBSCAN
    neighbors = 5
    nbrs = NearestNeighbors(n_neighbors=neighbors).fit(X_full_scaled)
    dist, idx = nbrs.kneighbors(X_full_scaled)
    dist = np.sort(dist[:, neighbors-1], axis=0)
    epsilon = dist[int(0.9 * len(dist))]
    dbscan = DBSCAN(eps=epsilon, min_samples=5)
    db_labels = dbscan.fit_predict(X_full_scaled)

    def cluster_scores(arr, labels):
        uset = set(labels)
        if len(uset) < 2:
            return (np.nan, np.nan, np.nan)
        return (
            silhouette_score(arr, labels),
            calinski_harabasz_score(arr, labels),
            davies_bouldin_score(arr, labels)
        )

    db_sil, db_cal, db_dav = cluster_scores(X_full_scaled, db_labels)

    # 3) Severity Index if Predicted_Health_Index is present
    if 'Predicted_Health_Index' in merged_df.columns:
        cluster_mean = (
            merged_df
            .groupby('Cluster')['Predicted_Health_Index']
            .mean()
            .sort_values()
            .reset_index()
        )
        cluster_mean['Severity_Index'] = range(1, len(cluster_mean)+1)
        c_map = dict(zip(cluster_mean['Cluster'], cluster_mean['Severity_Index']))
        merged_df['Severity_Index'] = merged_df['Cluster'].map(c_map)

    # 4) t-SNE & UMAP Visualization
    hue_col = 'Severity_Index' if 'Severity_Index' in merged_df.columns else 'Cluster'
    hue_vals = merged_df[hue_col].values

    tsne_2d = TSNE(n_components=2, random_state=42)
    X_tsne_2d = tsne_2d.fit_transform(X_full_scaled)
    plt.figure(figsize=(8,6))
    sns.scatterplot(x=X_tsne_2d[:,0], y=X_tsne_2d[:,1], hue=hue_vals, palette='viridis')
    plt.title(f"t-SNE 2D - KMeans best_k={best_k}, config={config_id}")
    tsne_path = os.path.join(plots_folder, f"tsne2d_{config_id}.png")
    plt.savefig(tsne_path, bbox_inches='tight')
    plt.close()

    umap_2d = umap.UMAP(n_components=2, random_state=42)
    X_umap_2d = umap_2d.fit_transform(X_full_scaled)
    plt.figure(figsize=(8,6))
    sns.scatterplot(x=X_umap_2d[:,0], y=X_umap_2d[:,1], hue=hue_vals, palette='viridis')
    plt.title(f"UMAP 2D - {config_id}")
    umap_path = os.path.join(plots_folder, f"umap2d_{config_id}.png")
    plt.savefig(umap_path, bbox_inches='tight')
    plt.close()

    # 5) Final K-Means metrics
    final_sil = silhouette_score(X_full_scaled, final_labels_kmeans)
    final_ch = calinski_harabasz_score(X_full_scaled, final_labels_kmeans)
    final_db = davies_bouldin_score(X_full_scaled, final_labels_kmeans)

    del X_full, X_full_scaled
    gc.collect()

    return {
        "config_id": config_id,
        "chosen_method": "KMeans",
        "chosen_k": best_k,
        "final_silhouette": final_sil,
        "final_calinski": final_ch,
        "final_davies_bouldin": final_db,
        "dbscan_silhouette": db_sil,
        "dbscan_calinski": db_cal,
        "dbscan_davies_bouldin": db_dav
    }


###############################################################################
# 6. TabNet Regression Performance
###############################################################################
def evaluate_regression_performance(config_id, output_prefix):
    """
    Reads TabNet's metrics JSON if available.
    """
    metrics_file = f"{output_prefix}_metrics.json"
    mse_val, r2_val = np.nan, np.nan
    if os.path.exists(metrics_file):
        # Also check if it's fully written
        if not file_is_fully_written(metrics_file, min_size=2):
            logger.warning(f"Metrics file {metrics_file} might be incomplete.")
            return {"config_id": config_id, "tabnet_mse": np.nan, "tabnet_r2": np.nan}

        try:
            with open(metrics_file, 'r') as f:
                data = json.load(f)
            mse_val = float(data.get("test_mse", np.nan))
            r2_val = float(data.get("test_r2", np.nan))
        except Exception as e:
            logger.warning(f"Could not parse {metrics_file}: {e}")
    else:
        logger.info(f"No TabNet metrics file {metrics_file} found; skipping.")
    return {"config_id": config_id, "tabnet_mse": mse_val, "tabnet_r2": r2_val}


###############################################################################
# 7. Save Overall Experiment Results
###############################################################################
def save_results_to_csv(output_path, results_list):
    df = pd.DataFrame(results_list)
    write_header = not os.path.exists(output_path)
    df.to_csv(output_path, mode='a', header=write_header, index=False)


###############################################################################
# 8. Main Execution
###############################################################################
def main():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(script_dir, "Data")

    # Ensure data is ready
    ensure_preprocessed_data(data_dir)
    cci_path = os.path.join(data_dir, "patient_data_with_health_index_cci.pkl")
    if not os.path.exists(cci_path):
        raise FileNotFoundError("patient_data_with_health_index_cci.pkl missing after preprocessing.")

    full_df = pd.read_pickle(cci_path)
    logger.info("[MAIN] Loaded data shape=%s", full_df.shape)

    feature_configs = ["composite", "cci", "combined"]
    subset_types = ["none", "diabetes", "ckd"]
    model_approaches = ["vae", "tabnet", "hybrid"]

    all_results = []
    # We'll use one global run_timestamp so each combination is time-labeled consistently
    run_timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    # For each (feature_config, subset, model_approach), we do an experiment
    for fc, ss, ma in itertools.product(feature_configs, subset_types, model_approaches):
        config_id = f"{fc}_{ss}_{ma}"
        logger.info("\n========================================")
        logger.info(" Running config: %s", config_id)
        logger.info("========================================")

        # Create a per-config subfolder
        config_folder = os.path.join(data_dir, "Experiments", config_id)
        os.makedirs(config_folder, exist_ok=True)

        # ---------------------------------------------------
        # (Optional) Check if entire config is done -> skip
        # ---------------------------------------------------
        if all_steps_done(fc, ss, ma, config_folder, run_timestamp):
            logger.info("[RESUME] Skipping entire config_id=%s – all artifacts found.", config_id)
            continue

        # 1) Filter subset
        sub_df = filter_subpopulation(full_df, ss, data_dir)
        # 2) Feature selection
        use_df = select_features(sub_df, fc)

        # Dump intermediate to disk, then free memory
        temp_file = os.path.join(config_folder, f"temp_{config_id}_{run_timestamp}.pkl")
        use_df.to_pickle(temp_file)
        del sub_df, use_df
        gc.collect()

        # Build typical model prefixes
        vae_prefix = os.path.join(config_folder, f"{config_id}_{run_timestamp}_vae")
        tabnet_prefix = os.path.join(config_folder, f"{config_id}_{run_timestamp}_tabnet")

        # 3) VAE step if needed
        if ma in ["vae", "hybrid"]:
            if step_vae_artifacts_ok(vae_prefix):
                logger.info("[RESUME] VAE artifacts found for %s, skipping VAE step.", config_id)
            else:
                run_vae(temp_file, vae_prefix)

        # 4) TabNet step if needed
        if ma in ["tabnet", "hybrid"]:
            if step_tabnet_artifacts_ok(tabnet_prefix):
                logger.info("[RESUME] TabNet artifacts found for %s, skipping TabNet step.", config_id)
            else:
                # Decide which target_col to use
                if fc == "cci":
                    # For cci feature config, predict CharlsonIndex
                    run_tabnet(temp_file, tabnet_prefix, target_col="CharlsonIndex")
                else:
                    # For composite or combined, predict Health_Index
                    run_tabnet(temp_file, tabnet_prefix, target_col="Health_Index")

        # 5) Merge outputs & cluster
        #    Only do clustering if at least one model produced an output
        frames = []
        if ma in ["vae", "hybrid"] and step_vae_artifacts_ok(vae_prefix):
            df_lat = pd.read_csv(f"{vae_prefix}_latent_features.csv")
            frames.append(df_lat)
        if ma in ["tabnet", "hybrid"] and step_tabnet_artifacts_ok(tabnet_prefix):
            df_tab = pd.read_csv(f"{tabnet_prefix}_predictions.csv")
            frames.append(df_tab)

        if frames:
            merged_df = frames[0]
            for fdf in frames[1:]:
                merged_df = merged_df.merge(fdf, on='Id', how='inner')

            temp_data = pd.read_pickle(temp_file)
            merged_df = temp_data.merge(merged_df, on='Id', how='inner')

            cat_cols = ['GENDER', 'RACE', 'ETHNICITY', 'MARITAL']
            existing_cat = [c for c in cat_cols if c in merged_df.columns]
            if existing_cat:
                for col in existing_cat:
                    merged_df[col] = merged_df[col].astype(str)
                merged_df = pd.get_dummies(
                    merged_df,
                    columns=existing_cat,
                    drop_first=True
                )

            # If clustering artifacts are missing, do clustering
            if not step_clustering_artifacts_ok(config_id, config_folder):
                logger.info("Performing clustering for %s", config_id)
                plots_folder = os.path.join(config_folder, "plots")
                clust_res = memory_optimized_clustering_and_visualization(
                    merged_df,
                    config_id,
                    plots_folder=plots_folder
                )
                all_results.append(clust_res)
            else:
                logger.info("[RESUME] Clustering plots found for %s, skipping clustering.", config_id)

            del merged_df, temp_data
            for fdf in frames:
                del fdf
            gc.collect()
        else:
            logger.info("[INFO] No model output CSV to cluster/visualize for %s", config_id)

        # 6) Evaluate TabNet if relevant
        if ma in ["tabnet", "hybrid"]:
            # We always do this because we might have partial metrics
            reg_res = evaluate_regression_performance(config_id, tabnet_prefix)
            all_results.append(reg_res)

        # 7) Remove temp file
        if os.path.exists(temp_file):
            os.remove(temp_file)
        gc.collect()

    # 8) Save all results
    results_csv_path = os.path.join(data_dir, OUTPUT_RESULTS_CSV)
    if all_results:
        save_results_to_csv(results_csv_path, all_results)
        logger.info("[MAIN] All experiment results appended to %s", results_csv_path)
    else:
        logger.info("[WARN] No results collected. Check logs for issues.")


if __name__ == "__main__":
    main()

"""
update_comprehensive_results.py
Author: Imran Feisal
Date: 18/01/2025

Description:
Extends the original comprehensive testing framework to:
 - Merge columns from BOTH `patient_data_with_health_index_cci.pkl` (Charlson, etc.)
   and `patient_data_with_health_index.pkl` (Hospital/Med/Abnormal counts, etc.).
 - Read existing VAE/TabNet outputs for each config, merges them with the base data,
   then performs clustering if no t-SNE/UMAP plots are found.
 - Appends newly generated clustering rows to `comprehensive_experiments_results_v2.csv`.
"""

import os
import gc
import glob
import logging
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score
)
from sklearn.manifold import TSNE
import umap.umap_ as umap
import seaborn as sns
import matplotlib.pyplot as plt

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants
DATA_DIR = "Data"
EXPERIMENTS_DIR = os.path.join(DATA_DIR, "Experiments")

ORIGINAL_RESULTS_FILE = "comprehensive_experiments_results.csv"
NEW_RESULTS_FILE = "comprehensive_experiments_results_v2.csv"

# Two pickles that, when merged, contain everything we need:
CCI_PICKLE = os.path.join(DATA_DIR, "patient_data_with_health_index_cci.pkl")
EXTRA_PICKLE = os.path.join(DATA_DIR, "patient_data_with_health_index.pkl")

###############################################################################
# 1. Check if clustering artifacts exist
###############################################################################
def clustering_artifacts_exist(config_id, config_folder):
    plots_folder = os.path.join(config_folder, "plots")
    tsne_file = os.path.join(plots_folder, f"tsne2d_{config_id}.png")
    umap_file = os.path.join(plots_folder, f"umap2d_{config_id}.png")
    return os.path.exists(tsne_file) and os.path.exists(umap_file)

###############################################################################
# 2. Perform Clustering + Visualisation
###############################################################################
def perform_clustering_and_visualization(merged_df, config_id, plots_folder):
    """
    Does KMeans in [6..9], picks best by silhouette. Also runs DBSCAN for reference.
    Excludes 'Id', 'Predicted_Health_Index', 'Predicted_CharlsonIndex' from features.
    Skips if <2 rows or <2 features.
    Saves t-SNE and UMAP into {plots_folder}/tsne2d_{config_id}.png, etc.
    Returns a dict of cluster metrics to be appended to final CSV.
    """
    os.makedirs(plots_folder, exist_ok=True)

    exclude_cols = {"Id", "Predicted_Health_Index", "Predicted_CharlsonIndex"}
    X_cols = [c for c in merged_df.columns if c not in exclude_cols]
    X = merged_df[X_cols].values

    n_rows, n_feats = X.shape
    if n_rows < 2 or n_feats < 2:
        logger.warning(
            f"[{config_id}] Cannot cluster: not enough rows/features ({n_rows}x{n_feats})."
        )
        return {}

    # Scale
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # KMeans [6..9]
    best_k = None
    best_metrics = None
    for k in range(6, 10):
        km = KMeans(n_clusters=k, random_state=42)
        labels = km.fit_predict(X_scaled)
        sil = silhouette_score(X_scaled, labels)
        cal = calinski_harabasz_score(X_scaled, labels)
        dav = davies_bouldin_score(X_scaled, labels)
        # Simple best-silhouette approach
        if (best_metrics is None) or (sil > best_metrics["sil"]):
            best_k = k
            best_metrics = {"sil": sil, "cal": cal, "dav": dav}

    # Fit final KMeans with best_k
    final_km = KMeans(n_clusters=best_k, random_state=42).fit(X_scaled)
    final_labels = final_km.predict(X_scaled)
    merged_df["Cluster"] = final_labels

    # Evaluate final cluster metrics
    final_sil = silhouette_score(X_scaled, final_labels)
    final_cal = calinski_harabasz_score(X_scaled, final_labels)
    final_dav = davies_bouldin_score(X_scaled, final_labels)

    # DBSCAN for reference
    db = DBSCAN(eps=0.5, min_samples=5).fit(X_scaled)
    db_labels = db.labels_
    if len(set(db_labels)) < 2:
        db_sil, db_cal, db_dav = None, None, None
    else:
        db_sil = silhouette_score(X_scaled, db_labels)
        db_cal = calinski_harabasz_score(X_scaled, db_labels)
        db_dav = davies_bouldin_score(X_scaled, db_labels)

    # t-SNE & UMAP if enough points
    if n_rows > 1:
        # t-SNE
        tsne = TSNE(n_components=2, random_state=42)
        X_tsne = tsne.fit_transform(X_scaled)
        plt.figure(figsize=(7,5))
        sns.scatterplot(
            x=X_tsne[:,0],
            y=X_tsne[:,1],
            hue=merged_df["Cluster"],
            palette="viridis"
        )
        plt.title(f"t-SNE for {config_id} (K={best_k})")
        tsne_path = os.path.join(plots_folder, f"tsne2d_{config_id}.png")
        plt.savefig(tsne_path, bbox_inches="tight")
        plt.close()

        # UMAP
        reducer = umap.UMAP(n_components=2, random_state=42)
        X_umap = reducer.fit_transform(X_scaled)
        plt.figure(figsize=(7,5))
        sns.scatterplot(
            x=X_umap[:,0],
            y=X_umap[:,1],
            hue=merged_df["Cluster"],
            palette="viridis"
        )
        plt.title(f"UMAP for {config_id} (K={best_k})")
        umap_path = os.path.join(plots_folder, f"umap2d_{config_id}.png")
        plt.savefig(umap_path, bbox_inches="tight")
        plt.close()

    return {
        "config_id": config_id,
        "chosen_method": "KMeans",
        "chosen_k": best_k,
        "final_silhouette": final_sil,
        "final_calinski": final_cal,
        "final_davies_bouldin": final_dav,
        "dbscan_silhouette": db_sil,
        "dbscan_calinski": db_cal,
        "dbscan_davies_bouldin": db_dav
    }

###############################################################################
# 3. Main Update Logic
###############################################################################
def main():
    # Load the original results file if it exists
    original_results_path = os.path.join(DATA_DIR, ORIGINAL_RESULTS_FILE)
    if os.path.exists(original_results_path):
        original_results = pd.read_csv(original_results_path)
    else:
        original_results = pd.DataFrame()

    # Load both pickles so we can have Charlson + extra derived columns
    if not os.path.exists(CCI_PICKLE):
        logger.error(f"Missing {CCI_PICKLE}; cannot proceed.")
        return
    if not os.path.exists(EXTRA_PICKLE):
        logger.error(f"Missing {EXTRA_PICKLE}; cannot proceed.")
        return

    df_cci = pd.read_pickle(CCI_PICKLE)   # Contains 'CharlsonIndex', etc.
    df_extra = pd.read_pickle(EXTRA_PICKLE)  # Contains 'Hospitalizations_Count', etc.

    # Merge them on "Id" to get all columns
    # If both have the same columns, 'how="outer"' or 'how="left"'; up to you.
    base_df = df_cci.merge(
        df_extra[[
            "Id",
            "Hospitalizations_Count",
            "Medications_Count",
            "Abnormal_Observations_Count"
        ]],
        on="Id",
        how="left"
    )
    logger.info(f"[BASE] Combined shape = {base_df.shape}")

    # If you still have a "SEQUENCE" or "PATIENT" column, drop it if present
    drop_cols = ["SEQUENCE", "PATIENT"]
    for col in drop_cols:
        if col in base_df.columns:
            base_df.drop(columns=[col], inplace=True)

    # For any columns that might contain lists, drop them
    for col in list(base_df.columns):
        if base_df[col].map(type).eq(list).any():
            logger.warning(f"[BASE] Dropping {col} because it contains list data.")
            base_df.drop(columns=[col], inplace=True)

    # Now let's do the main loop
    all_results = []
    config_dirs = sorted(os.listdir(EXPERIMENTS_DIR))
    for config_dir in config_dirs:
        config_path = os.path.join(EXPERIMENTS_DIR, config_dir)
        if not os.path.isdir(config_path):
            continue

        config_id = config_dir
        plots_folder = os.path.join(config_path, "plots")

        # Skip if clustering artifacts exist
        if clustering_artifacts_exist(config_id, config_path):
            logger.info(f"[SKIP] Already has clustering plots for {config_id}.")
            continue

        # Find VAE / TabNet outputs
        latents = glob.glob(os.path.join(config_path, "*_latent_features.csv"))
        preds = glob.glob(os.path.join(config_path, "*_predictions.csv"))

        latent_csv = latents[0] if latents else None
        preds_csv = preds[0] if preds else None

        if not latent_csv and not preds_csv:
            logger.warning(f"[MISSING] No VAE or TabNet CSV for {config_id}")
            continue

        # Merge the base data with whichever we have
        merged = base_df.copy()

        if latent_csv:
            df_lat = pd.read_csv(latent_csv)
            merged = merged.merge(df_lat, on="Id", how="inner")

        if preds_csv:
            df_pred = pd.read_csv(preds_csv)
            merged = merged.merge(df_pred, on="Id", how="inner")

        logger.info(f"[{config_id}] final merged shape={merged.shape}")

        # If any columns hold list data, drop them
        for col in list(merged.columns):
            if merged[col].map(type).eq(list).any():
                logger.warning(f"[{config_id}] Dropping column {col} with list data.")
                merged.drop(columns=[col], inplace=True)

        # One-hot encode the same demographic columns the original script used
        cat_cols = ["GENDER", "RACE", "ETHNICITY", "MARITAL"]
        for c in cat_cols:
            if c in merged.columns:
                merged[c] = merged[c].astype(str)
        merged = pd.get_dummies(
            merged, 
            columns=[c for c in cat_cols if c in merged.columns],
            drop_first=True
        )

        # Perform clustering
        res = perform_clustering_and_visualization(merged, config_id, plots_folder)
        if res:
            all_results.append(res)

        del merged
        gc.collect()

    # Append new rows to the original
    if all_results:
        new_df = pd.DataFrame(all_results)
        combined_df = pd.concat([original_results, new_df], ignore_index=True)
        out_path = os.path.join(DATA_DIR, NEW_RESULTS_FILE)
        combined_df.to_csv(out_path, index=False)
        logger.info(f"[DONE] Wrote updated results -> {NEW_RESULTS_FILE}")
    else:
        logger.info("[INFO] No new clustering results generated.")

if __name__ == "__main__":
    main()

