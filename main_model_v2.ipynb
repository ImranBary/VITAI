{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder with Clustering for Health Severity Analysis\n",
    "This notebook implements a Variational Autoencoder (VAE) on structured health data to identify patient clusters corresponding to different levels of health severity. The steps include:\n",
    "1. Setup and Configuration\n",
    "2. Data Loading and Preprocessing\n",
    "3. VAE Model Definition and Hyperparameter Tuning\n",
    "4. Model Training and Reconstruction Error Calculation\n",
    "5. Latent Space Extraction and Clustering\n",
    "6. Latent Space Visualization\n",
    "7. Health Severity Index Assignment\n",
    "8. Cluster Analysis and Validation\n",
    "9. Memory Management and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and deep learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Enabled memory growth on GPU\n"
     ]
    }
   ],
   "source": [
    "# Check if TensorFlow is using the GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Enable memory growth for GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Enabled memory growth on GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. The code will run on CPU, which might be slower.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Preprocessing\n",
    "We load the preprocessed structured data, standardize it, and split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed structured data\n",
    "structured_data = pd.read_csv('structured_data_preprocessed.csv')\n",
    "\n",
    "# Separate features\n",
    "X_structured = structured_data.values.astype('float32')  # Use float32 for efficiency\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_structured)\n",
    "\n",
    "# Save scaler for future use\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Clear unnecessary variables\n",
    "del X_structured\n",
    "del X_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. VAE Model Definition and Hyperparameter Tuning\n",
    "We define the VAE model and set up hyperparameter tuning using Keras Tuner's \n",
    "<code>BayesianOptimization</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sampling Layer\n",
    "# class Sampling(layers.Layer):\n",
    "#     def call(self, inputs):\n",
    "#         z_mean, z_log_var = inputs\n",
    "#         batch = tf.shape(z_mean)[0]\n",
    "#         dim = tf.shape(z_mean)[1]\n",
    "#         epsilon = tf.random.normal(shape=(batch, dim))\n",
    "#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# # VAE Model\n",
    "# class VAE(tf.keras.Model):\n",
    "#     def __init__(self, encoder, decoder, input_dim, **kwargs):\n",
    "#         super(VAE, self).__init__(**kwargs)\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.input_dim = input_dim\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         z_mean, z_log_var, z = self.encoder(inputs)\n",
    "#         reconstructed = self.decoder(z)\n",
    "        \n",
    "#         reconstruction_loss = tf.reduce_mean(\n",
    "#             tf.keras.losses.MeanSquaredError()(inputs, reconstructed)\n",
    "#         ) * self.input_dim\n",
    "        \n",
    "#         # KL divergence loss\n",
    "#         kl_loss = -0.5 * tf.reduce_mean(\n",
    "#             1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "#         )\n",
    "        \n",
    "#         # Add the losses to the model\n",
    "#         self.add_loss(reconstruction_loss)\n",
    "#         self.add_loss(kl_loss)\n",
    "        \n",
    "#         return reconstructed\n",
    "\n",
    "\n",
    "# Sampling Layer\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# VAE Model\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, input_dim, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.MeanSquaredError()(inputs, reconstructed)\n",
    "        ) * self.input_dim\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "        )\n",
    "        \n",
    "        # Add the losses to the model\n",
    "        self.add_loss(reconstruction_loss)\n",
    "        self.add_loss(kl_loss)\n",
    "        \n",
    "        return reconstructed\n",
    "\n",
    "    def get_config(self):\n",
    "        # Serialize encoder and decoder as part of the config\n",
    "        config = super(VAE, self).get_config()\n",
    "        config.update({\n",
    "            'encoder': self.encoder.get_config(),\n",
    "            'decoder': self.decoder.get_config(),\n",
    "            'input_dim': self.input_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Rebuild encoder and decoder from their configs\n",
    "        encoder_config = config.pop('encoder')\n",
    "        decoder_config = config.pop('decoder')\n",
    "        \n",
    "        encoder = tf.keras.Model.from_config(encoder_config)\n",
    "        decoder = tf.keras.Model.from_config(decoder_config)\n",
    "        input_dim = config.pop('input_dim')\n",
    "        \n",
    "        return cls(encoder, decoder, input_dim, **config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Define the VAE Hypermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vae(hp):\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Encoder\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    x = input_layer\n",
    "    num_layers = hp.Int('num_layers', 1, 3)\n",
    "    activation = hp.Choice('activation', ['relu', 'tanh'])\n",
    "    l2_reg = hp.Float('l2_reg', 1e-5, 1e-3, sampling='log')\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f'units_{i}', min_value=64, max_value=256, step=64)\n",
    "        x = layers.Dense(\n",
    "            units,\n",
    "            activation=activation,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
    "        )(x)\n",
    "    \n",
    "    # Latent Space\n",
    "    encoding_dim = hp.Int('encoding_dim', min_value=2, max_value=32, step=2)\n",
    "    z_mean = layers.Dense(encoding_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(encoding_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    # Encoder Model\n",
    "    encoder = tf.keras.Model(inputs=input_layer, outputs=[z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    # Decoder\n",
    "    latent_inputs = layers.Input(shape=(encoding_dim,))\n",
    "    x = latent_inputs\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f'units_dec_{i}', min_value=64, max_value=256, step=64)\n",
    "        x = layers.Dense(\n",
    "            units,\n",
    "            activation=activation,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
    "        )(x)\n",
    "    \n",
    "    outputs = layers.Dense(input_dim, activation='linear')(x)\n",
    "    \n",
    "    # Decoder Model\n",
    "    decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "    \n",
    "    # VAE Model\n",
    "    vae = VAE(encoder, decoder, input_dim=input_dim)\n",
    "    \n",
    "    # Compile the model\n",
    "    learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return vae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Set Up Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from vae_tuning\\vitai_vae\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Set up the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_vae,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='vae_tuning',\n",
    "    project_name='vitai_vae'\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Run Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter search\n",
    "tuner.search(\n",
    "    X_train, X_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training and Reconstruction Error Calculation\n",
    "After finding the best hyperparameters, we train the VAE model and compute the reconstruction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Train the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal number of layers is 2 encoder and decoder layers.\n",
      "The optimal number of units in each layer are:\n",
      "Encoder units: [256, 64]\n",
      "Decoder units: [192, 256]\n",
      "The optimal activation function is relu.\n",
      "The optimal encoding dimension is 30.\n",
      "The optimal learning rate is 0.0002519654858611606.\n",
      "\n",
      "Epoch 1/50\n",
      "1461/1461 [==============================] - 6s 3ms/step - loss: 23.1579 - val_loss: 11.1437\n",
      "Epoch 2/50\n",
      "1461/1461 [==============================] - 4s 3ms/step - loss: 9.6258 - val_loss: 8.3802\n",
      "Epoch 3/50\n",
      "1461/1461 [==============================] - 3s 2ms/step - loss: 7.7346 - val_loss: 6.9228\n",
      "Epoch 4/50\n",
      "1461/1461 [==============================] - 3s 2ms/step - loss: 6.9081 - val_loss: 6.4075\n",
      "Epoch 5/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 6.1535 - val_loss: 5.8434\n",
      "Epoch 6/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 5.7547 - val_loss: 5.6576\n",
      "Epoch 7/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 5.5261 - val_loss: 5.4472\n",
      "Epoch 8/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 5.2205 - val_loss: 5.1516\n",
      "Epoch 9/50\n",
      "1461/1461 [==============================] - 3s 2ms/step - loss: 5.1870 - val_loss: 4.9668\n",
      "Epoch 10/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 4.8852 - val_loss: 4.8838\n",
      "Epoch 11/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 4.7749 - val_loss: 4.7187\n",
      "Epoch 12/50\n",
      "1461/1461 [==============================] - 3s 2ms/step - loss: 4.9096 - val_loss: 4.6285\n",
      "Epoch 13/50\n",
      "1461/1461 [==============================] - 4s 3ms/step - loss: 4.6127 - val_loss: 4.4658\n",
      "Epoch 14/50\n",
      "1461/1461 [==============================] - 4s 3ms/step - loss: 4.4320 - val_loss: 4.4438\n",
      "Epoch 15/50\n",
      "1461/1461 [==============================] - 4s 3ms/step - loss: 4.4657 - val_loss: 4.3907\n",
      "Epoch 16/50\n",
      "1461/1461 [==============================] - 4s 3ms/step - loss: 4.3083 - val_loss: 4.3673\n",
      "Epoch 17/50\n",
      "1461/1461 [==============================] - 4s 3ms/step - loss: 4.5430 - val_loss: 4.2243\n",
      "Epoch 18/50\n",
      "1461/1461 [==============================] - 5s 3ms/step - loss: 4.1623 - val_loss: 4.2018\n",
      "Epoch 19/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 4.1856 - val_loss: 4.1483\n",
      "Epoch 20/50\n",
      "1461/1461 [==============================] - 3s 2ms/step - loss: 4.2304 - val_loss: 4.1165\n",
      "Epoch 21/50\n",
      "1461/1461 [==============================] - 3s 2ms/step - loss: 4.0485 - val_loss: 4.0480\n",
      "Epoch 22/50\n",
      "1461/1461 [==============================] - 4s 3ms/step - loss: 4.0084 - val_loss: 4.0406\n",
      "Epoch 23/50\n",
      "1461/1461 [==============================] - 4s 3ms/step - loss: 4.0689 - val_loss: 4.1602\n",
      "Epoch 24/50\n",
      "1461/1461 [==============================] - 4s 3ms/step - loss: 4.2057 - val_loss: 3.9491\n",
      "Epoch 25/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.9002 - val_loss: 3.9430\n",
      "Epoch 26/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 4.0899 - val_loss: 3.8531\n",
      "Epoch 27/50\n",
      "1461/1461 [==============================] - 4s 3ms/step - loss: 3.8434 - val_loss: 3.8652\n",
      "Epoch 28/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.8400 - val_loss: 3.8554\n",
      "Epoch 29/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.7872 - val_loss: 3.8003\n",
      "Epoch 30/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.7820 - val_loss: 3.7310\n",
      "Epoch 31/50\n",
      "1461/1461 [==============================] - 3s 2ms/step - loss: 4.0160 - val_loss: 3.7086\n",
      "Epoch 32/50\n",
      "1461/1461 [==============================] - 3s 2ms/step - loss: 3.7685 - val_loss: 3.7057\n",
      "Epoch 33/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.6863 - val_loss: 3.7127\n",
      "Epoch 34/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.7180 - val_loss: 3.6433\n",
      "Epoch 35/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.6423 - val_loss: 3.6574\n",
      "Epoch 36/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.6075 - val_loss: 3.6577\n",
      "Epoch 37/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.6161 - val_loss: 3.6306\n",
      "Epoch 38/50\n",
      "1461/1461 [==============================] - 3s 2ms/step - loss: 3.6418 - val_loss: 3.5811\n",
      "Epoch 39/50\n",
      "1461/1461 [==============================] - 3s 2ms/step - loss: 3.7508 - val_loss: 3.6239\n",
      "Epoch 40/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.6288 - val_loss: 3.5261\n",
      "Epoch 41/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.5052 - val_loss: 3.5493\n",
      "Epoch 42/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.5035 - val_loss: 3.5078\n",
      "Epoch 43/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.4812 - val_loss: 3.5225\n",
      "Epoch 44/50\n",
      "1461/1461 [==============================] - 4s 3ms/step - loss: 3.4882 - val_loss: 3.4976\n",
      "Epoch 45/50\n",
      "1461/1461 [==============================] - 4s 3ms/step - loss: 3.4920 - val_loss: 3.4739\n",
      "Epoch 46/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.4452 - val_loss: 3.4726\n",
      "Epoch 47/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.4905 - val_loss: 3.5167\n",
      "Epoch 48/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.5622 - val_loss: 3.4253\n",
      "Epoch 49/50\n",
      "1461/1461 [==============================] - 3s 2ms/step - loss: 3.3914 - val_loss: 3.4115\n",
      "Epoch 50/50\n",
      "1461/1461 [==============================] - 4s 2ms/step - loss: 3.3923 - val_loss: 3.4039\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 27\u001b[0m\n\u001b[0;32m     18\u001b[0m history \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     19\u001b[0m     X_train, X_train,\n\u001b[0;32m     20\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping]\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Save the model in TensorFlow format (use .keras or .h5 extension)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvae_model.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use .keras or .h5 for saving\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Clear session to free memory\u001b[39;00m\n\u001b[0;32m     31\u001b[0m keras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n",
      "File \u001b[1;32mc:\\Users\\imran\\miniconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\imran\\miniconda3\\envs\\py310\\lib\\site-packages\\keras\\saving\\save.py:153\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    145\u001b[0m     save_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile))\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m saving_utils\u001b[38;5;241m.\u001b[39mis_hdf5_filepath(filepath)\n\u001b[0;32m    148\u001b[0m ):\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# TODO(b/130258301): add utility method for detecting model type.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    151\u001b[0m         model, sequential\u001b[38;5;241m.\u001b[39mSequential\n\u001b[0;32m    152\u001b[0m     ):\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubclassed models, because such models are defined via the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody of a Python method, which isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt safely serializable. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider saving to the Tensorflow SavedModel format (by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetting save_format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) or using `save_weights`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    160\u001b[0m         )\n\u001b[0;32m    161\u001b[0m     hdf5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[0;32m    162\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the optimal hyperparameters\n",
    "print(f\"\"\"\n",
    "The optimal number of layers is {best_hps.get('num_layers')} encoder and decoder layers.\n",
    "The optimal number of units in each layer are:\n",
    "Encoder units: {[best_hps.get(f'units_{i}') for i in range(best_hps.get('num_layers'))]}\n",
    "Decoder units: {[best_hps.get(f'units_dec_{i}') for i in range(best_hps.get('num_layers'))]}\n",
    "The optimal activation function is {best_hps.get('activation')}.\n",
    "The optimal encoding dimension is {best_hps.get('encoding_dim')}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build and train the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Save the model in TensorFlow format (use .keras or .h5 extension)\n",
    "best_model.save('vae_model.keras')  # Use .keras or .h5 for saving\n",
    "\n",
    "\n",
    "# Clear session to free memory\n",
    "keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Compute Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1461/1461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "custom_objects = {'VAE': VAE, 'Sampling': Sampling}\n",
    "best_model = tf.keras.models.load_model('vae_model.keras', custom_objects=custom_objects)\n",
    "\n",
    "# Re-compile the model to include custom loss\n",
    "best_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_hps.get('learning_rate')))\n",
    "\n",
    "\n",
    "# Compute reconstruction error for the entire training dataset\n",
    "reconstructed = best_model.predict(X_train, batch_size=64)\n",
    "reconstruction_errors = np.mean(np.square(X_train - reconstructed), axis=1)\n",
    "\n",
    "# Add reconstruction error to the data\n",
    "train_reconstruction_error = pd.DataFrame({'Reconstruction Error': reconstruction_errors})\n",
    "\n",
    "# Save reconstruction errors\n",
    "train_reconstruction_error.to_csv('train_reconstruction_error.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 Plot Reconstruction Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the reconstruction error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstruction_errors, bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.title('Reconstruction Error Distribution (Training Data)')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Latent Space Extraction and Clustering\n",
    "We extract the latent features from the encoder and apply clustering algorithms to identify patient groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1 Extract Latent Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1461/1461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 482us/step\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Extract Latent Features\n",
    "\n",
    "# Use the encoder model directly from the best_model\n",
    "encoder = best_model.encoder\n",
    "\n",
    "# Obtain latent representation\n",
    "z_mean, z_log_var, z = encoder.predict(X_train, batch_size=64)\n",
    "\n",
    "# Use the sampled latent vector z for clustering\n",
    "latent_features = z\n",
    "\n",
    "# Create a DataFrame for latent features\n",
    "latent_dim = best_hps.get('encoding_dim')\n",
    "latent_features_df = pd.DataFrame(data=latent_features, columns=[f'latent_{i}' for i in range(latent_dim)])\n",
    "\n",
    "# Add reconstruction error to the latent features DataFrame\n",
    "latent_features_df['reconstruction_error'] = reconstruction_errors\n",
    "\n",
    "# Save the latent features\n",
    "latent_features_df.to_csv('latent_features.csv', index=False)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 Clustering and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing KMeans...\n",
      "For n_clusters = 2, the average silhouette_score is : 0.054483287036418915\n",
      "For n_clusters = 3, the average silhouette_score is : 0.05288277193903923\n",
      "For n_clusters = 4, the average silhouette_score is : 0.034134216606616974\n",
      "For n_clusters = 5, the average silhouette_score is : 0.027851931750774384\n",
      "For n_clusters = 6, the average silhouette_score is : 0.04369853064417839\n",
      "For n_clusters = 7, the average silhouette_score is : 0.04213356226682663\n",
      "For n_clusters = 8, the average silhouette_score is : 0.0467820018529892\n",
      "For n_clusters = 9, the average silhouette_score is : 0.04406581073999405\n",
      "\n",
      "Testing DBSCAN...\n",
      "\n",
      "Testing AgglomerativeClustering...\n"
     ]
    }
   ],
   "source": [
    "# Clustering algorithms to try\n",
    "clustering_algorithms = {\n",
    "    'KMeans': KMeans,\n",
    "    'DBSCAN': DBSCAN,\n",
    "    'AgglomerativeClustering': AgglomerativeClustering\n",
    "}\n",
    "\n",
    "# For KMeans and AgglomerativeClustering, test different numbers of clusters\n",
    "range_n_clusters = list(range(2, 10))\n",
    "\n",
    "best_algorithm = None\n",
    "best_score = -1\n",
    "best_labels = None\n",
    "best_n_clusters = None\n",
    "\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    if name == 'KMeans' or name == 'AgglomerativeClustering':\n",
    "        for n_clusters in range_n_clusters:\n",
    "            if name == 'KMeans':\n",
    "                clustering = algorithm(n_clusters=n_clusters, random_state=42)\n",
    "            else:\n",
    "                clustering = algorithm(n_clusters=n_clusters)\n",
    "            cluster_labels = clustering.fit_predict(latent_features)\n",
    "            silhouette_avg = silhouette_score(latent_features, cluster_labels)\n",
    "            print(f\"For n_clusters = {n_clusters}, the average silhouette_score is : {silhouette_avg}\")\n",
    "            if silhouette_avg > best_score:\n",
    "                best_score = silhouette_avg\n",
    "                best_algorithm = name\n",
    "                best_labels = cluster_labels\n",
    "                best_n_clusters = n_clusters\n",
    "    elif name == 'DBSCAN':\n",
    "        # Try different eps and min_samples\n",
    "        eps_values = [0.3, 0.5, 0.7]\n",
    "        min_samples_values = [5, 10]\n",
    "        for eps in eps_values:\n",
    "            for min_samples in min_samples_values:\n",
    "                clustering = algorithm(eps=eps, min_samples=min_samples)\n",
    "                cluster_labels = clustering.fit_predict(latent_features)\n",
    "                # For DBSCAN, some labels might be -1 (noise)\n",
    "                # We need at least 2 clusters to compute silhouette score\n",
    "                if len(set(cluster_labels)) > 1 and -1 not in set(cluster_labels):\n",
    "                    silhouette_avg = silhouette_score(latent_features, cluster_labels)\n",
    "                    print(f\"For eps = {eps}, min_samples = {min_samples}, the average silhouette_score is : {silhouette_avg}\")\n",
    "                    if silhouette_avg > best_score:\n",
    "                        best_score = silhouette_avg\n",
    "                        best_algorithm = f\"{name} (eps={eps}, min_samples={min_samples})\"\n",
    "                        best_labels = cluster_labels\n",
    "                        best_n_clusters = len(set(cluster_labels))\n",
    "    else:\n",
    "        print(f\"Algorithm {name} not implemented.\")\n",
    "\n",
    "print(f\"\\nBest algorithm: {best_algorithm} with {best_n_clusters} clusters and silhouette score of {best_score}\")\n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "latent_features_df['cluster'] = best_labels\n",
    "\n",
    "# Save the updated latent features\n",
    "latent_features_df.to_csv('latent_features_with_clusters.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Latent Space Visualization\n",
    "We visualize the latent space using t-SNE and UMAP to understand the cluster formations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1 t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use t-SNE for visualization\n",
    "print(\"Performing t-SNE...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "latent_2d = tsne.fit_transform(latent_features)\n",
    "\n",
    "# Add to DataFrame\n",
    "latent_features_df['tsne_1'] = latent_2d[:, 0]\n",
    "latent_features_df['tsne_2'] = latent_2d[:, 1]\n",
    "\n",
    "# Plot t-SNE\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='cluster', data=latent_features_df, palette='viridis', legend='full')\n",
    "plt.title('Latent Space Visualization with t-SNE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2 UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UMAP for visualization\n",
    "try:\n",
    "    import umap\n",
    "\n",
    "    print(\"Performing UMAP...\")\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    latent_2d_umap = reducer.fit_transform(latent_features)\n",
    "\n",
    "    # Add to DataFrame\n",
    "    latent_features_df['umap_1'] = latent_2d_umap[:, 0]\n",
    "    latent_features_df['umap_2'] = latent_2d_umap[:, 1]\n",
    "\n",
    "    # Plot UMAP\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='umap_1', y='umap_2', hue='cluster', data=latent_features_df, palette='viridis', legend='full')\n",
    "    plt.title('Latent Space Visualization with UMAP')\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"UMAP is not installed. Skipping UMAP visualization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Health Severity Index Assignment\n",
    "We assign a health severity index based on the cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map clusters to severity scores\n",
    "cluster_severity = {cluster: index for index, cluster in enumerate(sorted(latent_features_df['cluster'].unique()))}\n",
    "latent_features_df['severity_index'] = latent_features_df['cluster'].map(cluster_severity)\n",
    "\n",
    "# Optionally, scale severity index to 0-10 range\n",
    "scaler_severity = MinMaxScaler(feature_range=(0, 10))\n",
    "latent_features_df['severity_index_scaled'] = scaler_severity.fit_transform(latent_features_df[['severity_index']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Cluster Analysis and Validation\n",
    "We analyze the clusters to ensure they make clinical sense by examining the characteristics of each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.1 Combine Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original data with cluster labels\n",
    "analysis_df = pd.DataFrame(X_train, columns=structured_data.columns)\n",
    "analysis_df['cluster'] = latent_features_df['cluster']\n",
    "analysis_df['severity_index'] = latent_features_df['severity_index_scaled']\n",
    "analysis_df['reconstruction_error'] = latent_features_df['reconstruction_error']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.2 Compute Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and compute summary statistics\n",
    "cluster_summary = analysis_df.groupby('cluster').mean()\n",
    "print(\"\\nCluster Summary Statistics:\")\n",
    "print(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.3 Visualize Reconstruction Error by Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction error by cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='cluster', y='reconstruction_error', data=analysis_df)\n",
    "plt.title('Reconstruction Error by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4 Visualize Key Features by Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key features by cluster\n",
    "key_features = structured_data.columns.tolist()  # List of feature names\n",
    "\n",
    "# Limit to first 5 features for brevity\n",
    "for feature in key_features[:5]:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='cluster', y=feature, data=analysis_df)\n",
    "    plt.title(f'Distribution of {feature} by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Memory Management and Cleanup\n",
    "We perform cleanup to manage memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear variables to free memory\n",
    "del X_train\n",
    "del X_val\n",
    "del latent_features\n",
    "del latent_2d\n",
    "if 'latent_2d_umap' in locals():\n",
    "    del latent_2d_umap\n",
    "del encoder\n",
    "del best_model\n",
    "\n",
    "print(\"Analysis complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we successfully:\n",
    "<ul>\n",
    "<li>Implemented a Variational Autoencoder to learn latent representations of patient health data.</li>\n",
    "<li>Tuned hyperparameters to find the optimal VAE configuration.</li>\n",
    "<li>Calculated reconstruction errors to assess model performance.</li>\n",
    "<li>Applied different clustering algorithms to the latent space to identify patient groups.</li>\n",
    "<li>Visualized the latent space using t-SNE and UMAP.</li>\n",
    "<li>Assigned a health severity index based on cluster assignments.</li>\n",
    "<li>Analyzed clusters to interpret the severity levels.</li>\n",
    "</ul>\n",
    "<br>\n",
    "This approach allows us to identify patterns in patient data that may correspond to varying levels of health severity, aiding in clinical decision-making and resource allocation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
