{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder with Clustering for Health Severity Analysis\n",
    "This notebook implements a Variational Autoencoder (VAE) on structured health data to identify patient clusters corresponding to different levels of health severity. The steps include:\n",
    "1. Setup and Configuration\n",
    "2. Data Loading and Preprocessing\n",
    "3. VAE Model Definition and Hyperparameter Tuning\n",
    "4. Model Training and Reconstruction Error Calculation\n",
    "5. Latent Space Extraction and Clustering\n",
    "6. Latent Space Visualization\n",
    "7. Health Severity Index Assignment\n",
    "8. Cluster Analysis and Validation\n",
    "9. Memory Management and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and deep learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "No GPU found. The code will run on CPU, which might be slower.\n"
     ]
    }
   ],
   "source": [
    "# Check if TensorFlow is using the GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Enable memory growth for GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Enabled memory growth on GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. The code will run on CPU, which might be slower.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Preprocessing\n",
    "We load the preprocessed structured data, standardize it, and split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed structured data\n",
    "structured_data = pd.read_csv('structured_data_preprocessed.csv')\n",
    "\n",
    "# Separate features\n",
    "X_structured = structured_data.values.astype('float32')  # Use float32 for efficiency\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_structured)\n",
    "\n",
    "# Save scaler for future use\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Clear unnecessary variables\n",
    "del X_structured\n",
    "del X_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. VAE Model Definition and Hyperparameter Tuning\n",
    "We define the VAE model and set up hyperparameter tuning using Keras Tuner's \n",
    "<code>BayesianOptimization</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Define the VAE Hypermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vae(hp):\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Sampling Layer\n",
    "    class Sampling(layers.Layer):\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    # Encoder\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    x = input_layer\n",
    "    num_layers = hp.Int('num_layers', 1, 3)\n",
    "    activation = hp.Choice('activation', ['relu', 'tanh'])\n",
    "    l2_reg = hp.Float('l2_reg', 1e-5, 1e-3, sampling='log')\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f'units_{i}', min_value=64, max_value=256, step=64)\n",
    "        x = layers.Dense(\n",
    "            units,\n",
    "            activation=activation,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
    "        )(x)\n",
    "    encoding_dim = hp.Int('encoding_dim', min_value=2, max_value=32, step=2)\n",
    "    z_mean = layers.Dense(encoding_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(encoding_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = tf.keras.Model(inputs=input_layer, outputs=[z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    # Decoder\n",
    "    latent_inputs = layers.Input(shape=(encoding_dim,))\n",
    "    x = latent_inputs\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f'units_dec_{i}', min_value=64, max_value=256, step=64)\n",
    "        x = layers.Dense(\n",
    "            units,\n",
    "            activation=activation,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
    "        )(x)\n",
    "    outputs = layers.Dense(input_dim, activation='linear')(x)\n",
    "    decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "    \n",
    "    # VAE Model\n",
    "    class VAE(tf.keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super(VAE, self).__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var, z = self.encoder(inputs)\n",
    "            reconstructed = self.decoder(z)\n",
    "            # Reconstruction loss\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.keras.losses.mse(inputs, reconstructed)\n",
    "            ) * input_dim\n",
    "            # KL divergence loss\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            )\n",
    "            self.add_loss(reconstruction_loss + kl_loss)\n",
    "            return reconstructed\n",
    "    \n",
    "    vae = VAE(encoder, decoder)\n",
    "    \n",
    "    # Compile the model\n",
    "    learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return vae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Set Up Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_vae,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='vae_tuning',\n",
    "    project_name='vitai_vae'\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Run Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter search\n",
    "tuner.search(\n",
    "    X_train, X_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training and Reconstruction Error Calculation\n",
    "After finding the best hyperparameters, we train the VAE model and compute the reconstruction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Train the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the optimal hyperparameters\n",
    "print(f\"\"\"\n",
    "The optimal number of layers is {best_hps.get('num_layers')} encoder and decoder layers.\n",
    "The optimal number of units in each layer are:\n",
    "Encoder units: {[best_hps.get(f'units_{i}') for i in range(best_hps.get('num_layers'))]}\n",
    "Decoder units: {[best_hps.get(f'units_dec_{i}') for i in range(best_hps.get('num_layers'))]}\n",
    "The optimal activation function is {best_hps.get('activation')}.\n",
    "The optimal encoding dimension is {best_hps.get('encoding_dim')}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build and train the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "best_model.save('vae_model.h5')\n",
    "\n",
    "# Clear session to free memory\n",
    "keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Compute Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = keras.models.load_model('vae_model.h5', compile=False)\n",
    "\n",
    "# Re-compile the model to include custom loss\n",
    "best_model.compile(optimizer=keras.optimizers.Adam(learning_rate=best_hps.get('learning_rate')))\n",
    "\n",
    "# Compute reconstruction error for the entire training dataset\n",
    "reconstructed = best_model.predict(X_train, batch_size=64)\n",
    "reconstruction_errors = np.mean(np.square(X_train - reconstructed), axis=1)\n",
    "\n",
    "# Add reconstruction error to the data\n",
    "train_reconstruction_error = pd.DataFrame({'Reconstruction Error': reconstruction_errors})\n",
    "\n",
    "# Save reconstruction errors\n",
    "train_reconstruction_error.to_csv('train_reconstruction_error.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 Plot Reconstruction Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reconstruction error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstruction_errors, bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.title('Reconstruction Error Distribution (Training Data)')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Latent Space Extraction and Clustering\n",
    "We extract the latent features from the encoder and apply clustering algorithms to identify patient groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1 Extract Latent Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder model\n",
    "encoder = keras.Model(inputs=best_model.input, outputs=best_model.get_layer('z').output)\n",
    "\n",
    "# Obtain latent representation\n",
    "latent_features = encoder.predict(X_train, batch_size=64)\n",
    "\n",
    "# Create a DataFrame for latent features\n",
    "latent_dim = best_hps.get('encoding_dim')\n",
    "latent_features_df = pd.DataFrame(data=latent_features, columns=[f'latent_{i}' for i in range(latent_dim)])\n",
    "\n",
    "# Add reconstruction error to the latent features DataFrame\n",
    "latent_features_df['reconstruction_error'] = reconstruction_errors\n",
    "\n",
    "# Save the latent features\n",
    "latent_features_df.to_csv('latent_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 Clustering and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering algorithms to try\n",
    "clustering_algorithms = {\n",
    "    'KMeans': KMeans,\n",
    "    'DBSCAN': DBSCAN,\n",
    "    'AgglomerativeClustering': AgglomerativeClustering\n",
    "}\n",
    "\n",
    "# For KMeans and AgglomerativeClustering, test different numbers of clusters\n",
    "range_n_clusters = list(range(2, 10))\n",
    "\n",
    "best_algorithm = None\n",
    "best_score = -1\n",
    "best_labels = None\n",
    "best_n_clusters = None\n",
    "\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    if name == 'KMeans' or name == 'AgglomerativeClustering':\n",
    "        for n_clusters in range_n_clusters:\n",
    "            if name == 'KMeans':\n",
    "                clustering = algorithm(n_clusters=n_clusters, random_state=42)\n",
    "            else:\n",
    "                clustering = algorithm(n_clusters=n_clusters)\n",
    "            cluster_labels = clustering.fit_predict(latent_features)\n",
    "            silhouette_avg = silhouette_score(latent_features, cluster_labels)\n",
    "            print(f\"For n_clusters = {n_clusters}, the average silhouette_score is : {silhouette_avg}\")\n",
    "            if silhouette_avg > best_score:\n",
    "                best_score = silhouette_avg\n",
    "                best_algorithm = name\n",
    "                best_labels = cluster_labels\n",
    "                best_n_clusters = n_clusters\n",
    "    elif name == 'DBSCAN':\n",
    "        # Try different eps and min_samples\n",
    "        eps_values = [0.3, 0.5, 0.7]\n",
    "        min_samples_values = [5, 10]\n",
    "        for eps in eps_values:\n",
    "            for min_samples in min_samples_values:\n",
    "                clustering = algorithm(eps=eps, min_samples=min_samples)\n",
    "                cluster_labels = clustering.fit_predict(latent_features)\n",
    "                # For DBSCAN, some labels might be -1 (noise)\n",
    "                # We need at least 2 clusters to compute silhouette score\n",
    "                if len(set(cluster_labels)) > 1 and -1 not in set(cluster_labels):\n",
    "                    silhouette_avg = silhouette_score(latent_features, cluster_labels)\n",
    "                    print(f\"For eps = {eps}, min_samples = {min_samples}, the average silhouette_score is : {silhouette_avg}\")\n",
    "                    if silhouette_avg > best_score:\n",
    "                        best_score = silhouette_avg\n",
    "                        best_algorithm = f\"{name} (eps={eps}, min_samples={min_samples})\"\n",
    "                        best_labels = cluster_labels\n",
    "                        best_n_clusters = len(set(cluster_labels))\n",
    "    else:\n",
    "        print(f\"Algorithm {name} not implemented.\")\n",
    "\n",
    "print(f\"\\nBest algorithm: {best_algorithm} with {best_n_clusters} clusters and silhouette score of {best_score}\")\n",
    "\n",
    "# Add the best cluster labels to the DataFrame\n",
    "latent_features_df['cluster'] = best_labels\n",
    "\n",
    "# Save the updated latent features\n",
    "latent_features_df.to_csv('latent_features_with_clusters.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Latent Space Visualization\n",
    "We visualize the latent space using t-SNE and UMAP to understand the cluster formations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1 t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use t-SNE for visualization\n",
    "print(\"Performing t-SNE...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "latent_2d = tsne.fit_transform(latent_features)\n",
    "\n",
    "# Add to DataFrame\n",
    "latent_features_df['tsne_1'] = latent_2d[:, 0]\n",
    "latent_features_df['tsne_2'] = latent_2d[:, 1]\n",
    "\n",
    "# Plot t-SNE\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='cluster', data=latent_features_df, palette='viridis', legend='full')\n",
    "plt.title('Latent Space Visualization with t-SNE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2 UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UMAP for visualization\n",
    "try:\n",
    "    import umap\n",
    "\n",
    "    print(\"Performing UMAP...\")\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    latent_2d_umap = reducer.fit_transform(latent_features)\n",
    "\n",
    "    # Add to DataFrame\n",
    "    latent_features_df['umap_1'] = latent_2d_umap[:, 0]\n",
    "    latent_features_df['umap_2'] = latent_2d_umap[:, 1]\n",
    "\n",
    "    # Plot UMAP\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='umap_1', y='umap_2', hue='cluster', data=latent_features_df, palette='viridis', legend='full')\n",
    "    plt.title('Latent Space Visualization with UMAP')\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"UMAP is not installed. Skipping UMAP visualization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Health Severity Index Assignment\n",
    "We assign a health severity index based on the cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map clusters to severity scores\n",
    "cluster_severity = {cluster: index for index, cluster in enumerate(sorted(latent_features_df['cluster'].unique()))}\n",
    "latent_features_df['severity_index'] = latent_features_df['cluster'].map(cluster_severity)\n",
    "\n",
    "# Optionally, scale severity index to 0-10 range\n",
    "scaler_severity = MinMaxScaler(feature_range=(0, 10))\n",
    "latent_features_df['severity_index_scaled'] = scaler_severity.fit_transform(latent_features_df[['severity_index']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Cluster Analysis and Validation\n",
    "We analyze the clusters to ensure they make clinical sense by examining the characteristics of each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.1 Combine Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original data with cluster labels\n",
    "analysis_df = pd.DataFrame(X_train, columns=structured_data.columns)\n",
    "analysis_df['cluster'] = latent_features_df['cluster']\n",
    "analysis_df['severity_index'] = latent_features_df['severity_index_scaled']\n",
    "analysis_df['reconstruction_error'] = latent_features_df['reconstruction_error']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.2 Compute Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and compute summary statistics\n",
    "cluster_summary = analysis_df.groupby('cluster').mean()\n",
    "print(\"\\nCluster Summary Statistics:\")\n",
    "print(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.3 Visualize Reconstruction Error by Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction error by cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='cluster', y='reconstruction_error', data=analysis_df)\n",
    "plt.title('Reconstruction Error by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4 Visualize Key Features by Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key features by cluster\n",
    "key_features = structured_data.columns.tolist()  # List of feature names\n",
    "\n",
    "# Limit to first 5 features for brevity\n",
    "for feature in key_features[:5]:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='cluster', y=feature, data=analysis_df)\n",
    "    plt.title(f'Distribution of {feature} by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Memory Management and Cleanup\n",
    "We perform cleanup to manage memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear variables to free memory\n",
    "del X_train\n",
    "del X_val\n",
    "del latent_features\n",
    "del latent_2d\n",
    "if 'latent_2d_umap' in locals():\n",
    "    del latent_2d_umap\n",
    "del encoder\n",
    "del best_model\n",
    "\n",
    "print(\"Analysis complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we successfully:\n",
    "<ul>\n",
    "<li>Implemented a Variational Autoencoder to learn latent representations of patient health data.</li>\n",
    "<li>Tuned hyperparameters to find the optimal VAE configuration.</li>\n",
    "<li>Calculated reconstruction errors to assess model performance.</li>\n",
    "<li>Applied different clustering algorithms to the latent space to identify patient groups.</li>\n",
    "<li>Visualized the latent space using t-SNE and UMAP.</li>\n",
    "<li>Assigned a health severity index based on cluster assignments.</li>\n",
    "<li>Analyzed clusters to interpret the severity levels.</li>\n",
    "</ul>\n",
    "<br>\n",
    "This approach allows us to identify patterns in patient data that may correspond to varying levels of health severity, aiding in clinical decision-making and resource allocation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
