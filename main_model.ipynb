{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train Autoencoder on Strucutred Data</h3>\n",
    "<p>Train the autoencoder on the structured data and use the reconstruction error for each sample as an anomaly score.</p>\n",
    "<p>Samples with higher reconstruction errors may represent more severe health conditions</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Load preprocessed structured data\n",
    "structured_data = pd.read_csv('structured_data_preprocessed.csv')\n",
    "\n",
    "#Separate features and target (unspervised learning)\n",
    "X_structured = structured_data.values\n",
    "\n",
    "#Standardise the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_structured)\n",
    "\n",
    "#Train-test split  \n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "#Define the autoencoder model\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 64\n",
    "\n",
    "\n",
    "#Encoder\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "encoded = layers.Dense(64, activation='relu')(input_layer)\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(encoded)    \n",
    "\n",
    "#Decoder\n",
    "decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "#Autoencoder Model\n",
    "autoencoder = models.Model(inputs = input_layer,outputs = decoded)\n",
    "\n",
    "#Compile the Model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    " \n",
    "#Train the Autoencoder\n",
    "history = autoencoder.fit(X_train, X_train, \n",
    "                          epochs=50,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          validation_data=(X_val, X_val))\n",
    "\n",
    "\n",
    "#Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Autoencoder Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "autoencoder.save('autoencoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compute Reconstruction Error</h3>\n",
    "<p>After trainig the autoencoder, we will compute the reconstruction error for each patient</p>\n",
    "<p>The reconstruction error serves as an <strong>anomaly score</strong>. We can interpret higher reconstruction errors as higher severity levels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reconstruction error for the entire dataset   \n",
    "reconstructed = autoencoder.predict(X_scaled)\n",
    "reconstructed_error = np.mean(np.square(X_scaled - reconstructed,2), axis=1)\n",
    "\n",
    "#Add reconstruction error to the structured data\n",
    "structured_data['Reconstruction Error'] = reconstructed_error\n",
    "\n",
    "#Plot the reconstruction error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstructed_error,bins=50,color='blue',alpha=0.7,edgecolor='black')\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Latent Representaions</h3>\n",
    "<p>Extract the latent features from the encoder part of the autoencoder for further analysis</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create encoder model\n",
    "encoder = models.Model(inputs = autoencoder.input, outputs = encoded)\n",
    "\n",
    "#Obtain latent representation\n",
    "latent_features = encoder.predict(X_scaled)\n",
    "\n",
    "#Create a DataFrame for latent features\n",
    "latent_features_df = pd.DataFrame(data=latent_features, columns=[f'latent_{i}' for i in range(encoding_dim)])\n",
    "\n",
    "#Add reconsturction error to the latent features DF\n",
    "latent_features_df['reconstruction_error'] = reconstructed_error\n",
    "\n",
    "#Save the latent features\n",
    "latent_features_df.to_csv('latent_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apply Clustering Algorithm</h3>\n",
    "<p>We apply a clustering algorithm on the combined latent features to idnentify groups of patients that may correspond to different levels of health severity.</p>\n",
    "<p>We use K-Means clustering to group patients based on their latent features</p>\n",
    "<p>The silhouette score helps determine the optimal number of clusters</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Determine the optimal number of clusters using the silhouette score\n",
    "range_n_clusters = list(range(2, 10))\n",
    "best_n_clusters = 0\n",
    "best_score = -1\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(latent_features_df)\n",
    "    silhouette_avg = silhouette_score(latent_features_df, cluster_labels)\n",
    "    print(f'For n_clusters = {n_clusters}, the average silhouette_score is : {silhouette_avg}')\n",
    "    if silhouette_avg > best_score:\n",
    "        best_score = silhouette_avg\n",
    "        best_n_clusters = n_clusters\n",
    "\n",
    "print(f'Best number of clusters: {best_n_clusters}')\n",
    "\n",
    "# Apply KMeans with the best number of clusters\n",
    "kmeans = KMeans(n_clusters=best_n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(latent_features_df)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "latent_features_df['cluster'] = cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assign Health Severity Index</h3>\n",
    "<p>We assing a severit index based on the cluster assignments.</p>\n",
    "<p>Clusters represent different groups of patients with varying health conditions</p>\n",
    "<p>By assinging severity scores based on cluster labels, we derive a health severiy index without human bias</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map clusters to severity scores\n",
    "# Assuming higher cluster labels correspond to higher severity\n",
    "# If not, you may need to analyze clusters to order them appropriately\n",
    "\n",
    "# Map cluster labels to severity scores (e.g., 0 to N-1)\n",
    "cluster_severity = {cluster: index for index, cluster in enumerate(sorted(latent_features_df['cluster'].unique()))}\n",
    "latent_features_df['severity_index'] = latent_features_df['cluster'].map(cluster_severity)\n",
    "\n",
    "# Optionally, scale severity index to 0-10 range\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "latent_features_df['severity_index_scaled'] = scaler.fit_transform(latent_features_df[['severity_index']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Analyse and Validate Clusters</h3>\n",
    "<p>It's important to analyse the clusters to ensure they make clinical sense.</p>\n",
    "<p>By exmamining the characteristics of each cluster, you can interpret the severity levles.</p>\n",
    "<p>*look for patterns in the data that correlate with the clusters*</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original data with cluster labels\n",
    "analysis_df = structured_data.copy()\n",
    "analysis_df['cluster'] = latent_features_df['cluster']\n",
    "analysis_df['severity_index'] = latent_features_df['severity_index_scaled']\n",
    "\n",
    "# Group by cluster and compute summary statistics\n",
    "cluster_summary = analysis_df.groupby('cluster').mean()\n",
    "print(cluster_summary)\n",
    "\n",
    "# Visualize reconstruction error by cluster\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='cluster', y='reconstruction_error', data=analysis_df)\n",
    "plt.title('Reconstruction Error by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.show()\n",
    "\n",
    "# Visualize key features by cluster\n",
    "key_features = structured_data.columns.tolist()  # List of feature names\n",
    "\n",
    "for feature in key_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='cluster', y=feature, data=analysis_df)\n",
    "    plt.title(f'Distribution of {feature} by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Need restart kernel? Dw I've got you</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "\n",
    "# Restart the kernel\n",
    "os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
